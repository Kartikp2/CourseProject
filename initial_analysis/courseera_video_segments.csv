key,timeline_start,timeline_end,segment_link,segment_txt
cs-410_7_4_1,"00:00:00,006","00:00:03,253",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_7_4_2,"00:00:13,295","00:00:15,326",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,So let's plug in these model masses
cs-410_7_4_3,"00:00:15,326","00:00:18,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,into the ranking function to
cs-410_7_4_4,"00:00:18,610","00:00:20,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,This is a general smoothing.
cs-410_7_4_5,"00:00:20,780","00:00:24,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,So a general ranking function for
cs-410_7_4_6,"00:00:24,570","00:00:26,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,you have seen this before.
cs-410_7_4_7,"00:00:28,060","00:00:32,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,And now we have a very specific smoothing
cs-410_7_4_8,"00:00:33,690","00:00:39,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So now let's see what what's a value for
cs-410_7_4_9,"00:00:40,450","00:00:42,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,And what's the value for p sub c here?
cs-410_7_4_10,"00:00:42,900","00:00:46,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,"Right, so we may need to decide this"
cs-410_7_4_11,"00:00:46,930","00:00:50,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,in order to figure out the exact
cs-410_7_4_12,"00:00:50,470","00:00:52,598",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,And we also need to figure
cs-410_7_4_13,"00:00:52,598","00:00:55,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,So let's see.
cs-410_7_4_14,"00:00:55,910","00:01:00,666",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"Well this ratio is basically this,"
cs-410_7_4_15,"00:01:00,666","00:01:05,315",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"here, this is the probability"
cs-410_7_4_16,"00:01:05,315","00:01:09,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,and this is the probability
cs-410_7_4_17,"00:01:09,330","00:01:14,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,in other words basically 11
cs-410_7_4_18,"00:01:14,935","00:01:18,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"this, so it's easy to see that."
cs-410_7_4_19,"00:01:18,530","00:01:21,681",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,This can be then rewritten as this.
cs-410_7_4_20,"00:01:21,681","00:01:24,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Very simple.
cs-410_7_4_21,"00:01:24,500","00:01:26,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So we can plug this into here.
cs-410_7_4_22,"00:01:28,650","00:01:30,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"And then here, what's the value for alpha?"
cs-410_7_4_23,"00:01:30,710","00:01:31,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,What do you think?
cs-410_7_4_24,"00:01:31,660","00:01:35,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"So it would be just lambda, right?"
cs-410_7_4_25,"00:01:38,250","00:01:43,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,And what would happen if we plug in
cs-410_7_4_26,"00:01:43,900","00:01:45,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,What can we say about this?
cs-410_7_4_27,"00:01:47,940","00:01:49,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,Does it depend on the document?
cs-410_7_4_28,"00:01:50,660","00:01:52,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"No, so it can be ignored."
cs-410_7_4_29,"00:01:53,570","00:01:55,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,Right?
cs-410_7_4_30,"00:01:55,040","00:01:58,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So we'll end up having this
cs-410_7_4_31,"00:02:00,520","00:02:02,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"And in this case you can easy to see,"
cs-410_7_4_32,"00:02:02,690","00:02:07,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,this a precisely a vector space
cs-410_7_4_33,"00:02:07,780","00:02:13,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"a sum over all the matched query terms,"
cs-410_7_4_34,"00:02:13,480","00:02:16,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,What do you think is a element
cs-410_7_4_35,"00:02:18,670","00:02:20,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"Well it's this, right."
cs-410_7_4_36,"00:02:20,200","00:02:23,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,So that's our document left element.
cs-410_7_4_37,"00:02:23,210","00:02:29,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And let's further examine what's
cs-410_7_4_38,"00:02:30,370","00:02:32,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,Well one plus this.
cs-410_7_4_39,"00:02:32,440","00:02:36,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"So it's going to be nonnegative,"
cs-410_7_4_40,"00:02:36,630","00:02:37,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"it's going to be at least 1, right?"
cs-410_7_4_41,"00:02:39,450","00:02:42,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"And these, this is a parameter,"
cs-410_7_4_42,"00:02:42,900","00:02:44,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,And let's look at this.
cs-410_7_4_43,"00:02:44,340","00:02:45,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,Now this is a TF.
cs-410_7_4_44,"00:02:45,480","00:02:48,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,Now we see very clearly
cs-410_7_4_45,"00:02:49,250","00:02:54,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"And the larger the count is,"
cs-410_7_4_46,"00:02:54,080","00:02:57,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"We also see IDF weighting,"
cs-410_7_4_47,"00:02:58,720","00:03:00,996",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,And we see docking the lan's
cs-410_7_4_48,"00:03:00,996","00:03:03,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,So all these heuristics
cs-410_7_4_49,"00:03:04,532","00:03:08,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,What's interesting that
cs-410_7_4_50,"00:03:08,480","00:03:12,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,weighting function automatically
cs-410_7_4_51,"00:03:12,330","00:03:14,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"Whereas in the vector space model,"
cs-410_7_4_52,"00:03:14,270","00:03:19,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,we had to go through those heuristic
cs-410_7_4_53,"00:03:19,330","00:03:21,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,And in this case note that
cs-410_7_4_54,"00:03:21,880","00:03:25,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,And when you see whether this
cs-410_7_4_55,"00:03:26,690","00:03:31,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,All right so what do you think
cs-410_7_4_56,"00:03:31,050","00:03:33,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,This is a math of document.
cs-410_7_4_57,"00:03:33,320","00:03:37,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"Total number of words,"
cs-410_7_4_58,"00:03:38,400","00:03:42,727",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"given by the collection, right?"
cs-410_7_4_59,"00:03:42,727","00:03:48,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,So this actually can be interpreted
cs-410_7_4_60,"00:03:48,090","00:03:53,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"If we're going to draw, a word,"
cs-410_7_4_61,"00:03:53,730","00:03:57,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"And, we're going to draw as many as"
cs-410_7_4_62,"00:03:59,310","00:04:02,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"If you do that,"
cs-410_7_4_63,"00:04:02,940","00:04:06,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,would be precisely given
cs-410_7_4_64,"00:04:08,240","00:04:14,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"So, this ratio basically,"
cs-410_7_4_65,"00:04:15,860","00:04:21,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,The actual count of the word in the
cs-410_7_4_66,"00:04:21,280","00:04:29,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,product if the word is in fact following
cs-410_7_4_67,"00:04:29,570","00:04:33,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,And if this counter is larger than
cs-410_7_4_68,"00:04:33,250","00:04:34,789",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,this ratio would be larger than one.
cs-410_7_4_69,"00:04:37,100","00:04:40,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,So that's actually a very
cs-410_7_4_70,"00:04:40,460","00:04:43,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"It's very natural and intuitive,"
cs-410_7_4_71,"00:04:45,240","00:04:49,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,And this is one advantage of using
cs-410_7_4_72,"00:04:49,580","00:04:53,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,where we have made explicit assumptions.
cs-410_7_4_73,"00:04:53,240","00:04:56,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"And, we know precisely why"
cs-410_7_4_74,"00:04:56,490","00:04:58,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"And, why we have these probabilities here."
cs-410_7_4_75,"00:05:00,280","00:05:04,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"And, we also have a formula that"
cs-410_7_4_76,"00:05:04,290","00:05:07,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,does TF-IDF weighting and
cs-410_7_4_77,"00:05:09,010","00:05:11,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"Let's look at the,"
cs-410_7_4_78,"00:05:11,440","00:05:16,852",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,It's very similar to
cs-410_7_4_79,"00:05:16,852","00:05:21,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,"In this case,"
cs-410_7_4_80,"00:05:21,540","00:05:27,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,that's different from
cs-410_7_4_81,"00:05:27,660","00:05:30,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,But the format looks very similar.
cs-410_7_4_82,"00:05:30,660","00:05:32,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,The form of the function
cs-410_7_4_83,"00:05:34,540","00:05:36,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So we still have linear operation here.
cs-410_7_4_84,"00:05:38,090","00:05:40,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"And when we compute this ratio,"
cs-410_7_4_85,"00:05:40,130","00:05:45,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,one will find that is that
cs-410_7_4_86,"00:05:46,930","00:05:51,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,And what's interesting here is that we
cs-410_7_4_87,"00:05:51,620","00:05:54,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,We're comparing the actual count.
cs-410_7_4_88,"00:05:54,440","00:05:59,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Which is the expected account of the world
cs-410_7_4_89,"00:05:59,400","00:06:02,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,the collection world probability.
cs-410_7_4_90,"00:06:02,660","00:06:07,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,So note that it's interesting we don't
cs-410_7_4_91,"00:06:07,266","00:06:08,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,lighter in the JMs model.
cs-410_7_4_92,"00:06:08,910","00:06:13,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,All right so this of course
cs-410_7_4_93,"00:06:15,290","00:06:18,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,"So you might wonder, so"
cs-410_7_4_94,"00:06:18,200","00:06:23,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,Interestingly the docking lens
cs-410_7_4_95,"00:06:23,650","00:06:26,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,this would be plugged into this part.
cs-410_7_4_96,"00:06:26,850","00:06:31,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,As a result what we get is
cs-410_7_4_97,"00:06:31,860","00:06:35,239",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,this is again a sum over
cs-410_7_4_98,"00:06:36,290","00:06:40,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"And we're against the queer,"
cs-410_7_4_99,"00:06:41,410","00:06:45,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,And you can interpret this as
cs-410_7_4_100,"00:06:45,425","00:06:48,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,but this is no longer
cs-410_7_4_101,"00:06:50,100","00:06:55,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"Because we have this part,"
cs-410_7_4_102,"00:06:55,165","00:06:57,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,right?
cs-410_7_4_103,"00:06:57,810","00:07:01,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,So that just means if
cs-410_7_4_104,"00:07:01,510","00:07:05,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,we have to take a sum over
cs-410_7_4_105,"00:07:05,160","00:07:09,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,then do some adjustment of
cs-410_7_4_106,"00:07:11,510","00:07:15,974",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,"But it's still, it's still clear"
cs-410_7_4_107,"00:07:15,974","00:07:19,765",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,modulation because this lens
cs-410_7_4_108,"00:07:19,765","00:07:23,237",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,a longer document will
cs-410_7_4_109,"00:07:23,237","00:07:27,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,And we can also see it has tf here and
cs-410_7_4_110,"00:07:27,600","00:07:32,038",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Only that this time the form of the
cs-410_7_4_111,"00:07:32,038","00:07:34,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,in JMs one.
cs-410_7_4_112,"00:07:34,580","00:07:39,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,But intuitively it still implements TFIDF
cs-410_7_4_113,"00:07:39,780","00:07:44,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,the form of the function is dictated
cs-410_7_4_114,"00:07:44,340","00:07:45,938",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,assumptions that we have made.
cs-410_7_4_115,"00:07:45,938","00:07:50,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now there are also
cs-410_7_4_116,"00:07:50,420","00:07:53,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"And that is, there's no guarantee"
cs-410_7_4_117,"00:07:53,600","00:07:55,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,of the formula will actually work well.
cs-410_7_4_118,"00:07:55,800","00:08:01,037",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"So if we look about at this geo function,"
cs-410_7_4_119,"00:08:01,037","00:08:06,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,rendition for example it's unclear whether
cs-410_7_4_120,"00:08:06,860","00:08:13,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,Unfortunately we can see here there
cs-410_7_4_121,"00:08:13,110","00:08:17,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"So we do have also the,"
cs-410_7_4_122,"00:08:17,580","00:08:20,986",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,So we do have the sublinear
cs-410_7_4_123,"00:08:20,986","00:08:23,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,we do not intentionally do that.
cs-410_7_4_124,"00:08:23,320","00:08:27,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,That means there's no guarantee that
cs-410_7_4_125,"00:08:27,750","00:08:31,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"Suppose we don't have logarithm,"
cs-410_7_4_126,"00:08:31,800","00:08:35,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"As we discussed before, perhaps"
cs-410_7_4_127,"00:08:35,810","00:08:40,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,So that's an example of the gap
cs-410_7_4_128,"00:08:40,870","00:08:43,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"the relevance that we have to model,"
cs-410_7_4_129,"00:08:43,080","00:08:48,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,which is really a subject
cs-410_7_4_130,"00:08:50,640","00:08:53,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,So it doesn't mean we cannot fix this.
cs-410_7_4_131,"00:08:53,390","00:08:57,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"For example, imagine if we did"
cs-410_7_4_132,"00:08:57,390","00:08:59,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,So we can take a risk and
cs-410_7_4_133,"00:08:59,250","00:09:01,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,or we can even add double logarithm.
cs-410_7_4_134,"00:09:01,935","00:09:06,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"But then, it would mean that the function"
cs-410_7_4_135,"00:09:06,200","00:09:10,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,So the consequence of
cs-410_7_4_136,"00:09:10,780","00:09:14,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,longer as predictable as
cs-410_7_4_137,"00:09:15,810","00:09:21,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"So, that's also why, for example,"
cs-410_7_4_138,"00:09:21,410","00:09:26,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"still, open channel how to use"
cs-410_7_4_139,"00:09:26,720","00:09:28,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,better model than the PM25.
cs-410_7_4_140,"00:09:30,420","00:09:34,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,In particular how do we use query
cs-410_7_4_141,"00:09:34,500","00:09:37,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,that would work consistently
cs-410_7_4_142,"00:09:37,650","00:09:39,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,Currently we still cannot do that.
cs-410_7_4_143,"00:09:40,240","00:09:41,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,Still interesting open question.
cs-410_7_4_144,"00:09:43,450","00:09:46,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"So to summarize this part, we've talked"
cs-410_7_4_145,"00:09:46,975","00:09:52,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,Jelinek-Mercer which is doing the fixed
cs-410_7_4_146,"00:09:52,550","00:09:58,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,Dirichlet Prior this is what add a pseudo
cs-410_7_4_147,"00:09:58,430","00:10:04,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,interpolation in that the coefficient
cs-410_7_4_148,"00:10:05,940","00:10:10,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,"In most cases we can see, by using these"
cs-410_7_4_149,"00:10:10,890","00:10:16,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,reach a retrieval function where
cs-410_7_4_150,"00:10:16,670","00:10:17,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,So they are less heuristic.
cs-410_7_4_151,"00:10:19,090","00:10:23,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,Explaining the results also show
cs-410_7_4_152,"00:10:23,810","00:10:31,036",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,Also are very effective and they are
cs-410_7_4_153,"00:10:31,036","00:10:36,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,So this is a major advantage
cs-410_7_4_154,"00:10:36,260","00:10:39,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,where we don't have to do
cs-410_7_4_155,"00:10:40,770","00:10:44,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,Yet in the end that we naturally
cs-410_7_4_156,"00:10:44,240","00:10:45,239",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,doc length normalization.
cs-410_7_4_157,"00:10:46,480","00:10:51,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,Each of these functions also has
cs-410_7_4_158,"00:10:51,120","00:10:54,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,In this case of course we still need
cs-410_7_4_159,"00:10:54,840","00:10:58,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,There are also methods that can be
cs-410_7_4_160,"00:10:59,950","00:11:04,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"So overall,"
cs-410_7_4_161,"00:11:04,020","00:11:08,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,we follow very different strategies
cs-410_7_4_162,"00:11:08,900","00:11:12,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"Yet, in the end, we end up uh,with"
cs-410_7_4_163,"00:11:12,980","00:11:15,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,look very similar to
cs-410_7_4_164,"00:11:15,540","00:11:21,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,With some advantages in having
cs-410_7_4_165,"00:11:21,160","00:11:24,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"And then, the form dictated"
cs-410_7_4_166,"00:11:24,940","00:11:29,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,"Now, this also concludes our discussion of"
cs-410_7_4_167,"00:11:29,740","00:11:34,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,And let's recall what
cs-410_7_4_168,"00:11:34,680","00:11:39,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=694,in order to derive the functions
cs-410_7_4_169,"00:11:39,390","00:11:42,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,Well we basically have made four
cs-410_7_4_170,"00:11:42,130","00:11:48,399",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,The first assumption is that the relevance
cs-410_7_4_171,"00:11:49,470","00:11:53,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,"And the second assumption with med is, are"
cs-410_7_4_172,"00:11:53,450","00:11:57,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,that allows us to decompose
cs-410_7_4_173,"00:11:57,240","00:12:01,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,into a product of probabilities
cs-410_7_4_174,"00:12:03,090","00:12:07,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,"And then,"
cs-410_7_4_175,"00:12:07,850","00:12:10,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"if a word is not seen,"
cs-410_7_4_176,"00:12:10,550","00:12:14,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,its probability proportional to
cs-410_7_4_177,"00:12:14,870","00:12:17,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,That's a smoothing with
cs-410_7_4_178,"00:12:17,290","00:12:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"And finally, we made one of these"
cs-410_7_4_179,"00:12:20,980","00:12:24,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,So we either used JM smoothing or
cs-410_7_4_180,"00:12:24,940","00:12:28,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,If we make these four assumptions
cs-410_7_4_181,"00:12:28,820","00:12:33,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,to take the form of the retrieval
cs-410_7_4_182,"00:12:33,430","00:12:37,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Fortunately the function has a nice
cs-410_7_4_183,"00:12:37,730","00:12:44,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,weighting and document machine and
cs-410_7_4_184,"00:12:44,510","00:12:45,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"So in that sense,"
cs-410_7_4_185,"00:12:45,440","00:12:48,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,these functions are less heuristic
cs-410_7_4_186,"00:12:50,460","00:12:54,282",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,"And there are many extensions of this,"
cs-410_7_4_187,"00:12:54,282","00:12:59,336",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,you can find the discussion of them in
cs-410_7_4_188,"00:13:04,921","00:13:14,921",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,[MUSIC]
cs-410_4_4_1,"00:00:00,012","00:00:07,304",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_4_2,"00:00:07,304","00:00:10,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about smoothing
cs-410_4_4_3,"00:00:11,700","00:00:12,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture,"
cs-410_4_4_4,"00:00:12,390","00:00:16,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,we're going to continue talking about
cs-410_4_4_5,"00:00:16,110","00:00:19,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In particular,"
cs-410_4_4_6,"00:00:19,630","00:00:22,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,language model in the query
cs-410_4_4_7,"00:00:23,820","00:00:27,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So you have seen this slide
cs-410_4_4_8,"00:00:27,248","00:00:30,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,This is the ranking function
cs-410_4_4_9,"00:00:32,540","00:00:39,906",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"Here, we assume that the independence of"
cs-410_4_4_10,"00:00:39,906","00:00:45,367",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,would look like the following where
cs-410_4_4_11,"00:00:45,367","00:00:49,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,And inside the sum there is a log
cs-410_4_4_12,"00:00:49,878","00:00:52,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,the document or document image model.
cs-410_4_4_13,"00:00:52,700","00:00:57,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,So the main task now is to estimate this
cs-410_4_4_14,"00:00:57,750","00:01:02,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,document language model as we
cs-410_4_4_15,"00:01:02,100","00:01:06,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,estimating this model would lead
cs-410_4_4_16,"00:01:06,530","00:01:10,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"So in this lecture, we're going to"
cs-410_4_4_17,"00:01:10,810","00:01:13,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,So how do we estimate this language model?
cs-410_4_4_18,"00:01:13,110","00:01:16,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,Well the obvious choice would be
cs-410_4_4_19,"00:01:16,350","00:01:17,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,that we have seen before.
cs-410_4_4_20,"00:01:17,990","00:01:22,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,And that is we're going to normalize
cs-410_4_4_21,"00:01:24,110","00:01:26,913",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,And estimate the probability
cs-410_4_4_22,"00:01:30,234","00:01:33,194",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,This is a step function here.
cs-410_4_4_23,"00:01:35,934","00:01:38,543",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,Which means all of the words that have
cs-410_4_4_24,"00:01:38,543","00:01:43,016",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,the same frequency count will
cs-410_4_4_25,"00:01:43,016","00:01:48,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"This is another freedom to count,"
cs-410_4_4_26,"00:01:48,570","00:01:51,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,Note that for words that have not
cs-410_4_4_27,"00:01:52,850","00:01:55,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,they will have 0 probability.
cs-410_4_4_28,"00:01:55,130","00:02:00,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So we know this is just like the model
cs-410_4_4_29,"00:02:00,880","00:02:06,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,Where we assume that the use of
cs-410_4_4_30,"00:02:06,730","00:02:07,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,a formula to clear it.
cs-410_4_4_31,"00:02:09,200","00:02:13,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,And there's no chance of assembling any
cs-410_4_4_32,"00:02:13,510","00:02:14,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,we know that's not good.
cs-410_4_4_33,"00:02:15,420","00:02:17,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,So how do we improve this?
cs-410_4_4_34,"00:02:17,240","00:02:23,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,Well in order to assign
cs-410_4_4_35,"00:02:23,170","00:02:28,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,to words that have not been observed in
cs-410_4_4_36,"00:02:28,710","00:02:35,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,some probability mass from the words
cs-410_4_4_37,"00:02:35,200","00:02:39,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"So for example here, we have to take away"
cs-410_4_4_38,"00:02:39,894","00:02:45,103",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,need some extra probability mass for
cs-410_4_4_39,"00:02:45,103","00:02:47,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,So all these probabilities must sum to 1.
cs-410_4_4_40,"00:02:47,870","00:02:53,224",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,So to make this transformation and to
cs-410_4_4_41,"00:02:53,224","00:03:00,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,by assigning non zero probabilities to
cs-410_4_4_42,"00:03:01,970","00:03:06,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,We have to do smoothing and
cs-410_4_4_43,"00:03:06,630","00:03:11,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,the estimate by considering
cs-410_4_4_44,"00:03:13,970","00:03:17,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,had been asking to write more words for
cs-410_4_4_45,"00:03:17,800","00:03:22,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"the document,"
cs-410_4_4_46,"00:03:22,910","00:03:27,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,If you think about this factor
cs-410_4_4_47,"00:03:27,050","00:03:30,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,would be a more accurate than
cs-410_4_4_48,"00:03:30,830","00:03:35,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,Imagine you have seen an abstract
cs-410_4_4_49,"00:03:35,270","00:03:37,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,Let's say this document is abstract.
cs-410_4_4_50,"00:03:39,250","00:03:47,844",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,If we assume and see words in this
cs-410_4_4_51,"00:03:47,844","00:03:51,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,That would mean there's
cs-410_4_4_52,"00:03:51,900","00:03:57,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,a word outside the abstract
cs-410_4_4_53,"00:03:57,170","00:04:02,193",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,But imagine a user who is interested
cs-410_4_4_54,"00:04:02,193","00:04:06,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,The user might actually
cs-410_4_4_55,"00:04:06,475","00:04:08,973",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,that chapter to use as query.
cs-410_4_4_56,"00:04:08,973","00:04:13,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"So obviously,"
cs-410_4_4_57,"00:04:13,916","00:04:18,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,author would have written
cs-410_4_4_58,"00:04:18,760","00:04:23,627",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,So smoothing of the language
cs-410_4_4_59,"00:04:23,627","00:04:27,642",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,to recover the model for
cs-410_4_4_60,"00:04:27,642","00:04:32,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"And then of course,"
cs-410_4_4_61,"00:04:32,346","00:04:36,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,words that are not
cs-410_4_4_62,"00:04:36,310","00:04:39,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,So that's why smoothing is
cs-410_4_4_63,"00:04:39,250","00:04:43,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,So let's talk a little more about
cs-410_4_4_64,"00:04:43,670","00:04:48,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,"The key question here is, what probability"
cs-410_4_4_65,"00:04:50,480","00:04:52,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,And there are many different
cs-410_4_4_66,"00:04:53,290","00:04:59,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"One idea here, that's very useful for"
cs-410_4_4_67,"00:04:59,500","00:05:03,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,word be proportional to its probability
cs-410_4_4_68,"00:05:03,790","00:05:07,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,That means if you don't observe
cs-410_4_4_69,"00:05:07,785","00:05:11,583",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,We're going to assume that its
cs-410_4_4_70,"00:05:11,583","00:05:16,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,by another reference language
cs-410_4_4_71,"00:05:16,310","00:05:20,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,It will tell us which unseen words
cs-410_4_4_72,"00:05:22,440","00:05:26,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"In the case of retrieval,"
cs-410_4_4_73,"00:05:26,060","00:05:30,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,take the collection language model
cs-410_4_4_74,"00:05:30,080","00:05:33,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"That is to say, if you don't"
cs-410_4_4_75,"00:05:33,390","00:05:37,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,we're going to assume that
cs-410_4_4_76,"00:05:37,440","00:05:40,658",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,would be proportional to the probability
cs-410_4_4_77,"00:05:40,658","00:05:42,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"So more formally,"
cs-410_4_4_78,"00:05:42,990","00:05:46,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,we'll be estimating the probability
cs-410_4_4_79,"00:05:48,220","00:05:54,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,If the word is seen in
cs-410_4_4_80,"00:05:54,479","00:06:02,251",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,would be this counted the maximum
cs-410_4_4_81,"00:06:02,251","00:06:07,142",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"Otherwise, if the word is not seen in the"
cs-410_4_4_82,"00:06:07,142","00:06:12,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,be proportional to the probability
cs-410_4_4_83,"00:06:12,220","00:06:17,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And here the coefficient that offer is to
cs-410_4_4_84,"00:06:17,060","00:06:21,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,control the amount of probability
cs-410_4_4_85,"00:06:22,450","00:06:25,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"Obviously, all these"
cs-410_4_4_86,"00:06:25,390","00:06:28,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,alpha sub d is constrained in some way.
cs-410_4_4_87,"00:06:29,390","00:06:33,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,So what if we plug in this
cs-410_4_4_88,"00:06:33,370","00:06:35,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,query likelihood ranking function?
cs-410_4_4_89,"00:06:35,150","00:06:36,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,This is what we will get.
cs-410_4_4_90,"00:06:37,790","00:06:43,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"In this formula, we have this"
cs-410_4_4_91,"00:06:43,930","00:06:48,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,as a sum over all the query words and
cs-410_4_4_92,"00:06:48,900","00:06:54,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,those that we have written here as the sum
cs-410_4_4_93,"00:06:54,000","00:06:56,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,This is the sum of all
cs-410_4_4_94,"00:06:56,780","00:07:00,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,but not that we have a count
cs-410_4_4_95,"00:07:00,310","00:07:04,476",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"So in fact, we are just taking"
cs-410_4_4_96,"00:07:04,476","00:07:11,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,This is now a common
cs-410_4_4_97,"00:07:11,820","00:07:16,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,because of its convenience
cs-410_4_4_98,"00:07:18,710","00:07:21,949",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"So this is as I said,"
cs-410_4_4_99,"00:07:23,130","00:07:26,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,"In our smoothing method,"
cs-410_4_4_100,"00:07:26,950","00:07:31,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,are not observed in the method would have
cs-410_4_4_101,"00:07:31,310","00:07:33,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"Name it's four, this foru."
cs-410_4_4_102,"00:07:33,663","00:07:37,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"So we're going to do then,"
cs-410_4_4_103,"00:07:38,620","00:07:44,422",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,One sum is over all the query words
cs-410_4_4_104,"00:07:44,422","00:07:49,287",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"That means that in this sum, all the words"
cs-410_4_4_105,"00:07:49,287","00:07:54,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,have a non zero probability
cs-410_4_4_106,"00:07:54,580","00:07:59,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"Sorry, it's the non zero count"
cs-410_4_4_107,"00:07:59,740","00:08:01,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,They all occur in the document.
cs-410_4_4_108,"00:08:02,230","00:08:07,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,And they also have to of course
cs-410_4_4_109,"00:08:07,800","00:08:13,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So these are the query words
cs-410_4_4_110,"00:08:13,894","00:08:19,153",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"On the other hand, in this sum we"
cs-410_4_4_111,"00:08:19,153","00:08:23,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,that are not all query was
cs-410_4_4_112,"00:08:25,840","00:08:31,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,So they occur in the query
cs-410_4_4_113,"00:08:31,250","00:08:33,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,they don't occur in the document.
cs-410_4_4_114,"00:08:33,200","00:08:33,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,"In this case,"
cs-410_4_4_115,"00:08:33,920","00:08:39,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,these words have this probability because
cs-410_4_4_116,"00:08:39,346","00:08:44,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"That here, these seen words"
cs-410_4_4_117,"00:08:47,490","00:08:51,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Now, we can go further by"
cs-410_4_4_118,"00:08:52,570","00:08:54,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,as a difference of two other sums.
cs-410_4_4_119,"00:08:54,790","00:08:58,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"Basically, the first sum is"
cs-410_4_4_120,"00:09:00,060","00:09:05,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"Now, we know that the original sum"
cs-410_4_4_121,"00:09:05,190","00:09:10,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,This is over all the query words that
cs-410_4_4_122,"00:09:12,400","00:09:19,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,So here we pretend that they
cs-410_4_4_123,"00:09:19,740","00:09:21,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,So we take a sum over all the query words.
cs-410_4_4_124,"00:09:21,920","00:09:28,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"Obviously, this sum has extra"
cs-410_4_4_125,"00:09:30,770","00:09:33,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"Because, here we're taking"
cs-410_4_4_126,"00:09:33,710","00:09:37,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,"There, it's not matched in the document."
cs-410_4_4_127,"00:09:37,880","00:09:44,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"So in order to make them equal, we will"
cs-410_4_4_128,"00:09:44,370","00:09:48,758",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,And this is the sum over all the query
cs-410_4_4_129,"00:09:51,069","00:09:55,411",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"And this makes sense, because here"
cs-410_4_4_130,"00:09:55,411","00:09:59,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,And then we subtract the query
cs-410_4_4_131,"00:09:59,410","00:10:04,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,That would give us the query that
cs-410_4_4_132,"00:10:05,880","00:10:11,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,And this is almost a reverse
cs-410_4_4_133,"00:10:12,770","00:10:14,758",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,And you might wonder why
cs-410_4_4_134,"00:10:14,758","00:10:19,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"Well, that's because if we do this,"
cs-410_4_4_135,"00:10:19,510","00:10:25,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,then we have different forms
cs-410_4_4_136,"00:10:25,360","00:10:31,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"So now, you can see in this sum"
cs-410_4_4_137,"00:10:31,370","00:10:35,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,the query was matching the document
cs-410_4_4_138,"00:10:36,760","00:10:45,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,Here we have another sum over the same set
cs-410_4_4_139,"00:10:45,750","00:10:47,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,"But inside the sum, it's different."
cs-410_4_4_140,"00:10:49,180","00:10:52,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,But these two sums can clearly be merged.
cs-410_4_4_141,"00:10:54,300","00:10:57,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"So if we do that, we'll get another form"
cs-410_4_4_142,"00:10:57,530","00:11:02,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,of the formula that looks like
cs-410_4_4_143,"00:11:04,360","00:11:06,966",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,And note that this is
cs-410_4_4_144,"00:11:06,966","00:11:10,796",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,Because here we combine
cs-410_4_4_145,"00:11:10,796","00:11:16,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,some of the query words matching in
cs-410_4_4_146,"00:11:19,040","00:11:24,469",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,And the other sum now is
cs-410_4_4_147,"00:11:24,469","00:11:26,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,And these two parts
cs-410_4_4_148,"00:11:26,988","00:11:30,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,because these are the probabilities
cs-410_4_4_149,"00:11:31,630","00:11:36,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,This formula is very interesting
cs-410_4_4_150,"00:11:37,450","00:11:39,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,the match the query terms.
cs-410_4_4_151,"00:11:41,340","00:11:44,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,"And just like in the vector space model,"
cs-410_4_4_152,"00:11:46,030","00:11:49,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,of terms that are in the intersection of
cs-410_4_4_153,"00:11:51,320","00:11:55,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,So it already looks a little bit
cs-410_4_4_154,"00:11:55,620","00:12:02,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,"In fact, there's even more similarity"
cs-410_4_4_155,"00:12:02,573","00:12:12,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,[MUSIC]
cs-410_6_4_1,"00:00:00,008","00:00:03,638",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_6_4_2,"00:00:07,832","00:00:09,846",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the specific
cs-410_6_4_3,"00:00:09,846","00:00:14,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,smoothing methods for language models
cs-410_6_4_4,"00:00:16,560","00:00:21,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In this lecture, we will continue"
cs-410_6_4_5,"00:00:21,030","00:00:26,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,"information retrieval, particularly"
cs-410_6_4_6,"00:00:26,020","00:00:29,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,And we're going to talk about specifically
cs-410_6_4_7,"00:00:29,485","00:00:30,856",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,such a retrieval function.
cs-410_6_4_8,"00:00:33,591","00:00:39,021",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So this is a slide from a previous
cs-410_6_4_9,"00:00:39,021","00:00:44,638",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,likelihood ranking and smoothing
cs-410_6_4_10,"00:00:44,638","00:00:50,002",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,we add up having a retrieval function
cs-410_6_4_11,"00:00:50,002","00:00:57,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,So this is the retrieval function based on
cs-410_6_4_12,"00:00:57,370","00:01:02,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,You can see it's a sum of all
cs-410_6_4_13,"00:01:02,738","00:01:07,506",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,And inside its sum is the count
cs-410_6_4_14,"00:01:07,506","00:01:11,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,some weight for the term in the document.
cs-410_6_4_15,"00:01:12,240","00:01:18,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"We have t of i, the f weight here, and"
cs-410_6_4_16,"00:01:20,300","00:01:24,793",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,So clearly if we want to implement this
cs-410_6_4_17,"00:01:24,793","00:01:27,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,we still need to figure
cs-410_6_4_18,"00:01:27,650","00:01:33,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"In particular, we're going to need to"
cs-410_6_4_19,"00:01:33,730","00:01:39,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,of a word exactly and how do we set alpha.
cs-410_6_4_20,"00:01:40,270","00:01:44,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"So in order to answer this question,"
cs-410_6_4_21,"00:01:44,410","00:01:47,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"smoothing methods, and"
cs-410_6_4_22,"00:01:48,900","00:01:50,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We're going to talk about
cs-410_6_4_23,"00:01:50,512","00:01:55,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,The first is simple linear
cs-410_6_4_24,"00:01:55,575","00:01:59,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,And this is also called
cs-410_6_4_25,"00:02:01,170","00:02:04,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,So the idea is actually very simple.
cs-410_6_4_26,"00:02:04,140","00:02:09,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,This picture shows how
cs-410_6_4_27,"00:02:09,150","00:02:12,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,language model by using
cs-410_6_4_28,"00:02:12,440","00:02:17,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,That gives us word counts normalized by
cs-410_6_4_29,"00:02:17,950","00:02:21,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,The idea of using this method
cs-410_6_4_30,"00:02:22,230","00:02:26,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,is to maximize the probability
cs-410_6_4_31,"00:02:26,480","00:02:31,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"As a result,"
cs-410_6_4_32,"00:02:31,460","00:02:36,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"in the text, it's going to get"
cs-410_6_4_33,"00:02:37,810","00:02:42,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"So the idea of smoothing, then,"
cs-410_6_4_34,"00:02:42,620","00:02:47,158",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,where this word is not going to have
cs-410_6_4_35,"00:02:47,158","00:02:50,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,nonzero probability should
cs-410_6_4_36,"00:02:50,860","00:02:55,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,So we can note that network has
cs-410_6_4_37,"00:02:55,560","00:03:01,367",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So in this approach what we do is we do
cs-410_6_4_38,"00:03:01,367","00:03:06,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,likelihood placement here and
cs-410_6_4_39,"00:03:06,655","00:03:13,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,is computed by the smoothing parameter
cs-410_6_4_40,"00:03:13,040","00:03:15,817",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,So this is a smoothing parameter.
cs-410_6_4_41,"00:03:15,817","00:03:20,651",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"The larger lambda is,"
cs-410_6_4_42,"00:03:20,651","00:03:22,828",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"So by mixing them together,"
cs-410_6_4_43,"00:03:22,828","00:03:29,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,we achieve the goal of assigning nonzero
cs-410_6_4_44,"00:03:29,060","00:03:31,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,So let's see how it works for
cs-410_6_4_45,"00:03:32,430","00:03:36,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,"For example, if we compute"
cs-410_6_4_46,"00:03:37,940","00:03:41,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,Now the maximum likelihood
cs-410_6_4_47,"00:03:41,080","00:03:43,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,that's going to be here.
cs-410_6_4_48,"00:03:44,320","00:03:47,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,But the collection probability is this.
cs-410_6_4_49,"00:03:47,740","00:03:50,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So we'll just combine them
cs-410_6_4_50,"00:03:53,630","00:04:00,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"We can also see the word network,"
cs-410_6_4_51,"00:04:00,085","00:04:05,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,now is getting a non-zero
cs-410_6_4_52,"00:04:05,305","00:04:11,992",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,And that's because the count is
cs-410_6_4_53,"00:04:11,992","00:04:19,097",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"But this part is nonzero, and"
cs-410_6_4_54,"00:04:19,097","00:04:24,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,Now if you think about this and
cs-410_6_4_55,"00:04:24,109","00:04:29,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,sub d in this smoothing
cs-410_6_4_56,"00:04:29,250","00:04:34,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,Because that's remember the coefficient
cs-410_6_4_57,"00:04:34,830","00:04:40,256",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,of the word given by the collection
cs-410_6_4_58,"00:04:40,256","00:04:43,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"Okay, so"
cs-410_6_4_59,"00:04:43,340","00:04:47,903",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,The second one is similar but
cs-410_6_4_60,"00:04:47,903","00:04:49,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,linear interpolation.
cs-410_6_4_61,"00:04:49,565","00:04:52,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"It's often called Dirichlet Prior,"
cs-410_6_4_62,"00:04:54,540","00:04:59,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,So again here we face problem
cs-410_6_4_63,"00:04:59,015","00:05:01,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,an unseen word like network.
cs-410_6_4_64,"00:05:03,765","00:05:06,957",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,Again we will use the collection
cs-410_6_4_65,"00:05:06,957","00:05:09,707",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,we're going to combine them
cs-410_6_4_66,"00:05:09,707","00:05:14,739",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,The formula first can be seen as
cs-410_6_4_67,"00:05:14,739","00:05:20,258",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,likelihood estimate and
cs-410_6_4_68,"00:05:20,258","00:05:23,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,as in the J-M smoothing method.
cs-410_6_4_69,"00:05:23,580","00:05:28,388",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,Only that the coefficient now
cs-410_6_4_70,"00:05:28,388","00:05:31,532",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,"but a dynamic coefficient in this form,"
cs-410_6_4_71,"00:05:31,532","00:05:36,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"where mu is a parameter,"
cs-410_6_4_72,"00:05:36,760","00:05:40,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,And you can see if we
cs-410_6_4_73,"00:05:40,550","00:05:44,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,the effect is that a long document would
cs-410_6_4_74,"00:05:46,090","00:05:49,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,Because a long document
cs-410_6_4_75,"00:05:49,200","00:05:53,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,therefore the coefficient
cs-410_6_4_76,"00:05:53,140","00:05:59,949",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,And so a long document would have
cs-410_6_4_77,"00:05:59,949","00:06:05,734",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,So this seems to make more sense
cs-410_6_4_78,"00:06:05,734","00:06:08,979",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,"Of course,"
cs-410_6_4_79,"00:06:08,979","00:06:12,156",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,that the two coefficients would sum to 1.
cs-410_6_4_80,"00:06:12,156","00:06:16,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,Now this is one way to
cs-410_6_4_81,"00:06:16,400","00:06:21,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"Basically, it means it's a dynamic"
cs-410_6_4_82,"00:06:22,790","00:06:27,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,There is another way to understand
cs-410_6_4_83,"00:06:27,737","00:06:31,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"easier to remember, and"
cs-410_6_4_84,"00:06:33,310","00:06:38,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,So it's easier to see how we can rewrite
cs-410_6_4_85,"00:06:38,878","00:06:42,847",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,Now in this form we can easily
cs-410_6_4_86,"00:06:42,847","00:06:47,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"the maximum likelihood estimate,"
cs-410_6_4_87,"00:06:47,060","00:06:53,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,So normalize the count
cs-410_6_4_88,"00:06:53,346","00:07:00,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,So in this form we can see what we did is
cs-410_6_4_89,"00:07:01,800","00:07:03,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,So what does this mean?
cs-410_6_4_90,"00:07:03,230","00:07:08,042",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"Well, this is basically something related"
cs-410_6_4_91,"00:07:08,042","00:07:09,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,the collection.
cs-410_6_4_92,"00:07:10,390","00:07:13,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,And we multiply that by the parameter mu.
cs-410_6_4_93,"00:07:14,510","00:07:18,577",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,And when we combine this
cs-410_6_4_94,"00:07:18,577","00:07:24,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,essentially we are adding
cs-410_6_4_95,"00:07:24,265","00:07:31,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,We pretend every word has
cs-410_6_4_96,"00:07:31,090","00:07:35,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,So the total count would be
cs-410_6_4_97,"00:07:35,290","00:07:38,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,the actual count of
cs-410_6_4_98,"00:07:39,950","00:07:46,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"As a result, in total we would"
cs-410_6_4_99,"00:07:46,020","00:07:49,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,Why?
cs-410_6_4_100,"00:07:50,770","00:07:55,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"over all the words, then we'll see the"
cs-410_6_4_101,"00:07:55,480","00:07:57,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,and that gives us just mu.
cs-410_6_4_102,"00:07:57,380","00:08:00,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So this is the total number of
cs-410_6_4_103,"00:08:01,550","00:08:05,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,And so
cs-410_6_4_104,"00:08:05,270","00:08:12,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"So in this case, we can easily"
cs-410_6_4_105,"00:08:13,920","00:08:18,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,add this as a pseudocount to this data.
cs-410_6_4_106,"00:08:18,130","00:08:22,877",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Pretend we actually augment the data
cs-410_6_4_107,"00:08:22,877","00:08:26,022",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,defined by the collection language model.
cs-410_6_4_108,"00:08:26,022","00:08:30,201",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"As a result, we have more counts is that"
cs-410_6_4_109,"00:08:30,201","00:08:35,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,the total counts for
cs-410_6_4_110,"00:08:35,710","00:08:41,499",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,"And as a result, even if a word has zero"
cs-410_6_4_111,"00:08:41,499","00:08:47,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,"count here, then it would still have"
cs-410_6_4_112,"00:08:47,115","00:08:49,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So this is how this method works.
cs-410_6_4_113,"00:08:49,750","00:08:52,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,Let's also take a look at
cs-410_6_4_114,"00:08:52,650","00:08:58,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,So for text again we will
cs-410_6_4_115,"00:08:58,580","00:09:03,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"that we actually observe, but"
cs-410_6_4_116,"00:09:03,000","00:09:05,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,And so the probability of
cs-410_6_4_117,"00:09:05,725","00:09:11,051",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"Naturally, the probability of"
cs-410_6_4_118,"00:09:11,051","00:09:14,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And so here you can also see
cs-410_6_4_119,"00:09:15,600","00:09:16,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Can you see it?
cs-410_6_4_120,"00:09:16,850","00:09:19,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,"If you want to think about it,"
cs-410_6_4_121,"00:09:20,590","00:09:25,618",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,But you'll notice that this
cs-410_6_4_122,"00:09:25,618","00:09:29,122",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"So we can see, in this case,"
cs-410_6_4_123,"00:09:29,122","00:09:34,089",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,"alpha sub d does depend on the document,"
cs-410_6_4_124,"00:09:34,089","00:09:39,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,because this length
cs-410_6_4_125,"00:09:39,787","00:09:44,609",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"whereas in the linear interpolation,"
cs-410_6_4_126,"00:09:44,609","00:09:50,622",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,"the J-M smoothing method,"
cs-410_6_4_127,"00:09:50,622","00:09:54,919",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,[MUSIC]
cs-410_5_4_1,"00:00:00,005","00:00:03,962",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_4_2,"00:00:12,779","00:00:15,641",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,So I showed you how we rewrite the query
cs-410_5_4_3,"00:00:15,641","00:00:20,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,like holder which is a function into
cs-410_5_4_4,"00:00:20,830","00:00:25,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,of this slide after if we make
cs-410_5_4_5,"00:00:25,840","00:00:30,426",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,the language model based on
cs-410_5_4_6,"00:00:30,426","00:00:36,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"Now if you look at this rewriting,"
cs-410_5_4_7,"00:00:36,160","00:00:42,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,The first benefit is it helps us better
cs-410_5_4_8,"00:00:42,470","00:00:47,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,"In particular, we're going to show that"
cs-410_5_4_9,"00:00:47,050","00:00:51,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,with the collection language model would
cs-410_5_4_10,"00:00:51,340","00:00:52,412",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,and length normalization.
cs-410_5_4_11,"00:00:52,412","00:00:57,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,The second benefit is that
cs-410_5_4_12,"00:00:57,645","00:01:02,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,the query like holder more efficiently.
cs-410_5_4_13,"00:01:02,940","00:01:06,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,In particular we see that
cs-410_5_4_14,"00:01:06,020","00:01:07,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,is a sum over the match
cs-410_5_4_15,"00:01:09,670","00:01:14,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,So this is much better than if we
cs-410_5_4_16,"00:01:14,910","00:01:20,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,After we smooth the document the damage
cs-410_5_4_17,"00:01:20,257","00:01:21,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,for all the words.
cs-410_5_4_18,"00:01:21,400","00:01:25,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,So this new form of the formula is
cs-410_5_4_19,"00:01:27,580","00:01:29,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,It's also interesting to note that
cs-410_5_4_20,"00:01:29,850","00:01:34,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,the last term here is actually
cs-410_5_4_21,"00:01:34,420","00:01:36,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,Since our goal is to
cs-410_5_4_22,"00:01:36,610","00:01:40,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,the same query we can ignore this term for
cs-410_5_4_23,"00:01:40,610","00:01:43,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,Because it's going to be the same for
cs-410_5_4_24,"00:01:43,650","00:01:46,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,Ignoring it wouldn't affect
cs-410_5_4_25,"00:01:49,070","00:01:51,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"Inside the sum, we"
cs-410_5_4_26,"00:01:52,940","00:01:57,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,also see that each matched query
cs-410_5_4_27,"00:01:58,510","00:02:01,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,And this weight actually
cs-410_5_4_28,"00:02:01,990","00:02:07,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,is very interesting because it
cs-410_5_4_29,"00:02:07,070","00:02:11,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,First we can already see it has
cs-410_5_4_30,"00:02:11,830","00:02:14,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,just like in the vector space model.
cs-410_5_4_31,"00:02:14,250","00:02:16,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"When we take a thought product,"
cs-410_5_4_32,"00:02:16,240","00:02:20,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,we see the word frequency in
cs-410_5_4_33,"00:02:22,250","00:02:27,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,And so naturally this part would
cs-410_5_4_34,"00:02:27,670","00:02:31,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,element from the documented vector.
cs-410_5_4_35,"00:02:31,510","00:02:34,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,And here indeed we can see it actually
cs-410_5_4_36,"00:02:35,430","00:02:39,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,encodes a weight that has similar
cs-410_5_4_37,"00:02:41,160","00:02:43,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"I'll let you examine it, can you see it?"
cs-410_5_4_38,"00:02:43,660","00:02:46,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,Can you see which part is capturing TF?
cs-410_5_4_39,"00:02:46,110","00:02:49,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And which part is
cs-410_5_4_40,"00:02:51,680","00:02:54,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,So if want you can pause
cs-410_5_4_41,"00:02:55,830","00:03:02,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So have you noticed that this P sub
cs-410_5_4_42,"00:03:02,640","00:03:08,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,in the sense that if a word occurs
cs-410_5_4_43,"00:03:08,240","00:03:11,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,then the s made through probability
cs-410_5_4_44,"00:03:11,980","00:03:17,694",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,So this means this term is really
cs-410_5_4_45,"00:03:17,694","00:03:22,324",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,Now have you also noticed that
cs-410_5_4_46,"00:03:22,324","00:03:26,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,is actually achieving the factor of IDF?
cs-410_5_4_47,"00:03:26,090","00:03:29,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"Why, because this is the popularity"
cs-410_5_4_48,"00:03:31,750","00:03:37,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"But it's in the denominator, so if the"
cs-410_5_4_49,"00:03:37,110","00:03:39,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,then the weight is actually smaller.
cs-410_5_4_50,"00:03:39,700","00:03:41,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,And this means a popular term.
cs-410_5_4_51,"00:03:41,790","00:03:45,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,We actually have a smaller weight and this
cs-410_5_4_52,"00:03:47,040","00:03:50,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,Only that we now have
cs-410_5_4_53,"00:03:51,550","00:03:55,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,Remember IDF has a logarithm
cs-410_5_4_54,"00:03:55,920","00:03:57,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,But here we have something different.
cs-410_5_4_55,"00:03:58,300","00:04:02,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,But intuitively it
cs-410_5_4_56,"00:04:02,460","00:04:06,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"Interestingly, we also have something"
cs-410_5_4_57,"00:04:07,820","00:04:13,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"Again, can you see which factor is related"
cs-410_5_4_58,"00:04:14,790","00:04:18,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,What I just say is that this term
cs-410_5_4_59,"00:04:19,560","00:04:24,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,"This collection probability,"
cs-410_5_4_60,"00:04:24,700","00:04:29,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,this term here is actually related
cs-410_5_4_61,"00:04:29,360","00:04:35,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"In particular, F of sub d might"
cs-410_5_4_62,"00:04:35,110","00:04:40,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,So it encodes how much probability
cs-410_5_4_63,"00:04:41,740","00:04:43,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,How much smoothing do we want to do?
cs-410_5_4_64,"00:04:43,700","00:04:46,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,"Intuitively, if a document is long,"
cs-410_5_4_65,"00:04:46,470","00:04:50,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,then we need to do less smoothing because
cs-410_5_4_66,"00:04:50,980","00:04:55,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,We probably have observed all the words
cs-410_5_4_67,"00:04:55,720","00:05:00,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,But if the document is short then r of
cs-410_5_4_68,"00:05:00,900","00:05:02,432",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,We need to do more smoothing.
cs-410_5_4_69,"00:05:02,432","00:05:06,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,It's likey there are words that have
cs-410_5_4_70,"00:05:06,110","00:05:12,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,So this term appears to paralyze
cs-410_5_4_71,"00:05:12,250","00:05:19,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,other sub D would tend to be longer
cs-410_5_4_72,"00:05:19,100","00:05:23,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,But note that alpha sub d
cs-410_5_4_73,"00:05:23,065","00:05:28,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,this may not actually be necessary
cs-410_5_4_74,"00:05:28,570","00:05:30,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,The effect is not so clear yet.
cs-410_5_4_75,"00:05:31,930","00:05:36,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"But as we will see later, when we"
cs-410_5_4_76,"00:05:36,570","00:05:40,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,it turns out that they do
cs-410_5_4_77,"00:05:40,080","00:05:42,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,Just like in TF-IDF weighting and
cs-410_5_4_78,"00:05:42,730","00:05:45,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,document length normalization
cs-410_5_4_79,"00:05:47,490","00:05:50,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"So, that's a very interesting"
cs-410_5_4_80,"00:05:50,670","00:05:54,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,we don't even have to think about
cs-410_5_4_81,"00:05:54,880","00:05:59,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,We just need to assume that if we smooth
cs-410_5_4_82,"00:05:59,910","00:06:05,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,then we would have a formula that
cs-410_5_4_83,"00:06:05,480","00:06:06,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,documents length violation.
cs-410_5_4_84,"00:06:08,210","00:06:13,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,What's also interesting that we have
cs-410_5_4_85,"00:06:14,180","00:06:17,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,And see we have not heuristically
cs-410_5_4_86,"00:06:19,310","00:06:23,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"In fact, you can think about why"
cs-410_5_4_87,"00:06:23,790","00:06:28,651",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,You look at the assumptions that
cs-410_5_4_88,"00:06:28,651","00:06:33,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,it's because we have used a logarithm
cs-410_5_4_89,"00:06:33,720","00:06:38,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,And we turned the product into a sum
cs-410_5_4_90,"00:06:38,090","00:06:39,239",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,that's why we have this logarithm.
cs-410_5_4_91,"00:06:40,470","00:06:44,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,Note that if only want to heuristically
cs-410_5_4_92,"00:06:44,740","00:06:48,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,"IDF weighting, we don't necessary"
cs-410_5_4_93,"00:06:48,830","00:06:53,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"Imagine if we drop this logarithm,"
cs-410_5_4_94,"00:06:55,010","00:06:59,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,But what's nice with problem risk modeling
cs-410_5_4_95,"00:06:59,740","00:07:01,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,the logarithm function here.
cs-410_5_4_96,"00:07:01,950","00:07:07,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,And that's basically a fixed form
cs-410_5_4_97,"00:07:07,510","00:07:13,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,"really have to heuristically design,"
cs-410_5_4_98,"00:07:13,010","00:07:18,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,the logarithm the model probably won't
cs-410_5_4_99,"00:07:19,400","00:07:24,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,So a nice property of problem risk
cs-410_5_4_100,"00:07:24,300","00:07:28,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,assumptions and the probability rules
cs-410_5_4_101,"00:07:28,260","00:07:33,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,And the formula would have
cs-410_5_4_102,"00:07:34,600","00:07:38,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,And if we heuristically design
cs-410_5_4_103,"00:07:38,540","00:07:40,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,end up having such a specific formula.
cs-410_5_4_104,"00:07:41,700","00:07:46,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"So to summarize, we talked about the need"
cs-410_5_4_105,"00:07:46,940","00:07:52,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,Otherwise it would give zero probability
cs-410_5_4_106,"00:07:52,470","00:07:57,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,that's not good for
cs-410_5_4_107,"00:07:59,370","00:08:03,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"It's also necessary, in general,"
cs-410_5_4_108,"00:08:03,720","00:08:08,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,the model represent
cs-410_5_4_109,"00:08:08,730","00:08:16,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,The general idea of smoothing in retrieval
cs-410_5_4_110,"00:08:17,800","00:08:22,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,to give us some clue about which unseen
cs-410_5_4_111,"00:08:22,760","00:08:26,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"That is, the probability of an unseen"
cs-410_5_4_112,"00:08:26,840","00:08:28,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,to its probability in the collection.
cs-410_5_4_113,"00:08:29,610","00:08:34,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"With this assumption, we've shown that we"
cs-410_5_4_114,"00:08:34,330","00:08:38,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,query likelihood that has
cs-410_5_4_115,"00:08:38,280","00:08:39,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,document length normalization.
cs-410_5_4_116,"00:08:39,970","00:08:42,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"We also see that, through some rewriting,"
cs-410_5_4_117,"00:08:42,210","00:08:47,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,the scoring of such a ranking function
cs-410_5_4_118,"00:08:47,080","00:08:50,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"matched query terms,"
cs-410_5_4_119,"00:08:50,530","00:08:54,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"But, the actual ranking"
cs-410_5_4_120,"00:08:54,500","00:08:59,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,automatically by the probability rules and
cs-410_5_4_121,"00:08:59,010","00:09:02,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,And like in the vector space model
cs-410_5_4_122,"00:09:02,210","00:09:04,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,think about the form of the function.
cs-410_5_4_123,"00:09:04,580","00:09:09,234",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"However, we still need to address"
cs-410_5_4_124,"00:09:09,234","00:09:11,652",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,smooth the document and the model.
cs-410_5_4_125,"00:09:11,652","00:09:14,859",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,How exactly we should
cs-410_5_4_126,"00:09:14,859","00:09:19,223",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,model based on the connection
cs-410_5_4_127,"00:09:19,223","00:09:24,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,the maximum micro is made of and
cs-410_5_4_128,"00:09:24,226","00:09:34,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,[MUSIC]
cs-410_3_4_1,"00:00:00,012","00:00:03,532",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_4_2,"00:00:07,767","00:00:10,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,"This lecture is about query likelihood,"
cs-410_3_4_3,"00:00:10,058","00:00:11,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,probabilistic retrieval model.
cs-410_3_4_4,"00:00:14,040","00:00:15,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture,"
cs-410_3_4_5,"00:00:15,310","00:00:19,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,we continue the discussion of
cs-410_3_4_6,"00:00:19,190","00:00:22,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In particular, we're going to talk about"
cs-410_3_4_7,"00:00:25,870","00:00:31,073",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"In the query light holder retrieval model,"
cs-410_3_4_8,"00:00:31,073","00:00:35,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,How like their user who likes a document
cs-410_3_4_9,"00:00:36,990","00:00:41,462",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,"So in this case,"
cs-410_3_4_10,"00:00:41,462","00:00:46,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,particular document about
cs-410_3_4_11,"00:00:46,663","00:00:50,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,"Now we assume,"
cs-410_3_4_12,"00:00:50,410","00:00:54,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,a basis to impose a query to try and
cs-410_3_4_13,"00:00:57,340","00:01:03,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,"So again, imagine use a process"
cs-410_3_4_14,"00:01:03,840","00:01:06,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,Where we assume that
cs-410_3_4_15,"00:01:06,940","00:01:08,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,assembling words from the document.
cs-410_3_4_16,"00:01:10,560","00:01:15,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"So for example, a user might"
cs-410_3_4_17,"00:01:15,880","00:01:19,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,from this document and
cs-410_3_4_18,"00:01:20,600","00:01:24,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,And then the user would pick
cs-410_3_4_19,"00:01:24,590","00:01:25,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,that would be the second query word.
cs-410_3_4_20,"00:01:27,420","00:01:32,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,Now this of course is an assumption
cs-410_3_4_21,"00:01:32,400","00:01:35,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,how a user would pose a query.
cs-410_3_4_22,"00:01:35,008","00:01:39,788",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,Whether a user actually followed this
cs-410_3_4_23,"00:01:39,788","00:01:45,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,this assumption has allowed us to formerly
cs-410_3_4_24,"00:01:46,390","00:01:50,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,And this allows us to also not rely on
cs-410_3_4_25,"00:01:52,580","00:01:55,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,to use empirical data to
cs-410_3_4_26,"00:01:56,870","00:02:00,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And this is why we can use this
cs-410_3_4_27,"00:02:00,820","00:02:03,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,retrieval function that we can
cs-410_3_4_28,"00:02:04,900","00:02:08,991",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,So as you see the assumption
cs-410_3_4_29,"00:02:08,991","00:02:11,558",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,word is independent of the sample.
cs-410_3_4_30,"00:02:11,558","00:02:17,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,And also each word is basically
cs-410_3_4_31,"00:02:20,910","00:02:24,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,So now let's see how this works exactly.
cs-410_3_4_32,"00:02:24,540","00:02:28,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"Well, since we are completing"
cs-410_3_4_33,"00:02:29,730","00:02:34,444",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,then the probability here is just
cs-410_3_4_34,"00:02:34,444","00:02:37,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,which is a sequence of words.
cs-410_3_4_35,"00:02:37,210","00:02:42,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,And we make the assumption that each
cs-410_3_4_36,"00:02:42,140","00:02:46,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,"So as a result, the probability"
cs-410_3_4_37,"00:02:46,670","00:02:48,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,of the probability of each query word.
cs-410_3_4_38,"00:02:50,100","00:02:52,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,Now how do we compute
cs-410_3_4_39,"00:02:52,660","00:02:56,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"Well, based on the assumption that a word"
cs-410_3_4_40,"00:02:56,740","00:03:01,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,is picked from the document
cs-410_3_4_41,"00:03:01,360","00:03:05,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,Now we know the probability of each word
cs-410_3_4_42,"00:03:05,680","00:03:08,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,word in the document.
cs-410_3_4_43,"00:03:08,120","00:03:13,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"So for example, the probability of"
cs-410_3_4_44,"00:03:13,780","00:03:17,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,Would be just the count
cs-410_3_4_45,"00:03:17,520","00:03:23,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,divided by the total number of words
cs-410_3_4_46,"00:03:23,060","00:03:28,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,So with these assumptions we now have
cs-410_3_4_47,"00:03:28,940","00:03:30,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,We can use this to rank our documents.
cs-410_3_4_48,"00:03:32,650","00:03:34,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,So does this model work?
cs-410_3_4_49,"00:03:34,200","00:03:35,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,Let's take a look.
cs-410_3_4_50,"00:03:35,260","00:03:38,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,Here are some example documents
cs-410_3_4_51,"00:03:38,670","00:03:42,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,Suppose now the query is
cs-410_3_4_52,"00:03:42,210","00:03:44,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,we see the formula here on the top.
cs-410_3_4_53,"00:03:45,900","00:03:47,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So how do we score this document?
cs-410_3_4_54,"00:03:47,490","00:03:48,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"Well, it's very simple."
cs-410_3_4_55,"00:03:48,790","00:03:51,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,We just count how many times do
cs-410_3_4_56,"00:03:51,370","00:03:54,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"how many times do we have seen campaigns,"
cs-410_3_4_57,"00:03:54,380","00:03:57,458",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"And we see here 44, and"
cs-410_3_4_58,"00:03:57,458","00:04:02,297",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,So that's 2 over the length of
cs-410_3_4_59,"00:04:02,297","00:04:07,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,the length of document 4 for
cs-410_3_4_60,"00:04:07,710","00:04:11,146",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"And similarly, we can get probabilities"
cs-410_3_4_61,"00:04:13,189","00:04:17,505",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,Now if you look at these numbers or
cs-410_3_4_62,"00:04:17,505","00:04:22,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,"scoring all these documents,"
cs-410_3_4_63,"00:04:22,030","00:04:28,436",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,Because if we assume d3 and
cs-410_3_4_64,"00:04:28,436","00:04:35,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,then looks like a nominal rank d4
cs-410_3_4_65,"00:04:35,628","00:04:40,819",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"And as we would expect,"
cs-410_3_4_66,"00:04:40,819","00:04:45,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"a TF query state, and so"
cs-410_3_4_67,"00:04:45,916","00:04:50,096",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"However, if we try a different"
cs-410_3_4_68,"00:04:50,096","00:04:54,854",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,presidential campaign update
cs-410_3_4_69,"00:04:54,854","00:04:56,608",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,Well what problem?
cs-410_3_4_70,"00:04:56,608","00:04:58,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,Well think about the update.
cs-410_3_4_71,"00:04:58,930","00:05:02,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,Now none of these documents
cs-410_3_4_72,"00:05:02,500","00:05:08,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,So according to our assumption that a user
cs-410_3_4_73,"00:05:08,420","00:05:15,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"generate a query, then the probability of"
cs-410_3_4_74,"00:05:15,003","00:05:16,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,Would be 0.
cs-410_3_4_75,"00:05:17,230","00:05:21,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,"So that causes a problem,"
cs-410_3_4_76,"00:05:21,380","00:05:23,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,to have zero probability
cs-410_3_4_77,"00:05:25,330","00:05:31,127",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,Now why it's fine to have zero probability
cs-410_3_4_78,"00:05:31,127","00:05:33,902",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,It's not okay to have 0 for d3 and
cs-410_3_4_79,"00:05:33,902","00:05:38,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,d4 because now we no longer
cs-410_3_4_80,"00:05:38,600","00:05:39,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,What's worse?
cs-410_3_4_81,"00:05:39,135","00:05:41,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,We can't even distinguish them from d2.
cs-410_3_4_82,"00:05:41,735","00:05:45,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,So that's obviously not desirable.
cs-410_3_4_83,"00:05:45,700","00:05:48,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"Now when a [INAUDIBLE] has such result,"
cs-410_3_4_84,"00:05:48,630","00:05:50,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,we should think about what
cs-410_3_4_85,"00:05:52,530","00:05:56,773",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,So we have to examine what
cs-410_3_4_86,"00:05:56,773","00:05:59,644",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,as we derive this ranking function.
cs-410_3_4_87,"00:05:59,644","00:06:03,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,Now is you examine those assumptions
cs-410_3_4_88,"00:06:03,285","00:06:04,983",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,what has caused this problem?
cs-410_3_4_89,"00:06:04,983","00:06:09,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,So take a moment to think about it.
cs-410_3_4_90,"00:06:09,080","00:06:17,179",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,What do you think is the reason why update
cs-410_3_4_91,"00:06:17,179","00:06:22,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,So if you think about this from the moment
cs-410_3_4_92,"00:06:22,092","00:06:25,317",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,have made an assumption
cs-410_3_4_93,"00:06:25,317","00:06:29,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,be drawn from the document
cs-410_3_4_94,"00:06:29,220","00:06:33,982",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"So in order to fix this, we have to"
cs-410_3_4_95,"00:06:33,982","00:06:36,912",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,a word not necessarily from the document.
cs-410_3_4_96,"00:06:36,912","00:06:38,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,So that's the improved model.
cs-410_3_4_97,"00:06:38,930","00:06:40,912",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"An improvement here is to say that,"
cs-410_3_4_98,"00:06:40,912","00:06:43,687",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,well instead of drawing
cs-410_3_4_99,"00:06:43,687","00:06:48,064",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,let's imagine that the user would actually
cs-410_3_4_100,"00:06:48,064","00:06:50,107",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And so I show a model here.
cs-410_3_4_101,"00:06:50,107","00:06:54,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,And we assume that this document is
cs-410_3_4_102,"00:06:54,479","00:06:55,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,model.
cs-410_3_4_103,"00:06:55,920","00:07:01,297",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"Now, this model doesn't necessarily assign"
cs-410_3_4_104,"00:07:01,297","00:07:05,853",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,we can assume this model does not
cs-410_3_4_105,"00:07:05,853","00:07:09,621",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,Now if we're thinking this way then
cs-410_3_4_106,"00:07:09,621","00:07:10,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,different.
cs-410_3_4_107,"00:07:10,700","00:07:14,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,Now the user has this model in mind
cs-410_3_4_108,"00:07:14,940","00:07:18,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,Although the model has to be
cs-410_3_4_109,"00:07:18,669","00:07:22,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,So the user can again generate
cs-410_3_4_110,"00:07:22,960","00:07:27,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"Namely, pick a word for example,"
cs-410_3_4_111,"00:07:29,020","00:07:32,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,Now the difference is that this time
cs-410_3_4_112,"00:07:32,390","00:07:34,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,even though update doesn't
cs-410_3_4_113,"00:07:34,930","00:07:38,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,to potentially generate
cs-410_3_4_114,"00:07:38,050","00:07:43,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,So that a query was updated
cs-410_3_4_115,"00:07:43,840","00:07:45,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,So this would fix our problem.
cs-410_3_4_116,"00:07:45,720","00:07:50,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,And it's also reasonable because when our
cs-410_3_4_117,"00:07:50,140","00:07:55,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"in a more general way, that is unique"
cs-410_3_4_118,"00:07:55,160","00:07:57,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,So how do we compute
cs-410_3_4_119,"00:07:57,830","00:08:01,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,If we make this sum wide
cs-410_3_4_120,"00:08:01,000","00:08:07,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"The first one is compute this model, and"
cs-410_3_4_121,"00:08:07,390","00:08:15,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"For example, I've shown two pulse models"
cs-410_3_4_122,"00:08:15,070","00:08:19,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,And then given a query like a data mining
cs-410_3_4_123,"00:08:19,803","00:08:22,467",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,just compute the likelihood of this query.
cs-410_3_4_124,"00:08:22,467","00:08:26,574",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And by making independence
cs-410_3_4_125,"00:08:26,574","00:08:30,766",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,probability as a product of
cs-410_3_4_126,"00:08:30,766","00:08:34,798",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"We do this for both documents, and"
cs-410_3_4_127,"00:08:34,798","00:08:35,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,then rank them.
cs-410_3_4_128,"00:08:37,160","00:08:41,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,So that's the basic idea of this
cs-410_3_4_129,"00:08:41,310","00:08:47,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So more generally this ranking function
cs-410_3_4_130,"00:08:47,890","00:08:51,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Here we assume that the query has n words,"
cs-410_3_4_131,"00:08:51,800","00:08:56,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,"w1 through wn, and"
cs-410_3_4_132,"00:08:56,540","00:09:01,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,The ranking function is the probability
cs-410_3_4_133,"00:09:01,500","00:09:06,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,given that the user is
cs-410_3_4_134,"00:09:06,080","00:09:11,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,And this is assume it will be product of
cs-410_3_4_135,"00:09:11,970","00:09:15,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,This is based on independent assumption.
cs-410_3_4_136,"00:09:15,360","00:09:20,256",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Now we actually often score
cs-410_3_4_137,"00:09:20,256","00:09:25,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,using log of the query likelihood
cs-410_3_4_138,"00:09:26,710","00:09:30,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,Now we do this to avoid
cs-410_3_4_139,"00:09:30,220","00:09:35,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"having a lot of small probabilities,"
cs-410_3_4_140,"00:09:35,830","00:09:41,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,And this could cause under flow and we
cs-410_3_4_141,"00:09:41,060","00:09:44,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,the value in our algorithm function.
cs-410_3_4_142,"00:09:44,100","00:09:51,079",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,We maintain the order of these documents
cs-410_3_4_143,"00:09:51,079","00:09:54,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,And so if we take longer than
cs-410_3_4_144,"00:09:54,935","00:09:59,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,the product would become a sum
cs-410_3_4_145,"00:09:59,920","00:10:03,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,So the sum of all the query
cs-410_3_4_146,"00:10:03,620","00:10:07,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,that is one of the probability of
cs-410_3_4_147,"00:10:09,360","00:10:13,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,And then we can further rewrite
cs-410_3_4_148,"00:10:14,310","00:10:19,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"So in the first sum here, in this sum,"
cs-410_3_4_149,"00:10:21,910","00:10:28,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,we have it over all the query words and
cs-410_3_4_150,"00:10:28,800","00:10:33,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,And in this sum we have a sum
cs-410_3_4_151,"00:10:33,030","00:10:37,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,But we put a counter here
cs-410_3_4_152,"00:10:37,050","00:10:39,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,Essentially we are only considering
cs-410_3_4_153,"00:10:39,780","00:10:43,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,"because if a word is not in the query,"
cs-410_3_4_154,"00:10:43,570","00:10:46,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,So we're still considering
cs-410_3_4_155,"00:10:46,820","00:10:49,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,But we're using a different form as
cs-410_3_4_156,"00:10:49,760","00:10:51,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,all the words in the vocabulary.
cs-410_3_4_157,"00:10:52,960","00:10:56,435",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,"And of course, a word might occur"
cs-410_3_4_158,"00:10:56,435","00:10:58,631",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,That's why we have a count here.
cs-410_3_4_159,"00:11:00,407","00:11:04,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,And then this part is log of
cs-410_3_4_160,"00:11:04,168","00:11:06,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,given by the document language model.
cs-410_3_4_161,"00:11:08,647","00:11:11,497",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"So you can see in this retrieval function,"
cs-410_3_4_162,"00:11:11,497","00:11:13,547",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,we actually know the count
cs-410_3_4_163,"00:11:13,547","00:11:16,507",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,So the only thing that we don't know
cs-410_3_4_164,"00:11:17,817","00:11:21,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,"Therefore, we have converted"
cs-410_3_4_165,"00:11:21,310","00:11:24,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,include the problem of estimating
cs-410_3_4_166,"00:11:25,920","00:11:30,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,So that we can compute the probability of
cs-410_3_4_167,"00:11:32,260","00:11:36,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,And different estimation methods would
cs-410_3_4_168,"00:11:36,630","00:11:40,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,This is just like a different way to
cs-410_3_4_169,"00:11:40,980","00:11:45,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,which leads to a different ranking
cs-410_3_4_170,"00:11:45,485","00:11:49,353",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,Here different ways to
cs-410_3_4_171,"00:11:49,353","00:11:54,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,a different ranking function for
cs-410_3_4_172,"00:11:54,065","00:12:04,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,[MUSIC]
cs-410_2_4_1,"00:00:07,780","00:00:12,596",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,[SOUND] This lecture is about
cs-410_2_4_2,"00:00:12,596","00:00:13,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture,"
cs-410_2_4_3,"00:00:13,737","00:00:18,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,we're going to give an introduction
cs-410_2_4_4,"00:00:18,445","00:00:23,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,This has to do with how do you model
cs-410_2_4_5,"00:00:23,305","00:00:28,272",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So it's related to how we model
cs-410_2_4_6,"00:00:31,828","00:00:34,032",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,We're going to talk about
cs-410_2_4_7,"00:00:34,032","00:00:37,688",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,And then we're going to talk about the
cs-410_2_4_8,"00:00:37,688","00:00:42,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"language model, which also happens to be"
cs-410_2_4_9,"00:00:42,770","00:00:45,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,"And finally, what this class"
cs-410_2_4_10,"00:00:47,200","00:00:48,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,What is a language model?
cs-410_2_4_11,"00:00:48,750","00:00:53,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"Well, it's just a probability"
cs-410_2_4_12,"00:00:53,570","00:00:54,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"So here, I'll show one."
cs-410_2_4_13,"00:00:55,870","00:01:00,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,This model gives the sequence Today
cs-410_2_4_14,"00:01:00,430","00:01:03,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"It give Today Wednesday is a very,"
cs-410_2_4_15,"00:01:03,830","00:01:09,705",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,very small probability
cs-410_2_4_16,"00:01:11,796","00:01:15,447",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,You can see the probabilities
cs-410_2_4_17,"00:01:15,447","00:01:19,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,sequences of words can vary
cs-410_2_4_18,"00:01:19,670","00:01:23,256",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"Therefore, it's clearly context dependent."
cs-410_2_4_19,"00:01:23,256","00:01:24,552",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"In ordinary conversation,"
cs-410_2_4_20,"00:01:24,552","00:01:28,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,probably Today is Wednesday is most
cs-410_2_4_21,"00:01:28,510","00:01:32,132",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Imagine in the context of
cs-410_2_4_22,"00:01:32,132","00:01:36,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"maybe the eigenvalue is positive,"
cs-410_2_4_23,"00:01:36,890","00:01:41,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,This means it can be used to
cs-410_2_4_24,"00:01:42,240","00:01:45,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,The model can also be regarded
cs-410_2_4_25,"00:01:45,900","00:01:46,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,generating text.
cs-410_2_4_26,"00:01:46,950","00:01:51,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,And this is why it's also often
cs-410_2_4_27,"00:01:51,660","00:01:52,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,So what does that mean?
cs-410_2_4_28,"00:01:52,910","00:01:58,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,We can imagine this is a mechanism that's
cs-410_2_4_29,"00:01:58,540","00:02:05,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,visualised here as a stochastic system
cs-410_2_4_30,"00:02:05,340","00:02:08,608",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So, we can ask for a sequence,"
cs-410_2_4_31,"00:02:08,608","00:02:13,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"a sequence from the device if you want,"
cs-410_2_4_32,"00:02:13,548","00:02:18,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,"Today is Wednesday, but it could"
cs-410_2_4_33,"00:02:18,420","00:02:21,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"So for example,"
cs-410_2_4_34,"00:02:24,086","00:02:28,418",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"So in this sense,"
cs-410_2_4_35,"00:02:28,418","00:02:32,656",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,a sample observed from
cs-410_2_4_36,"00:02:32,656","00:02:33,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"So, why is such a model useful?"
cs-410_2_4_37,"00:02:33,840","00:02:39,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,"Well, it's mainly because it can quantify"
cs-410_2_4_38,"00:02:39,720","00:02:41,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,Where do uncertainties come from?
cs-410_2_4_39,"00:02:41,190","00:02:45,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"Well, one source is simply"
cs-410_2_4_40,"00:02:45,690","00:02:48,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,that we discussed earlier in the lecture.
cs-410_2_4_41,"00:02:48,870","00:02:52,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,Another source is because we don't
cs-410_2_4_42,"00:02:52,240","00:02:55,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,we lack all the knowledge
cs-410_2_4_43,"00:02:55,300","00:02:58,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"In that case,"
cs-410_2_4_44,"00:02:58,420","00:03:01,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,So let me show some examples of questions
cs-410_2_4_45,"00:03:01,800","00:03:06,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,that would have interesting
cs-410_2_4_46,"00:03:06,220","00:03:11,641",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"Given that we see John and feels,"
cs-410_2_4_47,"00:03:11,641","00:03:16,866",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,as opposed to habit as the next
cs-410_2_4_48,"00:03:16,866","00:03:21,123",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"Now, obviously, this would be very useful"
cs-410_2_4_49,"00:03:21,123","00:03:25,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,"habit would have similar acoustic sound,"
cs-410_2_4_50,"00:03:25,180","00:03:28,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,"But, if we look at the language model,"
cs-410_2_4_51,"00:03:28,190","00:03:32,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,we know that John feels happy would be
cs-410_2_4_52,"00:03:35,810","00:03:39,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Another example, given that we"
cs-410_2_4_53,"00:03:39,300","00:03:43,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"game once in a news article,"
cs-410_2_4_54,"00:03:43,700","00:03:47,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,This obviously is related to text
cs-410_2_4_55,"00:03:48,720","00:03:52,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"Also, given that a user is"
cs-410_2_4_56,"00:03:52,150","00:03:55,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,how likely would the user
cs-410_2_4_57,"00:03:55,570","00:03:58,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"Now, this is clearly related"
cs-410_2_4_58,"00:03:58,530","00:04:00,185",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,that we discussed in the previous lecture.
cs-410_2_4_59,"00:04:02,180","00:04:05,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"So now,"
cs-410_2_4_60,"00:04:05,710","00:04:07,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,called a unigram language model.
cs-410_2_4_61,"00:04:07,910","00:04:09,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"In such a case,"
cs-410_2_4_62,"00:04:09,690","00:04:13,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,we assume that we generate a text by
cs-410_2_4_63,"00:04:14,760","00:04:19,356",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,So this means the probability of
cs-410_2_4_64,"00:04:19,356","00:04:22,601",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,the product of
cs-410_2_4_65,"00:04:22,601","00:04:25,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"Now normally,"
cs-410_2_4_66,"00:04:25,800","00:04:30,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,So if you have single word in like
cs-410_2_4_67,"00:04:30,270","00:04:35,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,likely to observe model than if
cs-410_2_4_68,"00:04:35,470","00:04:37,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,So this assumption is not
cs-410_2_4_69,"00:04:37,780","00:04:39,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,we make this assumption
cs-410_2_4_70,"00:04:41,210","00:04:47,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So now the model has precisely N
cs-410_2_4_71,"00:04:47,060","00:04:51,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"We have one probability for each word, and"
cs-410_2_4_72,"00:04:51,380","00:04:57,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,"So strictly speaking,"
cs-410_2_4_73,"00:05:00,270","00:05:04,495",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"As I said,"
cs-410_2_4_74,"00:05:04,495","00:05:06,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,drawn from this word distribution.
cs-410_2_4_75,"00:05:08,080","00:05:11,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"So for example,"
cs-410_2_4_76,"00:05:11,540","00:05:18,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,the model to stochastically generate
cs-410_2_4_77,"00:05:18,020","00:05:19,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"So instead of giving a whole sequence,"
cs-410_2_4_78,"00:05:19,988","00:05:23,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"like Today is Wednesday,"
cs-410_2_4_79,"00:05:23,900","00:05:26,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,And we can get all kinds of words.
cs-410_2_4_80,"00:05:26,200","00:05:28,596",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,And we can assemble these
cs-410_2_4_81,"00:05:28,596","00:05:32,304",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,So that will still allow you
cs-410_2_4_82,"00:05:32,304","00:05:36,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,Today is Wednesday as the product
cs-410_2_4_83,"00:05:37,420","00:05:43,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"As you can see, even though we have not"
cs-410_2_4_84,"00:05:43,380","00:05:48,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,it actually allows us to compute
cs-410_2_4_85,"00:05:48,630","00:05:53,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,this model now only needs N
cs-410_2_4_86,"00:05:53,550","00:05:56,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,That means if we specify
cs-410_2_4_87,"00:05:56,370","00:06:01,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"all the words, then the model's"
cs-410_2_4_88,"00:06:01,850","00:06:06,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"Whereas if we don't make this assumption,"
cs-410_2_4_89,"00:06:06,220","00:06:09,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,all kinds of combinations
cs-410_2_4_90,"00:06:11,830","00:06:16,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"So by making this assumption, it makes it"
cs-410_2_4_91,"00:06:16,720","00:06:18,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,So let's see a specific example here.
cs-410_2_4_92,"00:06:19,810","00:06:25,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,Here I show two unigram language
cs-410_2_4_93,"00:06:25,450","00:06:28,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,And these are high probability
cs-410_2_4_94,"00:06:29,800","00:06:33,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,The first one clearly suggests
cs-410_2_4_95,"00:06:33,290","00:06:37,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,because the high probability
cs-410_2_4_96,"00:06:37,020","00:06:38,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,The second one is more related to health.
cs-410_2_4_97,"00:06:39,790","00:06:41,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"Now we can ask the question,"
cs-410_2_4_98,"00:06:41,290","00:06:46,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,how likely were observe a particular
cs-410_2_4_99,"00:06:46,520","00:06:49,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,Now suppose we sample
cs-410_2_4_100,"00:06:49,920","00:06:53,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,"Let's say we take the first distribution,"
cs-410_2_4_101,"00:06:53,150","00:06:56,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,What words do you think would be
cs-410_2_4_102,"00:06:56,140","00:06:58,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,maybe mining maybe another word?
cs-410_2_4_103,"00:06:58,280","00:06:58,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"Even food,"
cs-410_2_4_104,"00:06:58,860","00:07:02,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"which has a very small probability,"
cs-410_2_4_105,"00:07:03,880","00:07:06,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"But in general, high probability"
cs-410_2_4_106,"00:07:08,130","00:07:11,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,So we can imagine what general text
cs-410_2_4_107,"00:07:12,230","00:07:14,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"In fact, with small probability,"
cs-410_2_4_108,"00:07:14,630","00:07:19,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,you might be able to actually generate
cs-410_2_4_109,"00:07:19,940","00:07:23,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,"Now, it will actually be meaningful,"
cs-410_2_4_110,"00:07:23,660","00:07:24,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,very small.
cs-410_2_4_111,"00:07:26,100","00:07:30,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"In an extreme case, you might"
cs-410_2_4_112,"00:07:30,220","00:07:35,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,a text mining paper that would be
cs-410_2_4_113,"00:07:35,980","00:07:39,866",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"And in that case,"
cs-410_2_4_114,"00:07:39,866","00:07:42,152",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"But it's a non-zero probability,"
cs-410_2_4_115,"00:07:42,152","00:07:45,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,if we assume none of the words
cs-410_2_4_116,"00:07:47,430","00:07:49,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Similarly from the second topic,"
cs-410_2_4_117,"00:07:49,380","00:07:52,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,we can imagine we can generate
cs-410_2_4_118,"00:07:52,660","00:07:58,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,That doesn't mean we cannot generate this
cs-410_2_4_119,"00:07:59,650","00:08:05,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"We can, but the probability would be very,"
cs-410_2_4_120,"00:08:05,030","00:08:09,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,generating a paper that can be accepted
cs-410_2_4_121,"00:08:10,400","00:08:12,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,So the point is that
cs-410_2_4_122,"00:08:13,590","00:08:18,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,we can talk about the probability of
cs-410_2_4_123,"00:08:18,410","00:08:20,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Some texts will have higher
cs-410_2_4_124,"00:08:21,800","00:08:23,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,Now let's look at the problem
cs-410_2_4_125,"00:08:23,900","00:08:28,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,Suppose we now have available
cs-410_2_4_126,"00:08:28,260","00:08:31,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"In this case, many of the abstract or"
cs-410_2_4_127,"00:08:31,960","00:08:34,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,we see these word counts here.
cs-410_2_4_128,"00:08:34,350","00:08:36,846",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,The total number of words is 100.
cs-410_2_4_129,"00:08:36,846","00:08:39,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Now the question you ask here
cs-410_2_4_130,"00:08:39,530","00:08:42,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"We can ask the question which model,"
cs-410_2_4_131,"00:08:42,000","00:08:46,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,which one of these distribution has
cs-410_2_4_132,"00:08:46,340","00:08:50,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,assuming that the text has been generated
cs-410_2_4_133,"00:08:51,970","00:08:53,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,So what would be your guess?
cs-410_2_4_134,"00:08:54,230","00:08:58,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,What we have to decide are what
cs-410_2_4_135,"00:08:58,220","00:08:58,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,would have.
cs-410_2_4_136,"00:09:01,971","00:09:05,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"Suppose the view for a second, and"
cs-410_2_4_137,"00:09:09,616","00:09:14,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"If you're like a lot of people,"
cs-410_2_4_138,"00:09:14,109","00:09:18,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,my best guess is text has a probability
cs-410_2_4_139,"00:09:18,683","00:09:23,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"seen text 10 times, and"
cs-410_2_4_140,"00:09:23,310","00:09:25,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,So we simply normalize these counts.
cs-410_2_4_141,"00:09:27,242","00:09:29,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"And that's in fact the word justified, and"
cs-410_2_4_142,"00:09:29,550","00:09:33,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,your intuition is consistent
cs-410_2_4_143,"00:09:33,650","00:09:36,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,And this is called the maximum
cs-410_2_4_144,"00:09:36,170","00:09:40,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"In this estimator,"
cs-410_2_4_145,"00:09:40,130","00:09:44,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,of those that would give our observe
cs-410_2_4_146,"00:09:44,650","00:09:49,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,That means if we change these
cs-410_2_4_147,"00:09:49,050","00:09:53,319",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,observing the particular text
cs-410_2_4_148,"00:09:55,190","00:09:58,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,"So you can see,"
cs-410_2_4_149,"00:09:58,840","00:10:05,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,"Basically, we just need to look at"
cs-410_2_4_150,"00:10:05,030","00:10:08,987",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,and then divide it by the total number of
cs-410_2_4_151,"00:10:08,987","00:10:11,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,Normalize the frequency.
cs-410_2_4_152,"00:10:11,670","00:10:13,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,"A consequence of this is,"
cs-410_2_4_153,"00:10:13,090","00:10:18,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,"of course, we're going to assign"
cs-410_2_4_154,"00:10:18,200","00:10:19,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"If we have an observed word,"
cs-410_2_4_155,"00:10:19,730","00:10:25,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,there will be no incentive to assign a
cs-410_2_4_156,"00:10:25,300","00:10:26,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,Why?
cs-410_2_4_157,"00:10:26,210","00:10:30,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,Because that would take away probability
cs-410_2_4_158,"00:10:30,840","00:10:33,516",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,And that obviously wouldn't maximize
cs-410_2_4_159,"00:10:33,516","00:10:37,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,the probability of this
cs-410_2_4_160,"00:10:37,430","00:10:42,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,But one has still question whether
cs-410_2_4_161,"00:10:42,050","00:10:47,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"Well, the answer depends on what kind"
cs-410_2_4_162,"00:10:47,820","00:10:52,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,This estimator gives a best model
cs-410_2_4_163,"00:10:52,320","00:10:57,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,But if you are interested in a model
cs-410_2_4_164,"00:10:57,400","00:11:01,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,"paper for this abstract, then you"
cs-410_2_4_165,"00:11:01,910","00:11:07,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,"So for thing,"
cs-410_2_4_166,"00:11:07,330","00:11:11,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,"of that article, so"
cs-410_2_4_167,"00:11:11,570","00:11:14,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,even though they're not
cs-410_2_4_168,"00:11:14,390","00:11:17,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,So we're going to cover this
cs-410_2_4_169,"00:11:17,750","00:11:22,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,in this class in the query
cs-410_2_4_170,"00:11:24,350","00:11:29,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,So let's take a look at some possible
cs-410_2_4_171,"00:11:29,520","00:11:32,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,One use is simply to use
cs-410_2_4_172,"00:11:32,820","00:11:37,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,So here I show some general
cs-410_2_4_173,"00:11:37,140","00:11:39,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,We can use this text to
cs-410_2_4_174,"00:11:39,830","00:11:41,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,and the model might look like this.
cs-410_2_4_175,"00:11:42,720","00:11:47,845",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"Right, so on the top, we have those"
cs-410_2_4_176,"00:11:47,845","00:11:52,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"etc., and then we'll see some"
cs-410_2_4_177,"00:11:52,610","00:11:55,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"then some very,"
cs-410_2_4_178,"00:11:55,310","00:11:57,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,This is a background language model.
cs-410_2_4_179,"00:11:57,460","00:12:01,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,It represents the frequency of
cs-410_2_4_180,"00:12:01,900","00:12:04,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,This is the background model.
cs-410_2_4_181,"00:12:04,140","00:12:08,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"Now let's look at another text,"
cs-410_2_4_182,"00:12:08,000","00:12:09,979",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,we'll look at the computer
cs-410_2_4_183,"00:12:11,030","00:12:13,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,So we have a collection of
cs-410_2_4_184,"00:12:13,800","00:12:17,454",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,"we do as mentioned again, we can just"
cs-410_2_4_185,"00:12:17,454","00:12:19,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,where we simply normalize the frequencies.
cs-410_2_4_186,"00:12:20,690","00:12:24,326",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,"Now in this case, we'll get"
cs-410_2_4_187,"00:12:24,326","00:12:28,141",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,"On the top, it looks similar because"
cs-410_2_4_188,"00:12:28,141","00:12:29,406",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,they are very common.
cs-410_2_4_189,"00:12:29,406","00:12:34,243",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"But as we go down,"
cs-410_2_4_190,"00:12:34,243","00:12:38,806",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=754,"computer science,"
cs-410_2_4_191,"00:12:38,806","00:12:43,146",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,"And so although here, we might also see"
cs-410_2_4_192,"00:12:43,146","00:12:47,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,we can imagine the probability here is
cs-410_2_4_193,"00:12:47,490","00:12:55,776",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,And we will see many other words here that
cs-410_2_4_194,"00:12:55,776","00:12:58,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,So you can see this distribution
cs-410_2_4_195,"00:12:58,737","00:13:00,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,the corresponding text.
cs-410_2_4_196,"00:13:00,830","00:13:02,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,We can look at even the smaller text.
cs-410_2_4_197,"00:13:03,970","00:13:06,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,"So in this case,"
cs-410_2_4_198,"00:13:06,870","00:13:10,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"Now if we do the same,"
cs-410_2_4_199,"00:13:10,047","00:13:12,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,again the can be expected
cs-410_2_4_200,"00:13:12,740","00:13:16,927",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,"The sooner we see text, mining,"
cs-410_2_4_201,"00:13:16,927","00:13:20,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=796,these words have relatively
cs-410_2_4_202,"00:13:20,440","00:13:27,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,"In contrast, in this distribution, the"
cs-410_2_4_203,"00:13:27,540","00:13:32,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"So this means, again,"
cs-410_2_4_204,"00:13:32,190","00:13:36,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,"we can have a different model,"
cs-410_2_4_205,"00:13:36,266","00:13:40,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=816,So we call this document
cs-410_2_4_206,"00:13:40,450","00:13:42,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,we call this collection language model.
cs-410_2_4_207,"00:13:42,530","00:13:46,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"And later, you will see how they're"
cs-410_2_4_208,"00:13:47,650","00:13:50,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=827,"But now,"
cs-410_2_4_209,"00:13:50,690","00:13:55,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,Can we statistically find what words
cs-410_2_4_210,"00:13:56,900","00:13:58,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,Now how do we find such words?
cs-410_2_4_211,"00:13:58,770","00:14:04,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=838,"Well, our first thought is that let's take"
cs-410_2_4_212,"00:14:04,230","00:14:08,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,So we can take a look at all the documents
cs-410_2_4_213,"00:14:08,860","00:14:10,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,Let's build a language model.
cs-410_2_4_214,"00:14:10,930","00:14:13,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,We can see what words we see there.
cs-410_2_4_215,"00:14:13,220","00:14:19,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,"Well, not surprisingly, we see these"
cs-410_2_4_216,"00:14:19,430","00:14:23,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"So in this case, this language model gives"
cs-410_2_4_217,"00:14:23,490","00:14:26,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,the word in the context of computer.
cs-410_2_4_218,"00:14:26,260","00:14:29,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,And these common words will
cs-410_2_4_219,"00:14:29,370","00:14:31,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,But we also see the computer itself and
cs-410_2_4_220,"00:14:31,750","00:14:35,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,software will have relatively
cs-410_2_4_221,"00:14:35,490","00:14:37,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,"But if we just use this model,"
cs-410_2_4_222,"00:14:37,320","00:14:42,037",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,we cannot just say all these words
cs-410_2_4_223,"00:14:43,210","00:14:50,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,"So ultimately, what we'd like to"
cs-410_2_4_224,"00:14:50,700","00:14:51,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,How can we do that?
cs-410_2_4_225,"00:14:52,760","00:14:55,571",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=892,It turns out that it's possible
cs-410_2_4_226,"00:14:57,610","00:15:00,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,But I suggest you think about that.
cs-410_2_4_227,"00:15:00,020","00:15:03,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=900,So how can we know what
cs-410_2_4_228,"00:15:03,510","00:15:06,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,so that we want to kind
cs-410_2_4_229,"00:15:07,730","00:15:10,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,What model will tell us that?
cs-410_2_4_230,"00:15:10,220","00:15:14,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,"Well, maybe you can think about that."
cs-410_2_4_231,"00:15:14,180","00:15:18,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,So the background language model
cs-410_2_4_232,"00:15:18,170","00:15:21,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,It tells us what was
cs-410_2_4_233,"00:15:21,240","00:15:23,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,"So if we use this background model,"
cs-410_2_4_234,"00:15:23,510","00:15:28,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,we would know that these words
cs-410_2_4_235,"00:15:28,390","00:15:31,595",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,So it's not surprising to observe
cs-410_2_4_236,"00:15:31,595","00:15:36,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,Whereas computer has a very
cs-410_2_4_237,"00:15:36,380","00:15:41,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,it's very surprising that we have seen
cs-410_2_4_238,"00:15:41,200","00:15:42,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,the same is true for software.
cs-410_2_4_239,"00:15:44,220","00:15:48,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,So then we can use these two
cs-410_2_4_240,"00:15:48,750","00:15:52,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,the words that are related to computer.
cs-410_2_4_241,"00:15:52,590","00:15:57,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,"For example, we can simply take the ratio"
cs-410_2_4_242,"00:15:57,310","00:16:01,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,normalize the topic of language model
cs-410_2_4_243,"00:16:01,050","00:16:02,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=961,the background language model.
cs-410_2_4_244,"00:16:02,900","00:16:07,632",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,"So if we do that, we take the ratio,"
cs-410_2_4_245,"00:16:07,632","00:16:11,371",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"computer is ranked, and"
cs-410_2_4_246,"00:16:11,371","00:16:14,796",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"program, all these words"
cs-410_2_4_247,"00:16:14,796","00:16:19,371",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,Because they occur very frequently in the
cs-410_2_4_248,"00:16:19,371","00:16:23,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,"the whole collection, whereas these common"
cs-410_2_4_249,"00:16:23,960","00:16:27,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,"In fact,"
cs-410_2_4_250,"00:16:27,850","00:16:30,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=987,because they are not really
cs-410_2_4_251,"00:16:30,780","00:16:34,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=990,By taking the sample of text
cs-410_2_4_252,"00:16:34,920","00:16:39,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=994,we don't really see more occurrences
cs-410_2_4_253,"00:16:40,250","00:16:43,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,So this shows that even with
cs-410_2_4_254,"00:16:43,310","00:16:46,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1003,we can do some limited
cs-410_2_4_255,"00:16:48,370","00:16:52,343",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1008,"So in this lecture,"
cs-410_2_4_256,"00:16:52,343","00:16:56,776",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1012,which is basically a probability
cs-410_2_4_257,"00:16:56,776","00:17:00,067",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1016,We talked about the simplest language
cs-410_2_4_258,"00:17:00,067","00:17:02,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1020,which is also just a word distribution.
cs-410_2_4_259,"00:17:02,720","00:17:05,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,We talked about the two
cs-410_2_4_260,"00:17:05,320","00:17:10,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,One is we represent the topic in a
cs-410_2_4_261,"00:17:10,360","00:17:12,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,The other is we discover
cs-410_2_4_262,"00:17:16,456","00:17:20,089",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1036,"In the next lecture, we're going to talk"
cs-410_2_4_263,"00:17:20,089","00:17:21,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,design a retrieval function.
cs-410_2_4_264,"00:17:23,260","00:17:24,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,Here are two additional readings.
cs-410_2_4_265,"00:17:24,960","00:17:28,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,The first is a textbook on statistical
cs-410_2_4_266,"00:17:30,290","00:17:35,249",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,The second is an article that
cs-410_2_4_267,"00:17:35,249","00:17:40,326",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,language models with a lot of
cs-410_2_4_268,"00:17:40,326","00:17:50,326",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,[MUSIC]
cs-410_1_4_1,"00:00:00,086","00:00:07,516",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_4_2,"00:00:07,516","00:00:10,282",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about
cs-410_1_4_3,"00:00:10,282","00:00:11,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,"In this lecture,"
cs-410_1_4_4,"00:00:11,805","00:00:17,806",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,we're going to continue the discussion
cs-410_1_4_5,"00:00:17,806","00:00:22,942",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,We're going to look at another kind of
cs-410_1_4_6,"00:00:22,942","00:00:27,584",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,functions than the Vector Space Model
cs-410_1_4_7,"00:00:32,146","00:00:36,589",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"In probabilistic models,"
cs-410_1_4_8,"00:00:36,589","00:00:41,822",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,based on the probability that this
cs-410_1_4_9,"00:00:41,822","00:00:46,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"In other words, we introduce"
cs-410_1_4_10,"00:00:46,802","00:00:51,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,This is the variable R here.
cs-410_1_4_11,"00:00:51,400","00:00:54,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And we also assume that the query and
cs-410_1_4_12,"00:00:54,520","00:00:59,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,the documents are all observations
cs-410_1_4_13,"00:01:00,920","00:01:05,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"Note that in the vector-based models,"
cs-410_1_4_14,"00:01:05,810","00:01:11,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,here we assume they are the data
cs-410_1_4_15,"00:01:11,120","00:01:17,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"And so, the problem of retrieval becomes"
cs-410_1_4_16,"00:01:19,490","00:01:23,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"In this category of models,"
cs-410_1_4_17,"00:01:23,060","00:01:27,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,The classic probabilistic model has
cs-410_1_4_18,"00:01:27,130","00:01:30,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,which we discussed in in
cs-410_1_4_19,"00:01:30,150","00:01:33,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,because its a form is actually
cs-410_1_4_20,"00:01:35,260","00:01:40,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,"In this lecture,"
cs-410_1_4_21,"00:01:41,230","00:01:45,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,P class called a language
cs-410_1_4_22,"00:01:45,550","00:01:50,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,"In particular, we're going to discuss"
cs-410_1_4_23,"00:01:51,370","00:01:55,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,which is one of the most effective
cs-410_1_4_24,"00:01:57,050","00:02:01,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,There was also another line called
cs-410_1_4_25,"00:02:01,840","00:02:04,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"which has led to the PL2 function,"
cs-410_1_4_26,"00:02:06,440","00:02:11,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,it's also one of the most effective
cs-410_1_4_27,"00:02:11,070","00:02:16,847",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"In query likelihood, our assumption"
cs-410_1_4_28,"00:02:16,847","00:02:23,002",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,can be approximated by the probability
cs-410_1_4_29,"00:02:23,002","00:02:29,656",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,"So intuitively, this probability just"
cs-410_1_4_30,"00:02:29,656","00:02:34,808",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"And that is if a user likes document d,"
cs-410_1_4_31,"00:02:34,808","00:02:40,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"the user enter query q ,in"
cs-410_1_4_32,"00:02:40,220","00:02:47,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"So we assume that the user likes d,"
cs-410_1_4_33,"00:02:47,680","00:02:52,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,And then we ask the question about how
cs-410_1_4_34,"00:02:52,610","00:02:53,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,from this user?
cs-410_1_4_35,"00:02:54,890","00:02:56,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,So this is the basic idea.
cs-410_1_4_36,"00:02:56,508","00:03:00,676",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"Now, to understand this idea,"
cs-410_1_4_37,"00:03:00,676","00:03:03,741",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,the basic idea of
cs-410_1_4_38,"00:03:03,741","00:03:09,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"So here, I listed some imagined"
cs-410_1_4_39,"00:03:09,150","00:03:13,599",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,relevance judgments of queries and
cs-410_1_4_40,"00:03:13,599","00:03:17,576",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"For example, in this line,"
cs-410_1_4_41,"00:03:17,576","00:03:24,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,it shows that q1 is a query
cs-410_1_4_42,"00:03:24,546","00:03:28,043",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,And d1 is a document
cs-410_1_4_43,"00:03:28,043","00:03:33,036",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,And 1 means the user thinks
cs-410_1_4_44,"00:03:33,036","00:03:38,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,So this R here can be also approximated
cs-410_1_4_45,"00:03:38,685","00:03:44,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,engine can collect by watching how you
cs-410_1_4_46,"00:03:44,810","00:03:47,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"So in this case, let's say"
cs-410_1_4_47,"00:03:47,990","00:03:49,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So there's a 1 here.
cs-410_1_4_48,"00:03:50,080","00:03:56,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"Similarly, the user clicked on d2 also,"
cs-410_1_4_49,"00:03:56,480","00:03:59,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"In other words,"
cs-410_1_4_50,"00:04:00,700","00:04:05,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"On the other hand,"
cs-410_1_4_51,"00:04:07,430","00:04:13,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,And d4 is non-relevant and then d5 is
cs-410_1_4_52,"00:04:13,485","00:04:17,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"And this part, maybe,"
cs-410_1_4_53,"00:04:17,860","00:04:23,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,So this user typed in q1 and then found
cs-410_1_4_54,"00:04:23,009","00:04:26,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,so d1 is actually non-relevant.
cs-410_1_4_55,"00:04:26,170","00:04:31,124",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"In contrast, here we see it's relevant."
cs-410_1_4_56,"00:04:31,124","00:04:38,401",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,Or this could be the same query typed
cs-410_1_4_57,"00:04:38,401","00:04:42,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"But d2 is also relevant, etc."
cs-410_1_4_58,"00:04:42,660","00:04:47,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"And then here,"
cs-410_1_4_59,"00:04:48,390","00:04:50,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"Now, we can imagine we"
cs-410_1_4_60,"00:04:52,940","00:04:54,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"Now we can ask the question,"
cs-410_1_4_61,"00:04:54,740","00:04:58,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,how can we then estimate
cs-410_1_4_62,"00:05:00,390","00:05:03,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,So how can we compute this
cs-410_1_4_63,"00:05:03,690","00:05:06,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,"Well, intuitively that just means"
cs-410_1_4_64,"00:05:06,230","00:05:10,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,if we look at all the entries
cs-410_1_4_65,"00:05:10,770","00:05:16,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"this particular q, how likely we'll"
cs-410_1_4_66,"00:05:16,010","00:05:18,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,So basically that just means that
cs-410_1_4_67,"00:05:19,730","00:05:24,536",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,We can first count how many
cs-410_1_4_68,"00:05:24,536","00:05:29,576",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,d as a pair in this table and
cs-410_1_4_69,"00:05:29,576","00:05:34,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,we actually have also seen
cs-410_1_4_70,"00:05:34,518","00:05:37,227",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"And then, we just compute the ratio."
cs-410_1_4_71,"00:05:39,409","00:05:42,347",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,So let's take a look at
cs-410_1_4_72,"00:05:42,347","00:05:48,466",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Suppose we are trying to compute this
cs-410_1_4_73,"00:05:48,466","00:05:52,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,What is the estimated probability?
cs-410_1_4_74,"00:05:52,240","00:05:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"Now, think about that."
cs-410_1_4_75,"00:05:54,760","00:05:58,823",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,You can pause the video if needed.
cs-410_1_4_76,"00:05:58,823","00:06:01,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,Try to take a look at the table.
cs-410_1_4_77,"00:06:01,560","00:06:04,606",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,And try to give your
cs-410_1_4_78,"00:06:07,069","00:06:11,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"Have you seen that,"
cs-410_1_4_79,"00:06:11,802","00:06:15,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,we'll be looking at these two pairs?
cs-410_1_4_80,"00:06:15,050","00:06:18,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,"And in both cases, well,"
cs-410_1_4_81,"00:06:18,020","00:06:23,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"actually, in one of the cases, the user"
cs-410_1_4_82,"00:06:23,190","00:06:26,282",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,So R = 1 in only one of the two cases.
cs-410_1_4_83,"00:06:26,282","00:06:28,244",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"In the other case, it's 0."
cs-410_1_4_84,"00:06:28,244","00:06:30,846",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,So that's one out of two.
cs-410_1_4_85,"00:06:30,846","00:06:34,525",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,What about the d1 and the d2?
cs-410_1_4_86,"00:06:34,525","00:06:39,127",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"Well, they are here, d1 and d2, d1 and d2,"
cs-410_1_4_87,"00:06:39,127","00:06:42,729",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"in both cases, in this case, R = 1."
cs-410_1_4_88,"00:06:42,729","00:06:45,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So it's a two out of two and
cs-410_1_4_89,"00:06:45,700","00:06:48,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"So you can see with this approach,"
cs-410_1_4_90,"00:06:48,195","00:06:52,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,we can actually score these documents for
cs-410_1_4_91,"00:06:52,679","00:06:56,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"We now have a score for d1,"
cs-410_1_4_92,"00:06:56,625","00:07:00,334",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,And we can simply rank them
cs-410_1_4_93,"00:07:00,334","00:07:04,056",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,so that's the basic idea
cs-410_1_4_94,"00:07:04,056","00:07:06,971",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"And you can see it makes a lot of sense,"
cs-410_1_4_95,"00:07:06,971","00:07:10,036",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,it's going to rank d2 above
cs-410_1_4_96,"00:07:10,036","00:07:15,992",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"Because in all the cases,"
cs-410_1_4_97,"00:07:15,992","00:07:18,314",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,The user clicked on this document.
cs-410_1_4_98,"00:07:18,314","00:07:23,957",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,So this also should show that
cs-410_1_4_99,"00:07:23,957","00:07:30,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,a search engine can learn a lot from
cs-410_1_4_100,"00:07:30,830","00:07:33,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,This is a simple example
cs-410_1_4_101,"00:07:33,580","00:07:38,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,with small amount of entries here we can
cs-410_1_4_102,"00:07:38,760","00:07:42,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,These probabilities would give us
cs-410_1_4_103,"00:07:42,160","00:07:46,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,might be more relevant or more useful
cs-410_1_4_104,"00:07:47,170","00:07:51,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Now, of course, the problems that we"
cs-410_1_4_105,"00:07:51,100","00:07:54,048",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,all the documents and
cs-410_1_4_106,"00:07:55,320","00:07:57,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"There would be a lot of unseen documents,"
cs-410_1_4_107,"00:07:57,890","00:08:02,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,we have only collected the data from the
cs-410_1_4_108,"00:08:02,880","00:08:07,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,And there are even more unseen queries
cs-410_1_4_109,"00:08:07,370","00:08:10,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,queries will be typed in by users.
cs-410_1_4_110,"00:08:10,060","00:08:15,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"So obviously,"
cs-410_1_4_111,"00:08:15,090","00:08:17,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,it to unseen queries or unseen documents.
cs-410_1_4_112,"00:08:18,635","00:08:22,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"Nevertheless, this shows the basic idea"
cs-410_1_4_113,"00:08:22,278","00:08:23,646",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,it makes sense intuitively.
cs-410_1_4_114,"00:08:23,646","00:08:28,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,So what do we do in such a case when
cs-410_1_4_115,"00:08:28,275","00:08:29,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,unseen queries?
cs-410_1_4_116,"00:08:29,508","00:08:32,818",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"Well, the solutions that we have"
cs-410_1_4_117,"00:08:32,818","00:08:37,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,So in this particular case called
cs-410_1_4_118,"00:08:37,003","00:08:40,784",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,we just approximate this by
cs-410_1_4_119,"00:08:40,784","00:08:46,682",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"p(q given d, R=1)."
cs-410_1_4_120,"00:08:46,682","00:08:51,539",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"So in the condition part, we assume that"
cs-410_1_4_121,"00:08:51,539","00:08:54,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,have seen that the user
cs-410_1_4_122,"00:08:56,190","00:08:58,777",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,And this part shows that
cs-410_1_4_123,"00:08:58,777","00:09:01,438",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,likely the user would
cs-410_1_4_124,"00:09:01,438","00:09:04,653",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,How likely we will see this
cs-410_1_4_125,"00:09:04,653","00:09:08,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"So note that here, we have made"
cs-410_1_4_126,"00:09:08,880","00:09:13,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"Basically, we're going to do, assume that"
cs-410_1_4_127,"00:09:13,900","00:09:17,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,has something to do with whether
cs-410_1_4_128,"00:09:17,970","00:09:20,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,"In other words,"
cs-410_1_4_129,"00:09:22,160","00:09:27,671",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,And that is a user formulates a query
cs-410_1_4_130,"00:09:27,671","00:09:30,358",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,Where if you just look at this
cs-410_1_4_131,"00:09:30,358","00:09:32,629",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,it's not obvious we
cs-410_1_4_132,"00:09:32,629","00:09:37,941",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,So what I really meant is that
cs-410_1_4_133,"00:09:37,941","00:09:43,367",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"probability to help us score,"
cs-410_1_4_134,"00:09:43,367","00:09:48,794",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,probability will have to somehow
cs-410_1_4_135,"00:09:48,794","00:09:54,696",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,conditional probability without
cs-410_1_4_136,"00:09:54,696","00:09:59,306",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,Otherwise we would be having
cs-410_1_4_137,"00:09:59,306","00:10:04,537",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"by making this assumption,"
cs-410_1_4_138,"00:10:04,537","00:10:09,252",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,and try to just model how the user
cs-410_1_4_139,"00:10:09,252","00:10:13,639",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,So this is how you can
cs-410_1_4_140,"00:10:13,639","00:10:18,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,that we can derive a specific
cs-410_1_4_141,"00:10:18,570","00:10:22,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So let's look at how this model work for
cs-410_1_4_142,"00:10:22,020","00:10:23,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"And basically,"
cs-410_1_4_143,"00:10:23,300","00:10:27,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,what we are going to do in this case
cs-410_1_4_144,"00:10:27,420","00:10:30,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,Which of these documents is most
cs-410_1_4_145,"00:10:30,760","00:10:34,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,document in the user's mind when
cs-410_1_4_146,"00:10:34,300","00:10:38,488",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,So we ask this question and we quantify
cs-410_1_4_147,"00:10:38,488","00:10:43,443",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,a conditional probability of observing
cs-410_1_4_148,"00:10:43,443","00:10:47,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,fact the imaginary relevant
cs-410_1_4_149,"00:10:47,245","00:10:51,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,Here you can see we've computed all
cs-410_1_4_150,"00:10:51,885","00:10:55,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,The likelihood of queries
cs-410_1_4_151,"00:10:55,340","00:10:56,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,"Once we have these values,"
cs-410_1_4_152,"00:10:56,880","00:11:00,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,we can then rank these documents
cs-410_1_4_153,"00:11:00,370","00:11:05,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"So to summarize, the general idea"
cs-410_1_4_154,"00:11:05,420","00:11:11,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,risk model is to assume the we introduce
cs-410_1_4_155,"00:11:11,740","00:11:12,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"And then,"
cs-410_1_4_156,"00:11:12,690","00:11:16,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,let the scoring function be defined
cs-410_1_4_157,"00:11:16,740","00:11:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,We also talked about approximating
cs-410_1_4_158,"00:11:22,450","00:11:27,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,And in this case we have a ranking
cs-410_1_4_159,"00:11:27,065","00:11:31,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,based on the probability of
cs-410_1_4_160,"00:11:31,385","00:11:36,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,And this probability should be interpreted
cs-410_1_4_161,"00:11:36,165","00:11:39,236",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"likes document d, would pose query q."
cs-410_1_4_162,"00:11:40,265","00:11:44,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,"Now, the question of course is, how do"
cs-410_1_4_163,"00:11:44,645","00:11:49,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,At this in general has to do with how
cs-410_1_4_164,"00:11:49,500","00:11:51,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,because q is a text.
cs-410_1_4_165,"00:11:51,980","00:11:56,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,And this has to do with a model
cs-410_1_4_166,"00:11:56,560","00:12:00,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,And these kind of models
cs-410_1_4_167,"00:12:02,190","00:12:07,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"So more specifically, we will be"
cs-410_1_4_168,"00:12:07,440","00:12:12,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,conditional probability
cs-410_1_4_169,"00:12:12,050","00:12:18,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,"If the user liked this document,"
cs-410_1_4_170,"00:12:18,463","00:12:21,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,"And in the next lecture we're going to do,"
cs-410_1_4_171,"00:12:21,884","00:12:27,016",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,giving introduction to language
cs-410_1_4_172,"00:12:27,016","00:12:32,063",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,can model text that was a probable
cs-410_1_4_173,"00:12:32,063","00:12:42,063",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,[MUSIC]
cs-410_1_9_1,"00:00:00,012","00:00:08,224",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_9_2,"00:00:08,224","00:00:12,538",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,this is indeed a general idea of
cs-410_1_9_3,"00:00:12,538","00:00:13,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,Algorithm.
cs-410_1_9_4,"00:00:14,640","00:00:19,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,So in all the EM algorithms we
cs-410_1_9_5,"00:00:19,210","00:00:21,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,to help us solve the problem more easily.
cs-410_1_9_6,"00:00:21,970","00:00:25,453",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,In our case the hidden variable
cs-410_1_9_7,"00:00:25,453","00:00:27,203",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,each occurrence of a word.
cs-410_1_9_8,"00:00:27,203","00:00:32,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,And this binary variable would
cs-410_1_9_9,"00:00:32,020","00:00:35,144",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,been generated from 0 sub d or 0 sub p.
cs-410_1_9_10,"00:00:35,144","00:00:38,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,And here we show some possible
cs-410_1_9_11,"00:00:38,420","00:00:43,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"For example, for the it's from background,"
cs-410_1_9_12,"00:00:43,470","00:00:45,105",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,And text on the other hand.
cs-410_1_9_13,"00:00:45,105","00:00:52,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,Is from the topic then it's zero for
cs-410_1_9_14,"00:00:53,260","00:00:58,915",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"Now, of course, we don't observe these z"
cs-410_1_9_15,"00:00:58,915","00:01:01,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,Values of z attaching to other words.
cs-410_1_9_16,"00:01:02,905","00:01:04,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,And that's why we call
cs-410_1_9_17,"00:01:06,135","00:01:08,905",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Now, the idea that we"
cs-410_1_9_18,"00:01:08,905","00:01:12,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,predicting the word distribution that
cs-410_1_9_19,"00:01:12,930","00:01:18,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"is it a predictor,"
cs-410_1_9_20,"00:01:18,840","00:01:25,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"And, so, the EM algorithm then,"
cs-410_1_9_21,"00:01:25,080","00:01:30,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"First, we'll initialize all"
cs-410_1_9_22,"00:01:30,060","00:01:34,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,"In our case,"
cs-410_1_9_23,"00:01:34,960","00:01:37,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"of a word, given by theta sub d."
cs-410_1_9_24,"00:01:37,840","00:01:39,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,So this is an initial addition stage.
cs-410_1_9_25,"00:01:39,680","00:01:44,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,These initialized values would allow
cs-410_1_9_26,"00:01:44,150","00:01:48,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"of these z values, so"
cs-410_1_9_27,"00:01:48,510","00:01:53,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We can't say for sure whether
cs-410_1_9_28,"00:01:53,580","00:01:55,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,But we can have our guess.
cs-410_1_9_29,"00:01:55,090","00:01:57,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,This is given by this formula.
cs-410_1_9_30,"00:01:57,620","00:01:59,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,It's called an E-step.
cs-410_1_9_31,"00:01:59,710","00:02:06,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,And so the algorithm would then try to
cs-410_1_9_32,"00:02:06,520","00:02:12,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"After that, it would then invoke"
cs-410_1_9_33,"00:02:12,190","00:02:17,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,In this step we simply take advantage
cs-410_1_9_34,"00:02:17,490","00:02:22,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,then just group words that are in
cs-410_1_9_35,"00:02:22,825","00:02:26,315",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,from that ground including this as well.
cs-410_1_9_36,"00:02:27,585","00:02:32,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,We can then normalize the count
cs-410_1_9_37,"00:02:32,865","00:02:35,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,to revise our estimate of the parameters.
cs-410_1_9_38,"00:02:36,590","00:02:42,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,So let me also illustrate
cs-410_1_9_39,"00:02:42,310","00:02:46,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,that are believed to have
cs-410_1_9_40,"00:02:46,760","00:02:50,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"that's text, mining algorithm,"
cs-410_1_9_41,"00:02:51,760","00:02:55,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,And we group them together to help us
cs-410_1_9_42,"00:02:55,718","00:03:01,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,re-estimate the parameters
cs-410_1_9_43,"00:03:01,170","00:03:05,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,So these will help us
cs-410_1_9_44,"00:03:06,170","00:03:09,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,Note that before we just set
cs-410_1_9_45,"00:03:09,970","00:03:15,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"But with this guess, we will have"
cs-410_1_9_46,"00:03:15,670","00:03:18,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"Of course, we don't know exactly"
cs-410_1_9_47,"00:03:18,740","00:03:24,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,So we're not going to really
cs-410_1_9_48,"00:03:24,850","00:03:26,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,But rather we're going to
cs-410_1_9_49,"00:03:26,800","00:03:27,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,And this is what happened here.
cs-410_1_9_50,"00:03:29,150","00:03:34,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,So we're going to adjust the count by
cs-410_1_9_51,"00:03:34,420","00:03:38,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,this word has been generated
cs-410_1_9_52,"00:03:39,840","00:03:42,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"And you can see this,"
cs-410_1_9_53,"00:03:42,580","00:03:46,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"Well, this has come from here, right?"
cs-410_1_9_54,"00:03:46,630","00:03:48,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,From the E-step.
cs-410_1_9_55,"00:03:48,120","00:03:52,472",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,So the EM Algorithm would
cs-410_1_9_56,"00:03:52,472","00:03:57,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,estimate of parameters by using
cs-410_1_9_57,"00:03:57,375","00:04:02,458",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,The E-step is to augment the data
cs-410_1_9_58,"00:04:02,458","00:04:05,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,And the M-step is to take advantage
cs-410_1_9_59,"00:04:05,910","00:04:08,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,of the additional information
cs-410_1_9_60,"00:04:08,660","00:04:13,467",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,To split the data accounts and
cs-410_1_9_61,"00:04:13,467","00:04:17,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,re-estimate our parameter.
cs-410_1_9_62,"00:04:17,870","00:04:22,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And then once we have a new generation of
cs-410_1_9_63,"00:04:22,400","00:04:25,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,We are going the E-step again.
cs-410_1_9_64,"00:04:25,150","00:04:28,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,To improve our estimate
cs-410_1_9_65,"00:04:28,520","00:04:33,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,And then that would lead to another
cs-410_1_9_66,"00:04:34,770","00:04:37,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,For the word distribution
cs-410_1_9_67,"00:04:39,610","00:04:44,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"Okay, so, as I said,"
cs-410_1_9_68,"00:04:44,670","00:04:50,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,"is really the variable z, hidden variable,"
cs-410_1_9_69,"00:04:50,380","00:04:55,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,this water is from the top water
cs-410_1_9_70,"00:04:56,810","00:05:00,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"So, this slide has a lot of content and"
cs-410_1_9_71,"00:05:00,780","00:05:03,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,Pause the reader to digest it.
cs-410_1_9_72,"00:05:03,850","00:05:07,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,But this basically captures
cs-410_1_9_73,"00:05:07,300","00:05:12,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,Start with initial values that
cs-410_1_9_74,"00:05:12,500","00:05:18,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,And then we invoke E-step followed
cs-410_1_9_75,"00:05:18,150","00:05:19,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,setting of parameters.
cs-410_1_9_76,"00:05:19,690","00:05:23,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"And then we repeated this, so"
cs-410_1_9_77,"00:05:23,340","00:05:27,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,that would gradually improve
cs-410_1_9_78,"00:05:27,060","00:05:30,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,As I will explain later
cs-410_1_9_79,"00:05:30,050","00:05:35,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,reaching a local maximum of
cs-410_1_9_80,"00:05:35,340","00:05:40,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,So lets take a look at the computation for
cs-410_1_9_81,"00:05:40,180","00:05:41,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,these formulas are the EM.
cs-410_1_9_82,"00:05:41,840","00:05:48,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"Formulas that you see before, and"
cs-410_1_9_83,"00:05:48,220","00:05:53,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"here, like here, n,"
cs-410_1_9_84,"00:05:53,720","00:05:56,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,Like here for example we have n plus one.
cs-410_1_9_85,"00:05:56,040","00:05:59,728",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,That means we have improved.
cs-410_1_9_86,"00:05:59,728","00:06:04,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,From here to here we have an improvement.
cs-410_1_9_87,"00:06:04,047","00:06:08,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,So in this setting we have assumed the two
cs-410_1_9_88,"00:06:08,106","00:06:09,689",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,the background model is null.
cs-410_1_9_89,"00:06:09,689","00:06:11,872",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,So what are the relevance
cs-410_1_9_90,"00:06:11,872","00:06:13,892",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,Well these are the word counts.
cs-410_1_9_91,"00:06:13,892","00:06:18,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"So assume we have just four words,"
cs-410_1_9_92,"00:06:18,290","00:06:22,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,And this is our background model that
cs-410_1_9_93,"00:06:22,680","00:06:23,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,words like the.
cs-410_1_9_94,"00:06:25,910","00:06:29,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"And in the first iteration,"
cs-410_1_9_95,"00:06:29,860","00:06:32,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,Well first we initialize all the values.
cs-410_1_9_96,"00:06:32,280","00:06:37,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"So here, this probability that we're"
cs-410_1_9_97,"00:06:37,360","00:06:38,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,distribution of all the words.
cs-410_1_9_98,"00:06:40,330","00:06:45,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,And then the E-step would give us a guess
cs-410_1_9_99,"00:06:45,940","00:06:48,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,That will generate each word.
cs-410_1_9_100,"00:06:48,470","00:06:51,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,We can see we have different
cs-410_1_9_101,"00:06:51,450","00:06:52,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Why?
cs-410_1_9_102,"00:06:52,430","00:06:56,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Well, that's because these words have"
cs-410_1_9_103,"00:06:56,840","00:07:00,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,So even though the two
cs-410_1_9_104,"00:07:00,020","00:07:05,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And then our initial audition say uniform
cs-410_1_9_105,"00:07:05,320","00:07:09,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"in the background of the distribution,"
cs-410_1_9_106,"00:07:09,270","00:07:14,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,So these words are believed to
cs-410_1_9_107,"00:07:15,820","00:07:17,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,These on the other hand are less likely.
cs-410_1_9_108,"00:07:17,930","00:07:19,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,Probably from background.
cs-410_1_9_109,"00:07:20,620","00:07:23,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"So once we have these z values,"
cs-410_1_9_110,"00:07:23,040","00:07:28,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,we know in the M-step these probabilities
cs-410_1_9_111,"00:07:28,810","00:07:33,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So four must be multiplied by this 0.33
cs-410_1_9_112,"00:07:33,670","00:07:38,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,in order to get the allocated
cs-410_1_9_113,"00:07:39,550","00:07:43,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,And this is done by this multiplication.
cs-410_1_9_114,"00:07:43,770","00:07:49,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,Note that if our guess says this
cs-410_1_9_115,"00:07:52,380","00:07:58,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,then we just get the full count
cs-410_1_9_116,"00:07:58,010","00:08:01,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,In general it's not going
cs-410_1_9_117,"00:08:01,200","00:08:06,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So we're just going to get some percentage
cs-410_1_9_118,"00:08:06,760","00:08:09,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,Then we simply normalize these counts
cs-410_1_9_119,"00:08:09,550","00:08:13,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,to have a new generation
cs-410_1_9_120,"00:08:13,170","00:08:16,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"So you can see, compare this with"
cs-410_1_9_121,"00:08:18,330","00:08:23,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,So compare this with this one and
cs-410_1_9_122,"00:08:23,060","00:08:25,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,"Not only that, we also see some"
cs-410_1_9_123,"00:08:25,930","00:08:30,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,words that are believed to have come from
cs-410_1_9_124,"00:08:30,110","00:08:31,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"Like this one, text."
cs-410_1_9_125,"00:08:32,530","00:08:35,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,"And of course, this new generation of"
cs-410_1_9_126,"00:08:35,930","00:08:42,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,adjust the inferred latent variable or
cs-410_1_9_127,"00:08:42,680","00:08:45,742",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"So we have a new generation of values,"
cs-410_1_9_128,"00:08:45,742","00:08:51,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,because of the E-step based on
cs-410_1_9_129,"00:08:51,115","00:08:56,343",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,And these new inferred values
cs-410_1_9_130,"00:08:56,343","00:09:03,166",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,another generation of the estimate
cs-410_1_9_131,"00:09:03,166","00:09:07,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,And so on and so forth so this is what
cs-410_1_9_132,"00:09:07,990","00:09:11,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,these probabilities
cs-410_1_9_133,"00:09:11,750","00:09:16,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,As you can see in the last row
cs-410_1_9_134,"00:09:16,745","00:09:20,985",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,and the likelihood is increasing
cs-410_1_9_135,"00:09:20,985","00:09:25,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,And note that these log-likelihood is
cs-410_1_9_136,"00:09:25,875","00:09:30,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"between 0 and 1 when you take a logarithm,"
cs-410_1_9_137,"00:09:30,070","00:09:33,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"Now what's also interesting is,"
cs-410_1_9_138,"00:09:33,180","00:09:36,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,And these are the inverted word split.
cs-410_1_9_139,"00:09:36,600","00:09:42,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,And these are the probabilities
cs-410_1_9_140,"00:09:42,150","00:09:47,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,"have come from one distribution, in this"
cs-410_1_9_141,"00:09:47,980","00:09:50,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,And you might wonder whether
cs-410_1_9_142,"00:09:50,580","00:09:55,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Because our main goal is to
cs-410_1_9_143,"00:09:55,540","00:09:57,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,So this is our primary goal.
cs-410_1_9_144,"00:09:57,400","00:10:00,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,We hope to have a more discriminative
cs-410_1_9_145,"00:10:00,900","00:10:04,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,But the last column is also bi-product.
cs-410_1_9_146,"00:10:04,400","00:10:07,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,This also can actually be very useful.
cs-410_1_9_147,"00:10:07,170","00:10:08,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,You can think about that.
cs-410_1_9_148,"00:10:08,380","00:10:10,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"We want to use, is to for"
cs-410_1_9_149,"00:10:10,220","00:10:16,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,example is to estimate to what extent this
cs-410_1_9_150,"00:10:16,080","00:10:18,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"And this, when we add this up or"
cs-410_1_9_151,"00:10:18,165","00:10:23,304",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,take the average we will kind of know to
cs-410_1_9_152,"00:10:23,304","00:10:27,823",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,versus content was that are not
cs-410_1_9_153,"00:10:27,823","00:10:37,823",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,[MUSIC]
cs-410_2_9_1,"00:00:07,553","00:00:12,636",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,"So, I just showed you that empirically"
cs-410_2_9_2,"00:00:12,636","00:00:17,041",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,but theoretically it can also
cs-410_2_9_3,"00:00:17,041","00:00:19,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,converge to a local maximum.
cs-410_2_9_4,"00:00:19,295","00:00:24,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,So here's just an illustration of what
cs-410_2_9_5,"00:00:24,925","00:00:29,613",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"This required more knowledge about that,"
cs-410_2_9_6,"00:00:29,613","00:00:36,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"some of that inequalities,"
cs-410_2_9_7,"00:00:39,380","00:00:45,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,So here what you see is on the X
cs-410_2_9_8,"00:00:45,040","00:00:46,799",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,This is a parameter that we have.
cs-410_2_9_9,"00:00:46,799","00:00:49,714",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,On the y axis we see
cs-410_2_9_10,"00:00:49,714","00:00:57,171",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So this curve is the original
cs-410_2_9_11,"00:00:57,171","00:01:04,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,and this is the one that
cs-410_2_9_12,"00:01:04,110","00:01:06,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,And we hope to find a c0 value
cs-410_2_9_13,"00:01:06,630","00:01:11,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,But in the case of Mitsumoto we can
cs-410_2_9_14,"00:01:11,480","00:01:12,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,to the problem.
cs-410_2_9_15,"00:01:12,470","00:01:14,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"So, we have to resolve"
cs-410_2_9_16,"00:01:14,698","00:01:16,457",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,the EM algorithm is such an algorithm.
cs-410_2_9_17,"00:01:16,457","00:01:17,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,It's a Hill-Climb algorithm.
cs-410_2_9_18,"00:01:17,850","00:01:22,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,That would mean you start
cs-410_2_9_19,"00:01:22,490","00:01:26,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"Let's say you start from here,"
cs-410_2_9_20,"00:01:26,260","00:01:32,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,And then you try to improve
cs-410_2_9_21,"00:01:32,090","00:01:35,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,another point where you can
cs-410_2_9_22,"00:01:35,420","00:01:37,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,So that's the ideal hill climbing.
cs-410_2_9_23,"00:01:37,630","00:01:43,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"And in the EM algorithm, the way we"
cs-410_2_9_24,"00:01:43,030","00:01:46,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"First, we'll fix a lower"
cs-410_2_9_25,"00:01:46,940","00:01:48,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,So this is the lower bound.
cs-410_2_9_26,"00:01:48,628","00:01:49,128",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,See here.
cs-410_2_9_27,"00:01:51,010","00:01:57,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"And once we fit the lower bound,"
cs-410_2_9_28,"00:01:57,560","00:01:59,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,"And of course, the reason why this works,"
cs-410_2_9_29,"00:01:59,420","00:02:02,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,is because the lower bound
cs-410_2_9_30,"00:02:02,850","00:02:05,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,So we know our current guess is here.
cs-410_2_9_31,"00:02:05,780","00:02:11,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"And by maximizing the lower bound,"
cs-410_2_9_32,"00:02:11,530","00:02:12,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,To here.
cs-410_2_9_33,"00:02:13,300","00:02:14,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,Right?
cs-410_2_9_34,"00:02:14,650","00:02:20,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,And we can then map to the original
cs-410_2_9_35,"00:02:20,150","00:02:25,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,"Because it's a lower bound, we are"
cs-410_2_9_36,"00:02:25,600","00:02:30,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,Because we improve our lower bound and
cs-410_2_9_37,"00:02:30,570","00:02:35,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,curve which is above this lower bound
cs-410_2_9_38,"00:02:36,310","00:02:39,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,So we already know it's
cs-410_2_9_39,"00:02:39,090","00:02:42,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,So we definitely improve this
cs-410_2_9_40,"00:02:42,440","00:02:47,253",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,which is above this lower bound.
cs-410_2_9_41,"00:02:47,253","00:02:49,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"So, in our example,"
cs-410_2_9_42,"00:02:49,770","00:02:53,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,the current guess is parameter value
cs-410_2_9_43,"00:02:53,520","00:02:57,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,And then the next guess is
cs-410_2_9_44,"00:02:57,660","00:03:01,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,From this illustration you
cs-410_2_9_45,"00:03:01,110","00:03:03,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,is always better than the current guess.
cs-410_2_9_46,"00:03:03,620","00:03:06,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"Unless it has reached the maximum,"
cs-410_2_9_47,"00:03:06,930","00:03:08,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,So the two would be equal.
cs-410_2_9_48,"00:03:08,008","00:03:12,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"So, the E-step is basically"
cs-410_2_9_49,"00:03:12,821","00:03:17,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,to compute this lower bound.
cs-410_2_9_50,"00:03:17,650","00:03:22,061",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,We don't directly just compute
cs-410_2_9_51,"00:03:22,061","00:03:25,452",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,we compute the length of
cs-410_2_9_52,"00:03:25,452","00:03:28,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,these are basically a part
cs-410_2_9_53,"00:03:28,990","00:03:31,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,This helps determine the lower bound.
cs-410_2_9_54,"00:03:31,150","00:03:34,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,The M-step on the other hand is
cs-410_2_9_55,"00:03:34,460","00:03:37,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,It allows us to move
cs-410_2_9_56,"00:03:37,480","00:03:41,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,And that's why EM algorithm is guaranteed
cs-410_2_9_57,"00:03:42,490","00:03:46,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"Now, as you can imagine,"
cs-410_2_9_58,"00:03:46,720","00:03:50,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,we also have to repeat the EM
cs-410_2_9_59,"00:03:50,100","00:03:54,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,In order to figure out which one
cs-410_2_9_60,"00:03:54,340","00:03:59,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,And this actually in general is a
cs-410_2_9_61,"00:03:59,070","00:04:02,689",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,So here for
cs-410_2_9_62,"00:04:02,689","00:04:06,223",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,then we gradually just
cs-410_2_9_63,"00:04:06,223","00:04:11,227",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"So, that's not optimal, and"
cs-410_2_9_64,"00:04:11,227","00:04:16,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,so the only way to climb up to this gear
cs-410_2_9_65,"00:04:16,575","00:04:22,767",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"So, in the EM algorithm, we generally"
cs-410_2_9_66,"00:04:22,767","00:04:27,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,or have some other way to determine
cs-410_2_9_67,"00:04:29,840","00:04:34,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,To summarize in this lecture we
cs-410_2_9_68,"00:04:34,320","00:04:38,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,This is a general algorithm for computing
cs-410_2_9_69,"00:04:38,683","00:04:42,153",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"kinds of models, so"
cs-410_2_9_70,"00:04:42,153","00:04:46,468",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"And it's a hill-climbing algorithm, so it"
cs-410_2_9_71,"00:04:46,468","00:04:48,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,it will depend on initial points.
cs-410_2_9_72,"00:04:49,770","00:04:55,414",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,The general idea is that we will have
cs-410_2_9_73,"00:04:55,414","00:05:00,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,In the E-step we roughly [INAUDIBLE]
cs-410_2_9_74,"00:05:00,270","00:05:05,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,of useful hidden variables that we
cs-410_2_9_75,"00:05:05,560","00:05:10,056",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"In our case, this is the distribution"
cs-410_2_9_76,"00:05:10,056","00:05:15,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,In the M-step then we would exploit
cs-410_2_9_77,"00:05:15,750","00:05:20,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"it easier to estimate the distribution,"
cs-410_2_9_78,"00:05:20,790","00:05:24,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,Here improve is guaranteed in
cs-410_2_9_79,"00:05:24,860","00:05:30,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,Note that it's not necessary that we
cs-410_2_9_80,"00:05:30,240","00:05:35,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,parameter value even though the likelihood
cs-410_2_9_81,"00:05:35,260","00:05:40,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,There are some properties that have to
cs-410_2_9_82,"00:05:40,370","00:05:44,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,also to convert into some stable value.
cs-410_2_9_83,"00:05:47,500","00:05:50,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,Now here data augmentation
cs-410_2_9_84,"00:05:50,790","00:05:51,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"That means,"
cs-410_2_9_85,"00:05:51,360","00:05:54,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,we're not going to just say exactly
cs-410_2_9_86,"00:05:54,830","00:05:59,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,But we're going to have a probability
cs-410_2_9_87,"00:05:59,390","00:06:01,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,these hidden variables.
cs-410_2_9_88,"00:06:01,140","00:06:05,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,So this causes a split of counts
cs-410_2_9_89,"00:06:07,430","00:06:12,783",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,And in our case we'll split the word
cs-410_2_9_90,"00:06:12,783","00:06:22,783",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,[MUSIC]
cs-410_3_9_1,"00:00:00,012","00:00:07,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_9_2,"00:00:07,295","00:00:11,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about probabilistic and
cs-410_3_9_3,"00:00:12,710","00:00:18,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,In this lecture we're going to introduce
cs-410_3_9_4,"00:00:18,000","00:00:18,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,often called PLSA.
cs-410_3_9_5,"00:00:18,770","00:00:26,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"This is the most basic topic model,"
cs-410_3_9_6,"00:00:26,060","00:00:30,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,Now this kind of models
cs-410_3_9_7,"00:00:30,890","00:00:34,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,mine multiple topics from text documents.
cs-410_3_9_8,"00:00:34,560","00:00:39,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,And PRSA is one of the most basic
cs-410_3_9_9,"00:00:39,410","00:00:43,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,So let's first examine this power
cs-410_3_9_10,"00:00:43,800","00:00:47,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,Here I show a sample article which is
cs-410_3_9_11,"00:00:48,830","00:00:51,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And I show some simple topics.
cs-410_3_9_12,"00:00:51,100","00:00:55,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"For example government response,"
cs-410_3_9_13,"00:00:55,870","00:00:57,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,Donation and the background.
cs-410_3_9_14,"00:00:59,260","00:01:04,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,You can see in the article we use
cs-410_3_9_15,"00:01:05,150","00:01:09,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,So we first for example see there's
cs-410_3_9_16,"00:01:09,540","00:01:14,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,this is followed by discussion of flooding
cs-410_3_9_17,"00:01:14,740","00:01:17,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,We also see background
cs-410_3_9_18,"00:01:18,840","00:01:23,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,So the overall of topic analysis here
cs-410_3_9_19,"00:01:23,740","00:01:28,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"the text, to segment the topics,"
cs-410_3_9_20,"00:01:28,250","00:01:33,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"distribution and to figure out first,"
cs-410_3_9_21,"00:01:33,820","00:01:36,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,How do we know there's a topic
cs-410_3_9_22,"00:01:36,420","00:01:39,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,There's a topic about a flood in the city.
cs-410_3_9_23,"00:01:39,020","00:01:41,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,So these are the tasks
cs-410_3_9_24,"00:01:42,870","00:01:46,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,If we had discovered these
cs-410_3_9_25,"00:01:46,110","00:01:50,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"as you see here,"
cs-410_3_9_26,"00:01:50,030","00:01:54,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"Then you can do a lot of things,"
cs-410_3_9_27,"00:01:54,390","00:01:59,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"of the topics,"
cs-410_3_9_28,"00:01:59,800","00:02:04,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,So the formal definition of problem of
cs-410_3_9_29,"00:02:04,220","00:02:04,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,shown here.
cs-410_3_9_30,"00:02:04,870","00:02:09,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,And this is after a slide that you
cs-410_3_9_31,"00:02:09,270","00:02:14,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,"So the input is a collection, the number"
cs-410_3_9_32,"00:02:14,100","00:02:15,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,of course the text data.
cs-410_3_9_33,"00:02:16,300","00:02:18,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,And then the output is of two kinds.
cs-410_3_9_34,"00:02:18,760","00:02:21,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"One is the topic category,"
cs-410_3_9_35,"00:02:21,720","00:02:22,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Theta i's.
cs-410_3_9_36,"00:02:22,520","00:02:24,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,Each theta i is a word distribution.
cs-410_3_9_37,"00:02:24,790","00:02:28,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"And second, it's the topic coverage for"
cs-410_3_9_38,"00:02:28,160","00:02:30,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,These are pi sub i j's.
cs-410_3_9_39,"00:02:30,130","00:02:33,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,And they tell us which document it covers.
cs-410_3_9_40,"00:02:33,490","00:02:35,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,Which topic to what extent.
cs-410_3_9_41,"00:02:35,440","00:02:37,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So we hope to generate these as output.
cs-410_3_9_42,"00:02:37,960","00:02:41,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,Because there are many useful
cs-410_3_9_43,"00:02:42,880","00:02:47,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So the idea of PLSA is
cs-410_3_9_44,"00:02:47,100","00:02:50,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,the two component mixture model
cs-410_3_9_45,"00:02:50,660","00:02:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,The only difference is that we
cs-410_3_9_46,"00:02:54,760","00:02:57,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"Otherwise, it is essentially the same."
cs-410_3_9_47,"00:02:57,960","00:03:03,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,So here I illustrate how we can generate
cs-410_3_9_48,"00:03:03,730","00:03:06,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,naturally in all cases
cs-410_3_9_49,"00:03:06,490","00:03:11,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,of Probabilistic modelling would want
cs-410_3_9_50,"00:03:11,310","00:03:13,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"So we would also ask the question,"
cs-410_3_9_51,"00:03:13,400","00:03:18,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,what's the probability of observing
cs-410_3_9_52,"00:03:18,200","00:03:19,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,Now if you look at this picture and
cs-410_3_9_53,"00:03:19,470","00:03:21,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,compare this with the picture
cs-410_3_9_54,"00:03:21,840","00:03:25,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,you will see the only difference is
cs-410_3_9_55,"00:03:26,940","00:03:32,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"So, before we have just one topic,"
cs-410_3_9_56,"00:03:32,900","00:03:35,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,But now we have more topics.
cs-410_3_9_57,"00:03:35,990","00:03:38,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Specifically, we have k topics now."
cs-410_3_9_58,"00:03:38,260","00:03:43,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,All these are topics that we assume
cs-410_3_9_59,"00:03:43,930","00:03:49,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,So the consequence is that our switch for
cs-410_3_9_60,"00:03:49,450","00:03:51,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,Before it's just a two way switch.
cs-410_3_9_61,"00:03:51,210","00:03:53,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,We can think of it as flipping a coin.
cs-410_3_9_62,"00:03:53,420","00:03:55,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,But now we have multiple ways.
cs-410_3_9_63,"00:03:55,110","00:03:59,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,First we can flip a coin to decide
cs-410_3_9_64,"00:03:59,660","00:04:06,913",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,So it's the background lambda
cs-410_3_9_65,"00:04:06,913","00:04:11,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,1 minus lambda sub B gives
cs-410_3_9_66,"00:04:11,490","00:04:16,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,actually choosing a non-background topic.
cs-410_3_9_67,"00:04:16,300","00:04:17,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"After we have made this decision,"
cs-410_3_9_68,"00:04:17,860","00:04:24,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,we have to make another decision to
cs-410_3_9_69,"00:04:24,750","00:04:26,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,So there are K way switch here.
cs-410_3_9_70,"00:04:26,480","00:04:30,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"And this is characterized by pi,"
cs-410_3_9_71,"00:04:31,450","00:04:33,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,This is just the difference of designs.
cs-410_3_9_72,"00:04:33,775","00:04:36,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,Which is a little bit more complicated.
cs-410_3_9_73,"00:04:36,745","00:04:40,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,But once we decide which distribution to
cs-410_3_9_74,"00:04:40,655","00:04:45,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,just generate a word by using one of
cs-410_3_9_75,"00:04:46,885","00:04:50,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,So now lets look at the question
cs-410_3_9_76,"00:04:50,920","00:04:55,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,So what's the probability of observing
cs-410_3_9_77,"00:04:55,780","00:04:57,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,What do you think?
cs-410_3_9_78,"00:04:57,250","00:05:01,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,Now we've seen this
cs-410_3_9_79,"00:05:01,150","00:05:05,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"if you can recall, it's generally a sum."
cs-410_3_9_80,"00:05:05,210","00:05:08,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,Of all the different possibilities
cs-410_3_9_81,"00:05:08,540","00:05:14,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,So let's first look at how the word can
cs-410_3_9_82,"00:05:14,260","00:05:18,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"Well, the probability that the word is"
cs-410_3_9_83,"00:05:18,340","00:05:22,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,is lambda multiplied by the probability
cs-410_3_9_84,"00:05:22,700","00:05:24,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"Model, right."
cs-410_3_9_85,"00:05:24,200","00:05:25,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,Two things must happen.
cs-410_3_9_86,"00:05:25,150","00:05:28,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"First, we have to have"
cs-410_3_9_87,"00:05:28,270","00:05:31,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,"and that's the probability of lambda,"
cs-410_3_9_88,"00:05:31,730","00:05:36,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"Then second, we must have actually"
cs-410_3_9_89,"00:05:36,330","00:05:39,161",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,and that's probability
cs-410_3_9_90,"00:05:40,220","00:05:41,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"Okay, so similarly,"
cs-410_3_9_91,"00:05:41,790","00:05:46,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,we can figure out the probability of
cs-410_3_9_92,"00:05:46,020","00:05:48,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,Like the topic theta sub k.
cs-410_3_9_93,"00:05:48,530","00:05:51,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,Now notice that here's
cs-410_3_9_94,"00:05:51,890","00:05:57,023",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,And that's because of the choice
cs-410_3_9_95,"00:05:57,023","00:06:00,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,only happens if two things happen.
cs-410_3_9_96,"00:06:00,630","00:06:04,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,One is we decide not to
cs-410_3_9_97,"00:06:04,020","00:06:07,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"So, that's a probability"
cs-410_3_9_98,"00:06:07,630","00:06:13,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"Second, we also have to actually choose"
cs-410_3_9_99,"00:06:13,290","00:06:16,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"So that's probability of theta sub K,"
cs-410_3_9_100,"00:06:17,900","00:06:21,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"And similarly, the probability of"
cs-410_3_9_101,"00:06:21,460","00:06:26,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,The topic and the first topic
cs-410_3_9_102,"00:06:26,480","00:06:27,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,And so
cs-410_3_9_103,"00:06:27,250","00:06:32,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,in the end the probability of observing
cs-410_3_9_104,"00:06:32,480","00:06:38,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,And I have to stress again this is a very
cs-410_3_9_105,"00:06:38,080","00:06:44,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,really key to understanding all the topic
cs-410_3_9_106,"00:06:44,150","00:06:47,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,So make sure that you really
cs-410_3_9_107,"00:06:49,410","00:06:53,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,of w is indeed the sum of these terms.
cs-410_3_9_108,"00:06:56,540","00:07:00,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"So, next,"
cs-410_3_9_109,"00:07:00,620","00:07:05,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,we would be interested in
cs-410_3_9_110,"00:07:05,250","00:07:07,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"All right, so to estimate the parameters."
cs-410_3_9_111,"00:07:07,250","00:07:07,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,"But firstly,"
cs-410_3_9_112,"00:07:07,760","00:07:13,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,let's put all these together to have the
cs-410_3_9_113,"00:07:13,510","00:07:19,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,The first line shows the probability of a
cs-410_3_9_114,"00:07:19,010","00:07:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,And this is an important
cs-410_3_9_115,"00:07:22,560","00:07:24,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,So let's take a closer look at this.
cs-410_3_9_116,"00:07:24,250","00:07:27,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,This actually commands all
cs-410_3_9_117,"00:07:27,430","00:07:29,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,So first of all we see lambda sub b here.
cs-410_3_9_118,"00:07:29,280","00:07:31,539",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,This represents a percentage
cs-410_3_9_119,"00:07:32,610","00:07:35,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,that we believe exist in the text data.
cs-410_3_9_120,"00:07:35,560","00:07:39,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,And this can be a known value
cs-410_3_9_121,"00:07:41,180","00:07:43,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"Second, we see the background"
cs-410_3_9_122,"00:07:43,380","00:07:45,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,typically we also assume this is known.
cs-410_3_9_123,"00:07:45,210","00:07:48,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"We can use a large collection of text, or"
cs-410_3_9_124,"00:07:48,000","00:07:51,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,use all the text that we have available
cs-410_3_9_125,"00:07:52,890","00:07:55,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,Now next in the next stop this formula.
cs-410_3_9_126,"00:07:55,008","00:07:57,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,[COUGH] Excuse me.
cs-410_3_9_127,"00:07:57,960","00:08:00,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,You see two interesting
cs-410_3_9_128,"00:08:00,160","00:08:01,886",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,those are the most important parameters.
cs-410_3_9_129,"00:08:01,886","00:08:04,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,That we are.
cs-410_3_9_130,"00:08:04,690","00:08:06,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So one is pi's.
cs-410_3_9_131,"00:08:06,190","00:08:10,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,And these are the coverage
cs-410_3_9_132,"00:08:11,280","00:08:15,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,And the other is word distributions
cs-410_3_9_133,"00:08:18,530","00:08:23,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"So the next line,"
cs-410_3_9_134,"00:08:23,780","00:08:26,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,in to calculate
cs-410_3_9_135,"00:08:26,280","00:08:29,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"This is, again, of the familiar"
cs-410_3_9_136,"00:08:29,720","00:08:32,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,you have a count of
cs-410_3_9_137,"00:08:32,050","00:08:35,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,And then log of a probability.
cs-410_3_9_138,"00:08:35,100","00:08:39,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,Now it's a little bit more
cs-410_3_9_139,"00:08:39,040","00:08:43,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"Because now we have more components,"
cs-410_3_9_140,"00:08:43,890","00:08:47,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,And then this line is just
cs-410_3_9_141,"00:08:47,750","00:08:51,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"And it's very similar, just accounting for"
cs-410_3_9_142,"00:08:52,470","00:08:54,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,So what are the unknown parameters?
cs-410_3_9_143,"00:08:54,060","00:08:55,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,I already said that there are two kinds.
cs-410_3_9_144,"00:08:55,960","00:08:59,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"One is coverage,"
cs-410_3_9_145,"00:08:59,150","00:09:02,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,"Again, it's a useful exercise for"
cs-410_3_9_146,"00:09:02,350","00:09:04,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,Exactly how many
cs-410_3_9_147,"00:09:05,750","00:09:07,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,How many unknown parameters are there?
cs-410_3_9_148,"00:09:07,940","00:09:08,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"Now, try and"
cs-410_3_9_149,"00:09:08,680","00:09:13,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,think out that question will help you
cs-410_3_9_150,"00:09:13,090","00:09:17,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,And will also allow you to understand
cs-410_3_9_151,"00:09:17,760","00:09:20,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,when use PLSA to analyze text data?
cs-410_3_9_152,"00:09:20,430","00:09:22,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,And these are precisely
cs-410_3_9_153,"00:09:24,480","00:09:28,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,So after we have obtained
cs-410_3_9_154,"00:09:28,200","00:09:30,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,the next is to worry about
cs-410_3_9_155,"00:09:32,050","00:09:34,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"And we can do the usual think,"
cs-410_3_9_156,"00:09:34,770","00:09:40,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"So again, it's a constrained optimization"
cs-410_3_9_157,"00:09:40,190","00:09:44,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,Only that we have a collection of text and
cs-410_3_9_158,"00:09:44,350","00:09:48,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,"And we still have two constraints,"
cs-410_3_9_159,"00:09:48,655","00:09:50,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,One is the word distributions.
cs-410_3_9_160,"00:09:51,245","00:09:56,525",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,All the words must have probabilities
cs-410_3_9_161,"00:09:56,525","00:09:59,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,The other is the topic
cs-410_3_9_162,"00:09:59,975","00:10:05,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,a document will have to cover
cs-410_3_9_163,"00:10:05,200","00:10:08,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,the probability of covering each
cs-410_3_9_164,"00:10:08,820","00:10:13,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,So at this point though it's basically
cs-410_3_9_165,"00:10:13,190","00:10:16,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,you just need to figure out
cs-410_3_9_166,"00:10:16,370","00:10:18,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,There's a function with many variables.
cs-410_3_9_167,"00:10:18,670","00:10:22,481",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,and we need to just figure
cs-410_3_9_168,"00:10:22,481","00:10:26,397",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,variables to make the function
cs-410_3_9_169,"00:10:26,397","00:10:36,397",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,>> [MUSIC]
cs-410_3_7_1,"00:00:00,300","00:00:03,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_7_2,"00:00:09,170","00:00:13,893",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,This lecture is about natural language
cs-410_3_7_3,"00:00:13,893","00:00:16,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,content analysis.
cs-410_3_7_4,"00:00:16,330","00:00:21,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,Natural language content analysis
cs-410_3_7_5,"00:00:21,510","00:00:23,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,So we're going to first talk about this.
cs-410_3_7_6,"00:00:24,980","00:00:26,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"And in particular,"
cs-410_3_7_7,"00:00:26,330","00:00:31,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,natural language processing with
cs-410_3_7_8,"00:00:33,210","00:00:38,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,And this determines what algorithms can
cs-410_3_7_9,"00:00:40,820","00:00:44,991",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,We're going to take a look at the basic
cs-410_3_7_10,"00:00:46,330","00:00:48,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,And I'm going to explain these concepts
cs-410_3_7_11,"00:00:48,970","00:00:52,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,using a similar example
cs-410_3_7_12,"00:00:52,600","00:00:55,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,A dog is chasing a boy on the playground.
cs-410_3_7_13,"00:00:55,650","00:00:58,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,Now this is a very simple sentence.
cs-410_3_7_14,"00:00:58,310","00:01:01,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,When we read such a sentence
cs-410_3_7_15,"00:01:01,160","00:01:05,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,about it to get the meaning of it.
cs-410_3_7_16,"00:01:05,200","00:01:09,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,But when a computer has to
cs-410_3_7_17,"00:01:09,460","00:01:12,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,the computer has to go
cs-410_3_7_18,"00:01:13,430","00:01:16,532",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"First, the computer needs"
cs-410_3_7_19,"00:01:16,532","00:01:18,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,how to segment the words in English.
cs-410_3_7_20,"00:01:18,630","00:01:22,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"And this is very easy,"
cs-410_3_7_21,"00:01:22,010","00:01:26,136",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,And then the computer will need
cs-410_3_7_22,"00:01:26,136","00:01:27,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,syntactical categories.
cs-410_3_7_23,"00:01:27,870","00:01:34,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"So for example, dog is a noun,"
cs-410_3_7_24,"00:01:34,510","00:01:37,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,And this is called a Lexical analysis.
cs-410_3_7_25,"00:01:37,350","00:01:41,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"In particular, tagging these words"
cs-410_3_7_26,"00:01:41,590","00:01:43,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,is called a part-of-speech tagging.
cs-410_3_7_27,"00:01:45,030","00:01:48,383",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,After that the computer also needs to
cs-410_3_7_28,"00:01:48,383","00:01:49,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,these words.
cs-410_3_7_29,"00:01:49,040","00:01:53,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,So a and dog would form a noun phrase.
cs-410_3_7_30,"00:01:53,300","00:01:57,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,On the playground would be
cs-410_3_7_31,"00:01:57,590","00:02:01,378",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,And there is certain way for
cs-410_3_7_32,"00:02:01,378","00:02:03,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,them to create meaning.
cs-410_3_7_33,"00:02:03,620","00:02:06,469",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,Some other combinations
cs-410_3_7_34,"00:02:07,720","00:02:12,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"And this is called syntactical parsing, or"
cs-410_3_7_35,"00:02:12,620","00:02:17,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"syntactical analysis,"
cs-410_3_7_36,"00:02:17,090","00:02:21,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,The outcome is a parse tree
cs-410_3_7_37,"00:02:21,180","00:02:24,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,That tells us the structure
cs-410_3_7_38,"00:02:24,050","00:02:27,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,that we know how we can
cs-410_3_7_39,"00:02:27,430","00:02:29,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,But this is not semantics yet.
cs-410_3_7_40,"00:02:29,740","00:02:34,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,So in order to get the meaning we
cs-410_3_7_41,"00:02:34,530","00:02:39,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,these structures into some real world
cs-410_3_7_42,"00:02:39,860","00:02:45,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"So dog is a concept that we know,"
cs-410_3_7_43,"00:02:45,500","00:02:50,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,So connecting these phrases
cs-410_3_7_44,"00:02:52,160","00:02:58,788",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"Now for a computer, would have to formally"
cs-410_3_7_45,"00:02:58,788","00:03:03,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"So dog, d1 means d1 is a dog."
cs-410_3_7_46,"00:03:04,690","00:03:09,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"Boy, b1 means b1 refers to a boy etc."
cs-410_3_7_47,"00:03:09,420","00:03:13,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,And also represents the chasing
cs-410_3_7_48,"00:03:13,430","00:03:18,334",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"So, chasing is a predicate here with"
cs-410_3_7_49,"00:03:18,334","00:03:23,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"three arguments, d1, b1, and p1."
cs-410_3_7_50,"00:03:23,720","00:03:25,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,Which is playground.
cs-410_3_7_51,"00:03:25,920","00:03:31,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So this formal rendition of
cs-410_3_7_52,"00:03:31,320","00:03:35,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"Once we reach that level of understanding,"
cs-410_3_7_53,"00:03:35,950","00:03:42,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"For example, if we assume there's a rule"
cs-410_3_7_54,"00:03:42,050","00:03:48,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"the person can get scared, then we"
cs-410_3_7_55,"00:03:48,420","00:03:52,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"This is the inferred meaning,"
cs-410_3_7_56,"00:03:52,800","00:03:58,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"And finally, we might even further infer"
cs-410_3_7_57,"00:03:58,485","00:04:06,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,"what this sentence is requesting,"
cs-410_3_7_58,"00:04:06,170","00:04:12,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,or why the person who say it in
cs-410_3_7_59,"00:04:12,920","00:04:18,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"And so, this has to do with"
cs-410_3_7_60,"00:04:18,310","00:04:24,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,This is called speech act analysis or
cs-410_3_7_61,"00:04:24,550","00:04:27,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,Which first to the use of language.
cs-410_3_7_62,"00:04:27,920","00:04:32,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"So, in this case a person saying this"
cs-410_3_7_63,"00:04:32,704","00:04:34,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,bring back the dog.
cs-410_3_7_64,"00:04:35,240","00:04:42,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"So this means when saying a sentence,"
cs-410_3_7_65,"00:04:42,320","00:04:44,769",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,So the action here is to make a request.
cs-410_3_7_66,"00:04:46,770","00:04:51,408",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"Now, this slide clearly shows that"
cs-410_3_7_67,"00:04:51,408","00:04:55,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,a sentence there are a lot of
cs-410_3_7_68,"00:04:55,720","00:05:00,337",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"Now, in general it's very hard for"
cs-410_3_7_69,"00:05:00,337","00:05:04,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,especially if you would want
cs-410_3_7_70,"00:05:04,910","00:05:06,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,This is very difficult.
cs-410_3_7_71,"00:05:08,190","00:05:11,094",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"Now, the main reason why natural"
cs-410_3_7_72,"00:05:11,094","00:05:14,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,it's because it's designed it will
cs-410_3_7_73,"00:05:15,990","00:05:20,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"As a result, for example,"
cs-410_3_7_74,"00:05:21,250","00:05:25,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,Because we assume all of
cs-410_3_7_75,"00:05:25,150","00:05:28,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,there's no need to encode this knowledge.
cs-410_3_7_76,"00:05:29,780","00:05:31,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,That makes communication efficient.
cs-410_3_7_77,"00:05:32,480","00:05:37,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"We also keep a lot of ambiguities,"
cs-410_3_7_78,"00:05:39,090","00:05:45,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"And this is again, because we assume we"
cs-410_3_7_79,"00:05:45,130","00:05:48,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"So, there's no problem with"
cs-410_3_7_80,"00:05:48,800","00:05:50,869",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,possibly different things
cs-410_3_7_81,"00:05:52,610","00:05:55,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,Yet for
cs-410_3_7_82,"00:05:55,880","00:06:00,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,because a computer does not have
cs-410_3_7_83,"00:06:00,250","00:06:03,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,So the computer will be confused indeed.
cs-410_3_7_84,"00:06:03,620","00:06:06,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,And this makes it hard for
cs-410_3_7_85,"00:06:06,980","00:06:09,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"Indeed, it makes it very hard for"
cs-410_3_7_86,"00:06:09,440","00:06:15,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,every step in the slide
cs-410_3_7_87,"00:06:16,550","00:06:19,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,Ambiguity is a main killer.
cs-410_3_7_88,"00:06:19,380","00:06:22,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,Meaning that in every step
cs-410_3_7_89,"00:06:22,820","00:06:26,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,and the computer would have to
cs-410_3_7_90,"00:06:26,790","00:06:30,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,that decision can be very difficult
cs-410_3_7_91,"00:06:31,690","00:06:32,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"And in general,"
cs-410_3_7_92,"00:06:32,300","00:06:37,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,we need common sense reasoning in order
cs-410_3_7_93,"00:06:37,530","00:06:40,595",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,And computers today don't yet have that.
cs-410_3_7_94,"00:06:40,595","00:06:42,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,That's why it's very hard for
cs-410_3_7_95,"00:06:42,820","00:06:47,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,computers to precisely understand
cs-410_3_7_96,"00:06:48,310","00:06:51,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,So here are some specific
cs-410_3_7_97,"00:06:51,280","00:06:53,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Think about the world-level ambiguity.
cs-410_3_7_98,"00:06:53,390","00:06:56,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,A word like design can be a noun or
cs-410_3_7_99,"00:06:56,940","00:06:59,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,we've got ambiguous part of speech tag.
cs-410_3_7_100,"00:07:00,980","00:07:06,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"Root also has multiple meanings,"
cs-410_3_7_101,"00:07:06,190","00:07:10,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"like in the square of, or"
cs-410_3_7_102,"00:07:12,310","00:07:17,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,Syntactic ambiguity refers
cs-410_3_7_103,"00:07:19,440","00:07:21,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,of a sentence in terms structures.
cs-410_3_7_104,"00:07:21,670","00:07:23,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,"So for example,"
cs-410_3_7_105,"00:07:23,010","00:07:26,219",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,natural language processing can
cs-410_3_7_106,"00:07:28,240","00:07:33,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So one is the ordinary meaning that we
cs-410_3_7_107,"00:07:33,410","00:07:38,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,will be getting as we're
cs-410_3_7_108,"00:07:38,690","00:07:41,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"So, it's processing of natural language."
cs-410_3_7_109,"00:07:41,670","00:07:44,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,But there's is also another
cs-410_3_7_110,"00:07:44,600","00:07:47,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,which is to say language
cs-410_3_7_111,"00:07:48,950","00:07:53,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"Now we don't generally have this problem,"
cs-410_3_7_112,"00:07:53,500","00:07:56,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"the structure, the computer would have"
cs-410_3_7_113,"00:07:59,040","00:08:03,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,Another classic example is a man
cs-410_3_7_114,"00:08:03,530","00:08:10,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,And this ambiguity lies in
cs-410_3_7_115,"00:08:10,230","00:08:13,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,This is called a prepositional
cs-410_3_7_116,"00:08:14,960","00:08:20,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,Meaning where to attach this
cs-410_3_7_117,"00:08:20,440","00:08:22,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,Should it modify the boy?
cs-410_3_7_118,"00:08:22,670","00:08:28,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"Or should it be modifying, saw, the verb."
cs-410_3_7_119,"00:08:28,330","00:08:31,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,Another problem is anaphora resolution.
cs-410_3_7_120,"00:08:31,330","00:08:35,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,In John persuaded Bill to buy a TV for
cs-410_3_7_121,"00:08:35,740","00:08:37,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,Does himself refer to John or Bill?
cs-410_3_7_122,"00:08:39,380","00:08:41,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,Presupposition is another difficulty.
cs-410_3_7_123,"00:08:41,790","00:08:45,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,He has quit smoking implies
cs-410_3_7_124,"00:08:45,459","00:08:50,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,we need to have such a knowledge in
cs-410_3_7_125,"00:08:52,630","00:08:57,614",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"Because of these problems, the state"
cs-410_3_7_126,"00:08:57,614","00:09:01,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,techniques can not do anything perfectly.
cs-410_3_7_127,"00:09:01,410","00:09:04,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,Even for
cs-410_3_7_128,"00:09:04,560","00:09:07,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,we still can not solve the whole problem.
cs-410_3_7_129,"00:09:07,700","00:09:12,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"The accuracy that are listed here,"
cs-410_3_7_130,"00:09:12,930","00:09:16,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,was just taken from some studies earlier.
cs-410_3_7_131,"00:09:17,330","00:09:22,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,And these studies obviously have to
cs-410_3_7_132,"00:09:22,840","00:09:27,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,the numbers here are not
cs-410_3_7_133,"00:09:27,640","00:09:33,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,take it out of the context of the data
cs-410_3_7_134,"00:09:33,210","00:09:39,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,But I show these numbers mainly to give
cs-410_3_7_135,"00:09:39,350","00:09:42,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,or how well we can do things like this.
cs-410_3_7_136,"00:09:42,080","00:09:47,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,It doesn't mean any data set
cs-410_3_7_137,"00:09:47,670","00:09:52,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"But, in general, we can do parsing speech"
cs-410_3_7_138,"00:09:53,980","00:09:59,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,"Parsing would be more difficult, but for"
cs-410_3_7_139,"00:09:59,030","00:10:04,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"phrases correct, we can probably"
cs-410_3_7_140,"00:10:06,920","00:10:12,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,But to get the complete parse tree
cs-410_3_7_141,"00:10:13,610","00:10:18,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,"For semantic analysis, we can also do"
cs-410_3_7_142,"00:10:18,210","00:10:22,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"particularly, extraction of entities and"
cs-410_3_7_143,"00:10:22,570","00:10:27,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"For example, recognizing this is"
cs-410_3_7_144,"00:10:27,910","00:10:33,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,this person and
cs-410_3_7_145,"00:10:33,380","00:10:36,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,We can also do word sense to some extent.
cs-410_3_7_146,"00:10:38,000","00:10:45,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,The occurrence of root in this sentence
cs-410_3_7_147,"00:10:45,360","00:10:49,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,Sentiment analysis is another aspect
cs-410_3_7_148,"00:10:50,480","00:10:55,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,That means we can tag the senses
cs-410_3_7_149,"00:10:55,840","00:11:00,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,it's talking about the product or
cs-410_3_7_150,"00:11:02,790","00:11:08,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"Inference, however, is very hard,"
cs-410_3_7_151,"00:11:08,600","00:11:14,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,any big domain and if it's only
cs-410_3_7_152,"00:11:14,040","00:11:18,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,And that's a generally difficult
cs-410_3_7_153,"00:11:18,800","00:11:21,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,Speech act analysis is
cs-410_3_7_154,"00:11:21,961","00:11:26,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,we can only do this probably for
cs-410_3_7_155,"00:11:26,480","00:11:32,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,And with a lot of help from humans
cs-410_3_7_156,"00:11:32,090","00:11:34,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,the computers to learn from.
cs-410_3_7_157,"00:11:36,380","00:11:38,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,So the slide also shows that
cs-410_3_7_158,"00:11:38,890","00:11:44,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,computers are far from being able to
cs-410_3_7_159,"00:11:44,300","00:11:50,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,And that also explains why the text
cs-410_3_7_160,"00:11:50,320","00:11:54,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,Because we cannot rely on
cs-410_3_7_161,"00:11:54,390","00:11:58,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,computational methods to
cs-410_3_7_162,"00:11:58,940","00:12:04,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,"Therefore, we have to use"
cs-410_3_7_163,"00:12:04,770","00:12:10,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,A particular statistical machine learning
cs-410_3_7_164,"00:12:10,090","00:12:16,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,to try to get as much meaning
cs-410_3_7_165,"00:12:16,092","00:12:19,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"And, later you will see"
cs-410_3_7_166,"00:12:20,360","00:12:25,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,many such algorithms
cs-410_3_7_167,"00:12:25,450","00:12:30,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,interesting model from text even though
cs-410_3_7_168,"00:12:30,790","00:12:36,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,Meaning of all the natural
cs-410_3_7_169,"00:12:36,010","00:12:46,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,[MUSIC]
cs-410_2_7_1,"00:00:00,266","00:00:09,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_7_2,"00:00:09,956","00:00:14,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,looking at the text mining problem more
cs-410_2_7_3,"00:00:14,850","00:00:20,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"similar to general data mining, except"
cs-410_2_7_4,"00:00:21,710","00:00:26,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,And we're going to have text mining
cs-410_2_7_5,"00:00:26,400","00:00:32,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,into actionable knowledge that
cs-410_2_7_6,"00:00:32,240","00:00:34,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"especially for decision making, or"
cs-410_2_7_7,"00:00:34,130","00:00:39,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,for completing whatever tasks that
cs-410_2_7_8,"00:00:39,350","00:00:45,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Because, in general,"
cs-410_2_7_9,"00:00:45,000","00:00:49,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,we also tend to have other kinds
cs-410_2_7_10,"00:00:49,720","00:00:54,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So a more general picture would be
cs-410_2_7_11,"00:00:56,000","00:01:00,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And for this reason we might be
cs-410_2_7_12,"00:01:00,875","00:01:02,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,non-text data.
cs-410_2_7_13,"00:01:02,060","00:01:05,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,And so in this course we're
cs-410_2_7_14,"00:01:05,860","00:01:10,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,but we're also going to also touch how do
cs-410_2_7_15,"00:01:10,628","00:01:12,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,non-text data.
cs-410_2_7_16,"00:01:12,450","00:01:16,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,With this problem definition we
cs-410_2_7_17,"00:01:16,630","00:01:19,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,the topics in text mining and analytics.
cs-410_2_7_18,"00:01:21,010","00:01:25,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Now this slide shows the process of
cs-410_2_7_19,"00:01:27,018","00:01:29,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"More specifically, a human sensor or"
cs-410_2_7_20,"00:01:29,800","00:01:33,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,human observer would look at
cs-410_2_7_21,"00:01:34,660","00:01:38,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,Different people would be looking at
cs-410_2_7_22,"00:01:38,820","00:01:41,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,they'll pay attention to different things.
cs-410_2_7_23,"00:01:41,210","00:01:46,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,The same person at different times might
cs-410_2_7_24,"00:01:46,090","00:01:50,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,of the observed world.
cs-410_2_7_25,"00:01:50,990","00:01:55,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,And so the humans are able to perceive
cs-410_2_7_26,"00:01:55,450","00:02:01,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"And that human, the sensor,"
cs-410_2_7_27,"00:02:01,480","00:02:05,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,And that can be called the Observed World.
cs-410_2_7_28,"00:02:05,150","00:02:10,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"Of course, this would be different from"
cs-410_2_7_29,"00:02:10,040","00:02:14,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,that the person has taken
cs-410_2_7_30,"00:02:16,840","00:02:22,642",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,Now the Observed World can be
cs-410_2_7_31,"00:02:22,642","00:02:27,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,entity-relation graphs or
cs-410_2_7_32,"00:02:27,535","00:02:31,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,using knowledge representation language.
cs-410_2_7_33,"00:02:31,890","00:02:39,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"But in general, this is basically what"
cs-410_2_7_34,"00:02:39,190","00:02:43,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,And we don't really know what
cs-410_2_7_35,"00:02:43,800","00:02:48,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,But then the human would
cs-410_2_7_36,"00:02:48,250","00:02:52,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"observed using a natural language,"
cs-410_2_7_37,"00:02:52,920","00:02:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,And the result is text data.
cs-410_2_7_38,"00:02:55,870","00:03:00,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,Of course a person could have used
cs-410_2_7_39,"00:03:00,610","00:03:02,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,she has observed.
cs-410_2_7_40,"00:03:02,660","00:03:08,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,In that case we might have text data of
cs-410_2_7_41,"00:03:10,590","00:03:15,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,The main goal of text mining
cs-410_2_7_42,"00:03:15,790","00:03:19,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,process of generating text data.
cs-410_2_7_43,"00:03:19,280","00:03:24,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,We hope to be able to uncover
cs-410_2_7_44,"00:03:28,340","00:03:34,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"Specifically, we can think about mining,"
cs-410_2_7_45,"00:03:35,560","00:03:40,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,And that means by looking at text data
cs-410_2_7_46,"00:03:40,130","00:03:46,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,"something about English, some usage"
cs-410_2_7_47,"00:03:47,780","00:03:52,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"So this is one type of mining problems,"
cs-410_2_7_48,"00:03:52,340","00:03:57,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,some knowledge about language which
cs-410_2_7_49,"00:03:58,920","00:04:00,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,"If you look at the picture,"
cs-410_2_7_50,"00:04:00,620","00:04:06,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,we can also then mine knowledge
cs-410_2_7_51,"00:04:06,380","00:04:10,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,And so this has much to do with
cs-410_2_7_52,"00:04:11,490","00:04:15,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,We're going to look at what the text
cs-410_2_7_53,"00:04:15,640","00:04:20,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,get the essence of it or
cs-410_2_7_54,"00:04:20,820","00:04:25,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,about a particular aspect of
cs-410_2_7_55,"00:04:26,900","00:04:30,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"For example, everything that has been"
cs-410_2_7_56,"00:04:30,890","00:04:31,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,a particular entity.
cs-410_2_7_57,"00:04:31,630","00:04:36,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,And this can be regarded as mining content
cs-410_2_7_58,"00:04:36,550","00:04:43,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,to describe the observed world in
cs-410_2_7_59,"00:04:45,020","00:04:50,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"If you look further,"
cs-410_2_7_60,"00:04:50,060","00:04:54,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"we can mine knowledge about this observer,"
cs-410_2_7_61,"00:04:54,710","00:05:00,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,So this has also to do with
cs-410_2_7_62,"00:05:00,630","00:05:02,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,some properties of this person.
cs-410_2_7_63,"00:05:03,380","00:05:07,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,And these properties could
cs-410_2_7_64,"00:05:07,410","00:05:09,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,sentiment of the person.
cs-410_2_7_65,"00:05:10,200","00:05:15,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,And note that we distinguish
cs-410_2_7_66,"00:05:15,250","00:05:21,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,because text data can't describe what the
cs-410_2_7_67,"00:05:21,040","00:05:25,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,But the description can be also
cs-410_2_7_68,"00:05:25,960","00:05:30,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"in general, you can imagine the text"
cs-410_2_7_69,"00:05:30,280","00:05:34,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,descriptions of the world plus
cs-410_2_7_70,"00:05:34,770","00:05:37,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So that's why it's also possible to
cs-410_2_7_71,"00:05:37,330","00:05:41,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,do text mining to mine
cs-410_2_7_72,"00:05:41,970","00:05:45,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"Finally, if you look at the picture"
cs-410_2_7_73,"00:05:45,980","00:05:50,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,then you can see we can certainly also
cs-410_2_7_74,"00:05:50,150","00:05:50,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,Right?
cs-410_2_7_75,"00:05:50,880","00:05:56,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,So indeed we can do text mining to
cs-410_2_7_76,"00:05:56,860","00:05:59,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,And this is often called
cs-410_2_7_77,"00:06:00,600","00:06:04,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,And we want to predict the value
cs-410_2_7_78,"00:06:04,000","00:06:08,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"So, this picture basically covered"
cs-410_2_7_79,"00:06:08,935","00:06:13,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,multiple types of knowledge that
cs-410_2_7_80,"00:06:14,550","00:06:19,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,When we infer other
cs-410_2_7_81,"00:06:19,260","00:06:24,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,could also use some of the results from
cs-410_2_7_82,"00:06:24,990","00:06:30,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,mining text data as intermediate
cs-410_2_7_83,"00:06:30,910","00:06:31,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,"For example,"
cs-410_2_7_84,"00:06:31,940","00:06:38,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,after we mine the content of text data we
cs-410_2_7_85,"00:06:38,050","00:06:41,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,And that summary could be then used
cs-410_2_7_86,"00:06:41,190","00:06:45,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,to help us predict the variables
cs-410_2_7_87,"00:06:45,003","00:06:51,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,Now of course this is still generated
cs-410_2_7_88,"00:06:51,940","00:06:58,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,but I want to emphasize here that
cs-410_2_7_89,"00:06:58,410","00:07:03,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,to generate some features that can help
cs-410_2_7_90,"00:07:04,960","00:07:10,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,And that's why here we show the results of
cs-410_2_7_91,"00:07:10,100","00:07:15,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"some other mining tasks, including"
cs-410_2_7_92,"00:07:15,010","00:07:19,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"mining knowledge about the observer,"
cs-410_2_7_93,"00:07:21,380","00:07:26,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,"In fact, when we have non-text data,"
cs-410_2_7_94,"00:07:26,690","00:07:31,496",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"data to help prediction, and"
cs-410_2_7_95,"00:07:31,496","00:07:39,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"In general, non-text data can be very"
cs-410_2_7_96,"00:07:39,260","00:07:44,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"For example,"
cs-410_2_7_97,"00:07:44,530","00:07:49,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,changes of stock prices based on
cs-410_2_7_98,"00:07:49,870","00:07:53,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"in social media, then this is an example"
cs-410_2_7_99,"00:07:53,730","00:07:58,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,of using text data to predict
cs-410_2_7_100,"00:07:58,520","00:07:59,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"But in this case, obviously,"
cs-410_2_7_101,"00:07:59,950","00:08:04,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,the historical stock price data would
cs-410_2_7_102,"00:08:04,480","00:08:09,633",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,And so that's an example of
cs-410_2_7_103,"00:08:09,633","00:08:13,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,useful for the prediction.
cs-410_2_7_104,"00:08:13,750","00:08:17,161",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,And we're going to combine both kinds
cs-410_2_7_105,"00:08:17,161","00:08:24,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,Now non-text data can be also used for
cs-410_2_7_106,"00:08:25,580","00:08:27,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,"When we look at the text data alone,"
cs-410_2_7_107,"00:08:27,050","00:08:31,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,we'll be mostly looking at the content
cs-410_2_7_108,"00:08:32,790","00:08:36,149",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,But text data generally also
cs-410_2_7_109,"00:08:37,470","00:08:44,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,"For example, the time and the location"
cs-410_2_7_110,"00:08:44,150","00:08:47,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,And these are useful context information.
cs-410_2_7_111,"00:08:48,740","00:08:54,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,And the context can provide interesting
cs-410_2_7_112,"00:08:54,020","00:08:57,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"For example, we might partition text"
cs-410_2_7_113,"00:08:57,980","00:09:00,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,because of the availability of the time.
cs-410_2_7_114,"00:09:00,680","00:09:06,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,Now we can analyze text data in each
cs-410_2_7_115,"00:09:06,480","00:09:09,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,Similarly we can partition text
cs-410_2_7_116,"00:09:09,970","00:09:15,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,any meta data that's associated to
cs-410_2_7_117,"00:09:15,920","00:09:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"So, in this sense,"
cs-410_2_7_118,"00:09:20,980","00:09:24,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,interesting angles or
cs-410_2_7_119,"00:09:24,580","00:09:29,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,And it can help us make context-sensitive
cs-410_2_7_120,"00:09:29,340","00:09:33,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,analysis of content or
cs-410_2_7_121,"00:09:36,390","00:09:42,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,the opinions about the observer or
cs-410_2_7_122,"00:09:42,920","00:09:46,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,We could analyze the sentiment
cs-410_2_7_123,"00:09:46,920","00:09:54,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,So this is a fairly general landscape of
cs-410_2_7_124,"00:09:54,500","00:09:59,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,In this course we're going to
cs-410_2_7_125,"00:09:59,850","00:10:03,796",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,We actually hope to cover
cs-410_2_7_126,"00:10:06,675","00:10:11,321",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,First we're going to cover
cs-410_2_7_127,"00:10:11,321","00:10:16,053",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,briefly because this has to do
cs-410_2_7_128,"00:10:16,053","00:10:21,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,this determines how we can represent
cs-410_2_7_129,"00:10:21,580","00:10:27,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,"Second, we're going to talk about how to"
cs-410_2_7_130,"00:10:27,870","00:10:34,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,And word associations is a form of use for
cs-410_2_7_131,"00:10:34,340","00:10:38,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"Third, we're going to talk about"
cs-410_2_7_132,"00:10:38,340","00:10:43,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,And this is only one way to
cs-410_2_7_133,"00:10:43,190","00:10:46,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,it's a very useful ways
cs-410_2_7_134,"00:10:46,100","00:10:51,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,It's also one of the most useful
cs-410_2_7_135,"00:10:53,750","00:10:59,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,Then we're going to talk about
cs-410_2_7_136,"00:10:59,510","00:11:05,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So this can be regarded as one example
cs-410_2_7_137,"00:11:07,140","00:11:11,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,And finally we're going to
cs-410_2_7_138,"00:11:11,510","00:11:16,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,problems where we try to predict some
cs-410_2_7_139,"00:11:17,400","00:11:24,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,So this slide also serves as
cs-410_2_7_140,"00:11:24,880","00:11:27,554",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,And we're going to use
cs-410_2_7_141,"00:11:27,554","00:11:30,962",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,the topics that we'll cover
cs-410_2_7_142,"00:11:30,962","00:11:40,962",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,[MUSIC]
cs-410_7_7_1,"00:00:00,025","00:00:04,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is
cs-410_7_7_2,"00:00:04,546","00:00:10,323",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=4,about the word association
cs-410_7_7_3,"00:00:10,323","00:00:15,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,mining and analysis.
cs-410_7_7_4,"00:00:15,100","00:00:19,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"In this lecture,"
cs-410_7_7_5,"00:00:19,884","00:00:22,902",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,associations of words from text.
cs-410_7_7_6,"00:00:22,902","00:00:27,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,Now this is an example of knowledge
cs-410_7_7_7,"00:00:27,900","00:00:29,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,we can mine from text data.
cs-410_7_7_8,"00:00:33,942","00:00:35,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,Here's the outline.
cs-410_7_7_9,"00:00:35,090","00:00:39,828",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,We're going to first talk about
cs-410_7_7_10,"00:00:39,828","00:00:45,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,then explain why discovering such
cs-410_7_7_11,"00:00:45,100","00:00:50,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,we're going to talk about some general
cs-410_7_7_12,"00:00:50,070","00:00:55,209",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,In general there are two word
cs-410_7_7_13,"00:00:56,680","00:00:58,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,One is called a paradigmatic relation.
cs-410_7_7_14,"00:00:58,680","00:01:03,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,The other is syntagmatic relation.
cs-410_7_7_15,"00:01:03,000","00:01:07,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,A and B have paradigmatic relation
cs-410_7_7_16,"00:01:07,780","00:01:11,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,if they can be substituted for each other.
cs-410_7_7_17,"00:01:11,700","00:01:17,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,That means the two words that
cs-410_7_7_18,"00:01:17,910","00:01:23,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,"would be in the same semantic class,"
cs-410_7_7_19,"00:01:23,130","00:01:26,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And we can in general
cs-410_7_7_20,"00:01:26,910","00:01:30,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,without affecting
cs-410_7_7_21,"00:01:30,310","00:01:33,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,That means we would still
cs-410_7_7_22,"00:01:33,810","00:01:41,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"For example, cat and dog, these two"
cs-410_7_7_23,"00:01:41,530","00:01:47,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,because they are in
cs-410_7_7_24,"00:01:47,710","00:01:51,827",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And in general,"
cs-410_7_7_25,"00:01:51,827","00:01:56,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,the sentence would still be a valid
cs-410_7_7_26,"00:01:58,320","00:02:01,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,Similarly Monday and
cs-410_7_7_27,"00:02:04,930","00:02:09,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,The second kind of relation is
cs-410_7_7_28,"00:02:10,610","00:02:17,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"In this case, the two words that have this"
cs-410_7_7_29,"00:02:17,200","00:02:22,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,So A and B have syntagmatic relation if
cs-410_7_7_30,"00:02:22,190","00:02:29,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"a sentence, that means these two"
cs-410_7_7_31,"00:02:30,720","00:02:36,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,"So for example, cat and sit are related"
cs-410_7_7_32,"00:02:38,060","00:02:43,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"Similarly, car and"
cs-410_7_7_33,"00:02:43,870","00:02:47,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,they can be combined with
cs-410_7_7_34,"00:02:47,550","00:02:54,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"However, in general, we can not"
cs-410_7_7_35,"00:02:54,150","00:02:59,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,car with drive in the sentence
cs-410_7_7_36,"00:02:59,590","00:03:03,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"meaning that if we do that, the sentence"
cs-410_7_7_37,"00:03:03,950","00:03:10,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,So this is different from
cs-410_7_7_38,"00:03:10,135","00:03:15,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,And these two relations are in fact so
cs-410_7_7_39,"00:03:17,365","00:03:24,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,generalized to capture basic relations
cs-410_7_7_40,"00:03:24,180","00:03:27,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,And definitely they can be
cs-410_7_7_41,"00:03:27,880","00:03:31,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,relations of any items in a language.
cs-410_7_7_42,"00:03:31,630","00:03:36,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"So, A and B don't have to be words and"
cs-410_7_7_43,"00:03:37,960","00:03:44,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,And they can even be more complex
cs-410_7_7_44,"00:03:44,710","00:03:48,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,If you think about the general
cs-410_7_7_45,"00:03:48,820","00:03:53,066",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,then we can think about the units
cs-410_7_7_46,"00:03:53,066","00:03:58,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,Then we think of paradigmatic
cs-410_7_7_47,"00:03:58,980","00:04:05,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,are applied to units that tend to occur
cs-410_7_7_48,"00:04:05,890","00:04:11,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,or in a sequence of data
cs-410_7_7_49,"00:04:11,660","00:04:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So they occur in similar locations
cs-410_7_7_50,"00:04:20,980","00:04:25,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,Syntagmatical relation on
cs-410_7_7_51,"00:04:25,415","00:04:30,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,co-occurrent elements that tend
cs-410_7_7_52,"00:04:33,150","00:04:38,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,So these two are complimentary and
cs-410_7_7_53,"00:04:38,470","00:04:42,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,And we're interested in discovering
cs-410_7_7_54,"00:04:42,810","00:04:46,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,Discovering such worded
cs-410_7_7_55,"00:04:47,480","00:04:52,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"First, such relations can be directly"
cs-410_7_7_56,"00:04:52,920","00:04:58,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"tasks, and this is because this is part"
cs-410_7_7_57,"00:04:58,880","00:05:02,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,So if you know these two words
cs-410_7_7_58,"00:05:02,440","00:05:04,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,and then you can help a lot of tasks.
cs-410_7_7_59,"00:05:05,980","00:05:10,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,And grammar learning can be also
cs-410_7_7_60,"00:05:10,970","00:05:15,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,Because if we can learn
cs-410_7_7_61,"00:05:15,130","00:05:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"then we form classes of words,"
cs-410_7_7_62,"00:05:20,000","00:05:25,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"And if we learn syntagmatic relations,"
cs-410_7_7_63,"00:05:25,630","00:05:32,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,the rules for putting together a larger
cs-410_7_7_64,"00:05:32,400","00:05:37,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So we learn the structure and
cs-410_7_7_65,"00:05:39,855","00:05:43,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,Word relations can be also very useful for
cs-410_7_7_66,"00:05:43,070","00:05:46,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,many applications in text retrieval and
cs-410_7_7_67,"00:05:46,580","00:05:50,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,"For example, in search and"
cs-410_7_7_68,"00:05:50,520","00:05:55,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"associations to modify a query,"
cs-410_7_7_69,"00:05:55,930","00:06:00,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,introduce additional related words into
cs-410_7_7_70,"00:06:01,590","00:06:03,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,It's often called a query expansion.
cs-410_7_7_71,"00:06:05,290","00:06:10,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,Or you can use related words to
cs-410_7_7_72,"00:06:10,030","00:06:11,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,to explore the information space.
cs-410_7_7_73,"00:06:12,740","00:06:15,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,Another application is to
cs-410_7_7_74,"00:06:15,610","00:06:19,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,automatically construct the top
cs-410_7_7_75,"00:06:19,790","00:06:24,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,We can have words as nodes and
cs-410_7_7_76,"00:06:24,540","00:06:27,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,A user could navigate from
cs-410_7_7_77,"00:06:28,990","00:06:31,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,find information in the information space.
cs-410_7_7_78,"00:06:33,620","00:06:40,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"Finally, such word associations can also"
cs-410_7_7_79,"00:06:40,620","00:06:45,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"For example, we might be interested"
cs-410_7_7_80,"00:06:45,680","00:06:48,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,negative opinions about the iPhone 6.
cs-410_7_7_81,"00:06:48,620","00:06:55,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"In order to do that, we can look at what"
cs-410_7_7_82,"00:06:55,180","00:07:01,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,a feature word like battery in
cs-410_7_7_83,"00:07:01,630","00:07:05,147",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Such a syntagmatical
cs-410_7_7_84,"00:07:05,147","00:07:08,854",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,show the detailed opinions
cs-410_7_7_85,"00:07:16,696","00:07:20,837",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"So, how can we discover such"
cs-410_7_7_86,"00:07:20,837","00:07:24,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"Now, here are some intuitions"
cs-410_7_7_87,"00:07:24,450","00:07:27,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,Now let's first look at
cs-410_7_7_88,"00:07:29,080","00:07:32,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,Here we essentially can take
cs-410_7_7_89,"00:07:34,150","00:07:38,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,So here you see some simple
cs-410_7_7_90,"00:07:38,440","00:07:43,416",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,You can see they generally
cs-410_7_7_91,"00:07:43,416","00:07:48,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,and that after all is the definition
cs-410_7_7_92,"00:07:49,540","00:07:54,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,On the right side you can kind
cs-410_7_7_93,"00:07:54,510","00:07:59,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,the context of cat and
cs-410_7_7_94,"00:08:00,640","00:08:05,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,I've taken away cat and
cs-410_7_7_95,"00:08:05,230","00:08:07,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,that you can see just the context.
cs-410_7_7_96,"00:08:08,810","00:08:12,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Now, of course we can have different"
cs-410_7_7_97,"00:08:13,810","00:08:19,528",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"For example, we can look at"
cs-410_7_7_98,"00:08:19,528","00:08:24,222",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,part of this context.
cs-410_7_7_99,"00:08:24,222","00:08:28,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,So we can call this left context.
cs-410_7_7_100,"00:08:28,000","00:08:34,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,What words occur before we see cat or dog?
cs-410_7_7_101,"00:08:34,800","00:08:39,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,"So, you can see in this case, clearly"
cs-410_7_7_102,"00:08:41,810","00:08:47,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,You generally say his cat or my cat and
cs-410_7_7_103,"00:08:47,860","00:08:52,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So that makes them similar
cs-410_7_7_104,"00:08:53,660","00:08:58,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"Similarly, if you look at the words"
cs-410_7_7_105,"00:08:58,880","00:09:03,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"which we can call right context,"
cs-410_7_7_106,"00:09:03,970","00:09:07,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"Of course, it's an extreme case,"
cs-410_7_7_107,"00:09:08,670","00:09:12,883",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"And in general,"
cs-410_7_7_108,"00:09:12,883","00:09:15,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,that can't follow cat and dog.
cs-410_7_7_109,"00:09:17,830","00:09:21,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,You can also even look
cs-410_7_7_110,"00:09:21,700","00:09:24,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,And that might include all
cs-410_7_7_111,"00:09:24,690","00:09:26,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,in sentences around this word.
cs-410_7_7_112,"00:09:27,658","00:09:34,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"And even in the general context, you also"
cs-410_7_7_113,"00:09:35,400","00:09:41,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,So this was just a suggestion
cs-410_7_7_114,"00:09:41,480","00:09:47,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,relation by looking at
cs-410_7_7_115,"00:09:47,000","00:09:50,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"So, for example,"
cs-410_7_7_116,"00:09:50,900","00:09:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,How similar are context of cat and
cs-410_7_7_117,"00:09:56,240","00:10:01,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,In contrast how similar are context
cs-410_7_7_118,"00:10:02,660","00:10:07,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,"Now, intuitively,"
cs-410_7_7_119,"00:10:07,610","00:10:11,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,the context of dog would
cs-410_7_7_120,"00:10:11,030","00:10:16,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,the context of cat and
cs-410_7_7_121,"00:10:16,550","00:10:20,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"That means, in the first case"
cs-410_7_7_122,"00:10:21,910","00:10:25,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,between the context of cat and
cs-410_7_7_123,"00:10:25,940","00:10:30,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,the similarity between context of cat and
cs-410_7_7_124,"00:10:30,248","00:10:35,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,because they all not having a paradigmatic
cs-410_7_7_125,"00:10:35,750","00:10:40,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,relationship and imagine what words
cs-410_7_7_126,"00:10:40,550","00:10:44,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,It would be very different from
cs-410_7_7_127,"00:10:46,620","00:10:50,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,So this is the basic idea of what
cs-410_7_7_128,"00:10:52,040","00:10:54,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,What about the syntagmatic relation?
cs-410_7_7_129,"00:10:54,180","00:10:58,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"Well, here we're going to explore"
cs-410_7_7_130,"00:10:58,550","00:11:02,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,again based on the definition
cs-410_7_7_131,"00:11:03,990","00:11:05,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,Here you see the same sample of text.
cs-410_7_7_132,"00:11:06,640","00:11:10,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,But here we're interested in knowing
cs-410_7_7_133,"00:11:10,710","00:11:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,with the verb eats and
cs-410_7_7_134,"00:11:16,380","00:11:20,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,And if you look at the right
cs-410_7_7_135,"00:11:20,880","00:11:25,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,"you see,"
cs-410_7_7_136,"00:11:27,110","00:11:30,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,I've taken away the word to its left and
cs-410_7_7_137,"00:11:30,140","00:11:33,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,also the word to its
cs-410_7_7_138,"00:11:35,340","00:11:41,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,"And then we ask the question, what words"
cs-410_7_7_139,"00:11:43,650","00:11:47,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,And what words tend to
cs-410_7_7_140,"00:11:49,560","00:11:54,997",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,Now thinking about this question
cs-410_7_7_141,"00:11:54,997","00:12:00,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,relations because syntagmatic relations
cs-410_7_7_142,"00:12:03,070","00:12:07,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,So the important question to ask for
cs-410_7_7_143,"00:12:07,290","00:12:14,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"whenever eats occurs,"
cs-410_7_7_144,"00:12:16,180","00:12:19,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,So the question here has
cs-410_7_7_145,"00:12:19,120","00:12:23,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,are some other words that tend
cs-410_7_7_146,"00:12:23,940","00:12:28,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,Meaning that whenever you see eats
cs-410_7_7_147,"00:12:29,620","00:12:34,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"And if you don't see eats, probably,"
cs-410_7_7_148,"00:12:36,560","00:12:40,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,So this intuition can help
cs-410_7_7_149,"00:12:41,530","00:12:43,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"Now again, consider example."
cs-410_7_7_150,"00:12:44,210","00:12:48,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,How helpful is occurrence of eats for
cs-410_7_7_151,"00:12:49,870","00:12:53,056",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,Right.
cs-410_7_7_152,"00:12:53,056","00:12:58,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,in a sentence would generally help us
cs-410_7_7_153,"00:12:58,930","00:13:01,801",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,"And if we see eats occur in the sentence,"
cs-410_7_7_154,"00:13:01,801","00:13:05,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,that should increase the chance
cs-410_7_7_155,"00:13:08,490","00:13:12,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"In contrast,"
cs-410_7_7_156,"00:13:12,150","00:13:15,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,how helpful is the occurrence of eats for
cs-410_7_7_157,"00:13:17,330","00:13:20,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,Because eats and
cs-410_7_7_158,"00:13:20,270","00:13:24,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,knowing whether eats occurred
cs-410_7_7_159,"00:13:24,840","00:13:30,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,"really help us predict the weather,"
cs-410_7_7_160,"00:13:30,140","00:13:34,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,So this is in contrast to
cs-410_7_7_161,"00:13:35,550","00:13:38,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,This also helps explain that intuition
cs-410_7_7_162,"00:13:38,790","00:13:43,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,behind the methods of what
cs-410_7_7_163,"00:13:43,100","00:13:49,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,Mainly we need to capture the correlation
cs-410_7_7_164,"00:13:50,440","00:13:52,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,So to summarize the general ideas for
cs-410_7_7_165,"00:13:52,860","00:13:55,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,discovering word associations
cs-410_7_7_166,"00:13:56,880","00:14:02,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,"For paradigmatic relation,"
cs-410_7_7_167,"00:14:02,240","00:14:04,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,And then compute its context similarity.
cs-410_7_7_168,"00:14:04,830","00:14:09,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,We're going to assume the words
cs-410_7_7_169,"00:14:09,030","00:14:12,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,to have paradigmatic relation.
cs-410_7_7_170,"00:14:14,640","00:14:19,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,"For syntagmatic relation, we will count"
cs-410_7_7_171,"00:14:19,970","00:14:25,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"in a context, which can be a sentence,"
cs-410_7_7_172,"00:14:25,180","00:14:28,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,And we're going to compare
cs-410_7_7_173,"00:14:28,180","00:14:31,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=868,their co-occurrences with
cs-410_7_7_174,"00:14:33,280","00:14:36,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,We're going to assume words
cs-410_7_7_175,"00:14:36,660","00:14:42,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=876,relatively low individual occurrences
cs-410_7_7_176,"00:14:42,335","00:14:46,581",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,because they attempt to occur together and
cs-410_7_7_177,"00:14:46,581","00:14:51,635",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,Note that the paradigmatic relation and
cs-410_7_7_178,"00:14:51,635","00:14:57,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,are actually closely related
cs-410_7_7_179,"00:14:57,065","00:15:02,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,related words tend to have syntagmatic
cs-410_7_7_180,"00:15:02,810","00:15:05,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,They tend to be associated
cs-410_7_7_181,"00:15:05,420","00:15:10,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=905,that suggests that we can also do join
cs-410_7_7_182,"00:15:10,870","00:15:15,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,So these general ideas can be
cs-410_7_7_183,"00:15:15,190","00:15:19,129",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=915,"And the course won't cover all of them,"
cs-410_7_7_184,"00:15:19,129","00:15:24,774",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,we will cover at least some of
cs-410_7_7_185,"00:15:24,774","00:15:27,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=924,discovering these relations.
cs-410_7_7_186,"00:15:27,669","00:15:37,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,[MUSIC]
cs-410_6_7_1,"00:00:00,532","00:00:08,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_6_7_2,"00:00:08,683","00:00:11,442",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,"So, as we explained the different text"
cs-410_6_7_3,"00:00:11,442","00:00:15,299",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,representation tends to
cs-410_6_7_4,"00:00:16,560","00:00:19,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In particular,"
cs-410_6_7_5,"00:00:19,780","00:00:24,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,more deeper analysis results
cs-410_6_7_6,"00:00:24,720","00:00:27,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,And that would open up a more
cs-410_6_7_7,"00:00:29,520","00:00:33,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,opportunities and
cs-410_6_7_8,"00:00:33,780","00:00:37,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"So, this table summarizes"
cs-410_6_7_9,"00:00:37,470","00:00:39,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,So the first column shows
cs-410_6_7_10,"00:00:39,800","00:00:44,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,The second visualizes the generality
cs-410_6_7_11,"00:00:44,820","00:00:48,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,Meaning whether we can do this
cs-410_6_7_12,"00:00:48,430","00:00:51,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,all the text data or only some of them.
cs-410_6_7_13,"00:00:51,880","00:00:54,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And the third column shows
cs-410_6_7_14,"00:00:56,040","00:01:00,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And the final column shows some
cs-410_6_7_15,"00:01:00,130","00:01:04,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,can be achieved through this
cs-410_6_7_16,"00:01:04,670","00:01:06,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,So let's take a look at them.
cs-410_6_7_17,"00:01:06,310","00:01:12,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,So as a stream text can only be processed
cs-410_6_7_18,"00:01:12,180","00:01:14,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"It's very robust, it's general."
cs-410_6_7_19,"00:01:15,100","00:01:17,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,And there was still some interesting
cs-410_6_7_20,"00:01:17,690","00:01:18,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,at this level.
cs-410_6_7_21,"00:01:18,290","00:01:20,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"For example, compression of text."
cs-410_6_7_22,"00:01:20,380","00:01:24,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,Doesn't necessarily need to
cs-410_6_7_23,"00:01:24,080","00:01:27,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,Although knowing word boundaries
cs-410_6_7_24,"00:01:28,540","00:01:32,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Word base repetition is a very
cs-410_6_7_25,"00:01:32,470","00:01:34,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,It's quite general and
cs-410_6_7_26,"00:01:34,630","00:01:39,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"relatively robust, indicating they"
cs-410_6_7_27,"00:01:39,140","00:01:44,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"Such as word relation analysis,"
cs-410_6_7_28,"00:01:44,480","00:01:48,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,And there are many applications that can
cs-410_6_7_29,"00:01:48,930","00:01:54,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"For example, thesaurus discovery has"
cs-410_6_7_30,"00:01:54,930","00:02:00,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,And topic and
cs-410_6_7_31,"00:02:00,550","00:02:03,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"And there are, for example, people"
cs-410_6_7_32,"00:02:03,360","00:02:08,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,might be interesting in knowing the major
cs-410_6_7_33,"00:02:08,190","00:02:12,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,And this can be the case
cs-410_6_7_34,"00:02:12,730","00:02:18,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,And scientists want to know what are the
cs-410_6_7_35,"00:02:18,500","00:02:22,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,Or customer service people might want to
cs-410_6_7_36,"00:02:22,950","00:02:28,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,customers by mining their e-mail messages.
cs-410_6_7_37,"00:02:28,480","00:02:33,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,And business intelligence
cs-410_6_7_38,"00:02:33,850","00:02:38,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,understanding consumers' opinions about
cs-410_6_7_39,"00:02:38,090","00:02:42,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,products to figure out what are the
cs-410_6_7_40,"00:02:43,170","00:02:47,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,"And, in general, there are many"
cs-410_6_7_41,"00:02:47,140","00:02:51,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,applications that can be enabled by
cs-410_6_7_42,"00:02:53,720","00:02:58,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"Now, moving down, we'll see we can"
cs-410_6_7_43,"00:02:58,550","00:03:01,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"By adding syntactical structures,"
cs-410_6_7_44,"00:03:01,640","00:03:03,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,syntactical graph analysis.
cs-410_6_7_45,"00:03:03,890","00:03:09,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,We can use graph mining algorithms
cs-410_6_7_46,"00:03:09,490","00:03:13,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,And some applications are related
cs-410_6_7_47,"00:03:13,550","00:03:14,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"For example,"
cs-410_6_7_48,"00:03:14,090","00:03:18,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,stylistic analysis generally requires
cs-410_6_7_49,"00:03:22,000","00:03:26,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,We can also generate
cs-410_6_7_50,"00:03:26,240","00:03:32,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,And those are features that might help us
cs-410_6_7_51,"00:03:32,090","00:03:37,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,categories by looking at the structures
cs-410_6_7_52,"00:03:37,320","00:03:39,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,It can be more accurate.
cs-410_6_7_53,"00:03:39,350","00:03:43,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"For example,"
cs-410_6_7_54,"00:03:45,120","00:03:49,298",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,different categories corresponding
cs-410_6_7_55,"00:03:49,298","00:03:56,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,You want to figure out which of
cs-410_6_7_56,"00:03:56,320","00:04:01,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"this article, then you generally need"
cs-410_6_7_57,"00:04:03,340","00:04:05,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"When we add entities and relations,"
cs-410_6_7_58,"00:04:05,400","00:04:09,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,then we can enable other techniques
cs-410_6_7_59,"00:04:09,690","00:04:13,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"answers, or information network and"
cs-410_6_7_60,"00:04:13,920","00:04:20,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,And this analysis enable
cs-410_6_7_61,"00:04:22,285","00:04:22,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"For example,"
cs-410_6_7_62,"00:04:22,875","00:04:27,525",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,discovery of all the knowledge and
cs-410_6_7_63,"00:04:28,865","00:04:31,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,You can also use this level representation
cs-410_6_7_64,"00:04:31,825","00:04:35,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,to integrate everything about
cs-410_6_7_65,"00:04:37,520","00:04:40,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"Finally, when we add logical predicates,"
cs-410_6_7_66,"00:04:40,280","00:04:44,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"that would enable large inference,"
cs-410_6_7_67,"00:04:44,330","00:04:46,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,And this can be very useful for
cs-410_6_7_68,"00:04:46,190","00:04:48,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,integrating analysis of
cs-410_6_7_69,"00:04:50,190","00:04:53,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"For example,"
cs-410_6_7_70,"00:04:54,920","00:04:58,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"extracted the information from text,"
cs-410_6_7_71,"00:04:59,830","00:05:04,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,A good of example of application in this
cs-410_6_7_72,"00:05:04,470","00:05:07,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,is a knowledge assistant for biologists.
cs-410_6_7_73,"00:05:07,375","00:05:14,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,And this program that can help a biologist
cs-410_6_7_74,"00:05:14,535","00:05:21,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,literature about a research problem such
cs-410_6_7_75,"00:05:22,070","00:05:27,143",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,And the computer can make inferences
cs-410_6_7_76,"00:05:27,143","00:05:32,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,about some of the hypothesis that
cs-410_6_7_77,"00:05:32,490","00:05:36,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"For example,"
cs-410_6_7_78,"00:05:36,110","00:05:42,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,then the intelligent program can read the
cs-410_6_7_79,"00:05:42,135","00:05:45,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,doing compiling and
cs-410_6_7_80,"00:05:45,250","00:05:50,891",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,And then using a logic system to
cs-410_6_7_81,"00:05:50,891","00:05:56,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,to researchers questioning about what
cs-410_6_7_82,"00:05:57,990","00:06:01,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,So in order to support
cs-410_6_7_83,"00:06:01,240","00:06:04,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,we need to go as far as
cs-410_6_7_84,"00:06:04,910","00:06:10,585",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"Now, this course is covering techniques"
cs-410_6_7_85,"00:06:12,090","00:06:14,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And these techniques are general and
cs-410_6_7_86,"00:06:14,610","00:06:19,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,robust and that's more widely
cs-410_6_7_87,"00:06:21,120","00:06:26,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"In fact, in virtually all the text mining"
cs-410_6_7_88,"00:06:26,565","00:06:32,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,representation and then techniques that
cs-410_6_7_89,"00:06:35,909","00:06:39,652",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,But obviously all these other
cs-410_6_7_90,"00:06:39,652","00:06:45,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,should be combined in order to support
cs-410_6_7_91,"00:06:45,440","00:06:48,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"So to summarize,"
cs-410_6_7_92,"00:06:48,790","00:06:53,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,Text representation determines what
cs-410_6_7_93,"00:06:53,615","00:06:57,908",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,And there are multiple ways to
cs-410_6_7_94,"00:06:57,908","00:07:03,099",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,"syntactic structures, entity-relation"
cs-410_6_7_95,"00:07:03,099","00:07:08,326",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,And these different
cs-410_6_7_96,"00:07:08,326","00:07:13,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,be combined in real applications
cs-410_6_7_97,"00:07:13,540","00:07:20,263",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"For example, even if we cannot"
cs-410_6_7_98,"00:07:20,263","00:07:25,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"of syntactic structures, we can state"
cs-410_6_7_99,"00:07:25,380","00:07:29,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,"And if we can recognize some entities,"
cs-410_6_7_100,"00:07:29,610","00:07:32,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,So in general we want to
cs-410_6_7_101,"00:07:34,570","00:07:37,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,And when different levels
cs-410_6_7_102,"00:07:37,210","00:07:41,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"we can enable a richer analysis,"
cs-410_6_7_103,"00:07:42,830","00:07:46,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,This course however focuses
cs-410_6_7_104,"00:07:46,610","00:07:52,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,Such techniques have also several
cs-410_6_7_105,"00:07:52,170","00:07:55,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"robust, so they are applicable"
cs-410_6_7_106,"00:07:55,460","00:07:59,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,That's a big advantage over
cs-410_6_7_107,"00:07:59,780","00:08:03,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,more fragile natural language
cs-410_6_7_108,"00:08:03,510","00:08:07,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,"Secondly, it does not require"
cs-410_6_7_109,"00:08:07,680","00:08:11,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"sometimes, it does not"
cs-410_6_7_110,"00:08:11,520","00:08:14,037",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"So that's, again, an important benefit,"
cs-410_6_7_111,"00:08:14,037","00:08:17,962",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,because that means that you can apply
cs-410_6_7_112,"00:08:20,910","00:08:25,373",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"Third, these techniques are actually"
cs-410_6_7_113,"00:08:25,373","00:08:27,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,effective form in implications.
cs-410_6_7_114,"00:08:29,210","00:08:32,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,Although not all of course
cs-410_6_7_115,"00:08:34,340","00:08:38,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,Now they are very effective
cs-410_6_7_116,"00:08:38,460","00:08:44,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,are invented by humans as basically
cs-410_6_7_117,"00:08:45,610","00:08:51,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,So they are actually quite sufficient for
cs-410_6_7_118,"00:08:53,680","00:09:00,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,So that makes this kind of word-based
cs-410_6_7_119,"00:09:00,310","00:09:05,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"And finally, such a word-based"
cs-410_6_7_120,"00:09:05,010","00:09:11,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,by such a representation can be combined
cs-410_6_7_121,"00:09:14,020","00:09:15,191",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,So they're not competing with each other.
cs-410_6_7_122,"00:09:15,191","00:09:25,191",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,[MUSIC]
cs-410_8_7_1,"00:00:00,025","00:00:07,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_8_7_2,"00:00:07,935","00:00:14,253",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about
cs-410_8_7_3,"00:00:14,253","00:00:19,131",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,In this lecture we are going to talk about
cs-410_8_7_4,"00:00:19,131","00:00:22,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,association called
cs-410_8_7_5,"00:00:25,400","00:00:30,307",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"By definition,"
cs-410_8_7_6,"00:00:30,307","00:00:34,503",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,related if they share a similar context.
cs-410_8_7_7,"00:00:34,503","00:00:39,086",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,"Namely, they occur in"
cs-410_8_7_8,"00:00:39,086","00:00:44,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,So naturally our idea of discovering such
cs-410_8_7_9,"00:00:44,280","00:00:49,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,of each word and then try to compute
cs-410_8_7_10,"00:00:50,160","00:00:54,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,So here is an example of
cs-410_8_7_11,"00:00:55,800","00:01:01,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,Here I have taken the word
cs-410_8_7_12,"00:01:01,690","00:01:08,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,you can see we are seeing some remaining
cs-410_8_7_13,"00:01:09,610","00:01:12,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,"Now, we can do the same thing for"
cs-410_8_7_14,"00:01:13,660","00:01:18,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,So in general we would like to capture
cs-410_8_7_15,"00:01:18,370","00:01:23,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,the similarity of the context of cat and
cs-410_8_7_16,"00:01:24,790","00:01:29,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So now the question is how can we
cs-410_8_7_17,"00:01:29,970","00:01:31,458",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,then define the similarity function.
cs-410_8_7_18,"00:01:33,340","00:01:38,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"So first, we note that the context"
cs-410_8_7_19,"00:01:38,560","00:01:43,637",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"So, they can be regarded as"
cs-410_8_7_20,"00:01:43,637","00:01:49,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"document, but there are also different"
cs-410_8_7_21,"00:01:49,370","00:01:57,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"For example, we can look at the word"
cs-410_8_7_22,"00:01:57,470","00:02:00,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,We can call this context Left1 context.
cs-410_8_7_23,"00:02:00,440","00:02:04,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"All right, so in this case you"
cs-410_8_7_24,"00:02:04,980","00:02:07,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"big, a, the, et cetera."
cs-410_8_7_25,"00:02:07,430","00:02:12,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,These are the words that can
cs-410_8_7_26,"00:02:12,690","00:02:19,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"So we say my cat, his cat,"
cs-410_8_7_27,"00:02:19,280","00:02:24,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"Similarly, we can also collect the words"
cs-410_8_7_28,"00:02:24,180","00:02:28,156",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"We can call this context Right1, and"
cs-410_8_7_29,"00:02:28,156","00:02:34,128",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"here we see words like eats,"
cs-410_8_7_30,"00:02:34,128","00:02:35,907",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"Or, more generally,"
cs-410_8_7_31,"00:02:35,907","00:02:41,253",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,we can look at all the words in
cs-410_8_7_32,"00:02:41,253","00:02:46,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"Here, let's say we can take a window"
cs-410_8_7_33,"00:02:46,960","00:02:48,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,We call this context Window8.
cs-410_8_7_34,"00:02:49,850","00:02:54,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"Now, of course, you can see all"
cs-410_8_7_35,"00:02:54,680","00:02:58,829",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,so we'll have a bag of words in
cs-410_8_7_36,"00:03:01,270","00:03:06,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,"Now, such a word based representation"
cs-410_8_7_37,"00:03:06,410","00:03:12,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,an interesting way to define the
cs-410_8_7_38,"00:03:12,230","00:03:15,911",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,Because if you look at just
cs-410_8_7_39,"00:03:15,911","00:03:21,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,then we'll see words that share
cs-410_8_7_40,"00:03:21,750","00:03:27,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,and we kind of ignored the other words
cs-410_8_7_41,"00:03:27,650","00:03:32,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,So that gives us one perspective to
cs-410_8_7_42,"00:03:32,380","00:03:34,244",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,"if we only use the Right1 context,"
cs-410_8_7_43,"00:03:34,244","00:03:38,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,we will capture this narrative
cs-410_8_7_44,"00:03:38,420","00:03:43,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,Using both the Left1 and
cs-410_8_7_45,"00:03:43,040","00:03:47,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,the similarity with even
cs-410_8_7_46,"00:03:49,910","00:03:54,744",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"So in general, context may contain"
cs-410_8_7_47,"00:03:54,744","00:03:59,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"my, that you see here, or"
cs-410_8_7_48,"00:03:59,575","00:04:02,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Tuesday, or"
cs-410_8_7_49,"00:04:05,461","00:04:10,174",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,And this flexibility also allows us
cs-410_8_7_50,"00:04:10,174","00:04:11,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,different ways.
cs-410_8_7_51,"00:04:11,660","00:04:13,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"Sometimes this is useful,"
cs-410_8_7_52,"00:04:13,500","00:04:19,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,as we might want to capture
cs-410_8_7_53,"00:04:19,130","00:04:25,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,That would give us loosely
cs-410_8_7_54,"00:04:25,270","00:04:29,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,Whereas if you use only the words
cs-410_8_7_55,"00:04:29,340","00:04:35,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"to the right of the word, then you"
cs-410_8_7_56,"00:04:35,520","00:04:39,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,much related by their syntactical
cs-410_8_7_57,"00:04:41,170","00:04:46,304",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So the general idea of discovering
cs-410_8_7_58,"00:04:46,304","00:04:50,754",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,is to compute the similarity
cs-410_8_7_59,"00:04:50,754","00:04:55,264",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"So here, for example,"
cs-410_8_7_60,"00:04:55,264","00:04:59,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,dog based on the similarity
cs-410_8_7_61,"00:04:59,110","00:05:02,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"In general, we can combine all"
cs-410_8_7_62,"00:05:02,890","00:05:06,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"And so the similarity function is,"
cs-410_8_7_63,"00:05:06,395","00:05:10,336",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,a combination of similarities
cs-410_8_7_64,"00:05:10,336","00:05:14,849",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"And of course, we can also assign"
cs-410_8_7_65,"00:05:14,849","00:05:20,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,similarities to allow us to focus
cs-410_8_7_66,"00:05:20,170","00:05:24,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,And this would be naturally
cs-410_8_7_67,"00:05:24,395","00:05:28,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,here the main idea for discovering
cs-410_8_7_68,"00:05:28,935","00:05:32,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,to computer the similarity
cs-410_8_7_69,"00:05:32,470","00:05:37,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So next let's see how we exactly
cs-410_8_7_70,"00:05:37,670","00:05:42,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Now to answer this question,"
cs-410_8_7_71,"00:05:42,235","00:05:46,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,representation as vectors
cs-410_8_7_72,"00:05:48,340","00:05:53,016",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,Now those of you who have been
cs-410_8_7_73,"00:05:53,016","00:05:57,936",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,textual retrieval techniques would
cs-410_8_7_74,"00:05:57,936","00:06:02,711",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,been used frequently for
cs-410_8_7_75,"00:06:02,711","00:06:08,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,But here we also find it convenient
cs-410_8_7_76,"00:06:08,115","00:06:11,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,paradigmatic relation discovery.
cs-410_8_7_77,"00:06:11,130","00:06:15,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,So the idea of this
cs-410_8_7_78,"00:06:15,440","00:06:20,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,word in our vocabulary as defining one
cs-410_8_7_79,"00:06:20,140","00:06:23,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,So we have N words in
cs-410_8_7_80,"00:06:23,615","00:06:27,462",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,"then we have N dimensions,"
cs-410_8_7_81,"00:06:27,462","00:06:34,311",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"And on the bottom, you can see a frequency"
cs-410_8_7_82,"00:06:34,311","00:06:39,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,and here we see where eats
cs-410_8_7_83,"00:06:39,855","00:06:43,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"ate occurred 3 times, et cetera."
cs-410_8_7_84,"00:06:43,140","00:06:48,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,So this vector can then be placed
cs-410_8_7_85,"00:06:48,003","00:06:53,347",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"So in general,"
cs-410_8_7_86,"00:06:53,347","00:06:58,933",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,"context of cat as one vector,"
cs-410_8_7_87,"00:06:58,933","00:07:04,045",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"dog, might give us a different context,"
cs-410_8_7_88,"00:07:04,045","00:07:07,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,And then we can measure
cs-410_8_7_89,"00:07:07,880","00:07:10,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,So by viewing context in
cs-410_8_7_90,"00:07:10,980","00:07:15,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,we convert the problem of
cs-410_8_7_91,"00:07:15,100","00:07:18,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,into the problem of computing
cs-410_8_7_92,"00:07:20,300","00:07:24,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,So the two questions that we
cs-410_8_7_93,"00:07:24,170","00:07:28,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"how to compute each vector, and"
cs-410_8_7_94,"00:07:31,050","00:07:33,579",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,And the other question is how
cs-410_8_7_95,"00:07:35,580","00:07:40,515",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"Now in general, there are many approaches"
cs-410_8_7_96,"00:07:40,515","00:07:43,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,most of them are developed for
cs-410_8_7_97,"00:07:43,795","00:07:47,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,And they have been shown to work well for
cs-410_8_7_98,"00:07:47,821","00:07:52,712",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,matching a query vector and
cs-410_8_7_99,"00:07:52,712","00:07:57,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,But we can adapt many of
cs-410_8_7_100,"00:07:57,555","00:08:01,378",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,of context documents for our purpose here.
cs-410_8_7_101,"00:08:01,378","00:08:05,829",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So let's first look at
cs-410_8_7_102,"00:08:05,829","00:08:10,481",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,where we try to match
cs-410_8_7_103,"00:08:10,481","00:08:15,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"the expected overlap of words,"
cs-410_8_7_104,"00:08:17,020","00:08:22,495",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,So the idea here is to represent
cs-410_8_7_105,"00:08:22,495","00:08:28,438",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,where each word has a weight
cs-410_8_7_106,"00:08:28,438","00:08:35,336",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,that a randomly picked word from
cs-410_8_7_107,"00:08:35,336","00:08:39,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,"So in other words,"
cs-410_8_7_108,"00:08:39,956","00:08:43,476",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"account of word wi in the context, and"
cs-410_8_7_109,"00:08:43,476","00:08:48,756",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,this can be interpreted as
cs-410_8_7_110,"00:08:48,756","00:08:54,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,actually pick this word from d1
cs-410_8_7_111,"00:08:56,760","00:09:01,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"Now, of course these xi's would sum to one"
cs-410_8_7_112,"00:09:02,930","00:09:05,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,and this means the vector is
cs-410_8_7_113,"00:09:05,750","00:09:08,193",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,actually probability of
cs-410_8_7_114,"00:09:10,500","00:09:15,883",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"So, the vector d2 can be also"
cs-410_8_7_115,"00:09:15,883","00:09:23,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,this would give us then two probability
cs-410_8_7_116,"00:09:24,840","00:09:28,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"So, that addresses the problem"
cs-410_8_7_117,"00:09:28,220","00:09:31,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,next let's see how we can define
cs-410_8_7_118,"00:09:31,760","00:09:35,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"Well, here, we simply define"
cs-410_8_7_119,"00:09:35,668","00:09:39,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"vectors, and"
cs-410_8_7_120,"00:09:41,410","00:09:43,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,of the corresponding
cs-410_8_7_121,"00:09:46,630","00:09:51,847",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"Now, it's interesting to see"
cs-410_8_7_122,"00:09:51,847","00:09:57,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"actually has a nice interpretation,"
cs-410_8_7_123,"00:09:57,360","00:10:02,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"Dot product, in fact that gives"
cs-410_8_7_124,"00:10:02,548","00:10:08,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,randomly picked words from
cs-410_8_7_125,"00:10:08,570","00:10:12,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,That means if we try to pick a word
cs-410_8_7_126,"00:10:12,630","00:10:17,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"word from another context, we can then"
cs-410_8_7_127,"00:10:17,860","00:10:22,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,"If the two contexts are very similar,"
cs-410_8_7_128,"00:10:22,650","00:10:27,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,see the two words picked from
cs-410_8_7_129,"00:10:27,390","00:10:30,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,"If they are very different,"
cs-410_8_7_130,"00:10:30,900","00:10:34,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,identical words being picked from
cs-410_8_7_131,"00:10:34,890","00:10:39,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"So this intuitively makes sense, right,"
cs-410_8_7_132,"00:10:41,490","00:10:46,819",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,Now you might want to also take
cs-410_8_7_133,"00:10:46,819","00:10:51,627",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,see why this can be interpreted
cs-410_8_7_134,"00:10:51,627","00:10:55,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,two randomly picked words are identical.
cs-410_8_7_135,"00:10:57,440","00:11:04,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,So if you just stare at the formula
cs-410_8_7_136,"00:11:04,550","00:11:12,034",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,then you will see basically in each
cs-410_8_7_137,"00:11:12,034","00:11:17,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,we will see an overlap on
cs-410_8_7_138,"00:11:17,170","00:11:23,661",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,And where xi gives us a probability that
cs-410_8_7_139,"00:11:23,661","00:11:28,503",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,and yi gives us the probability
cs-410_8_7_140,"00:11:28,503","00:11:32,024",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,And when we pick the same
cs-410_8_7_141,"00:11:32,024","00:11:34,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,"then we have an identical pick, right so."
cs-410_8_7_142,"00:11:34,920","00:11:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=694,"That's one possible approach, EOWC,"
cs-410_8_7_143,"00:11:42,380","00:11:49,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"Now as always, we would like to assess"
cs-410_8_7_144,"00:11:49,440","00:11:52,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,"Now of course, ultimately we have to"
cs-410_8_7_145,"00:11:52,880","00:11:56,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,see if it gives us really
cs-410_8_7_146,"00:11:57,730","00:12:01,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,"Really give us paradigmatical relations,"
cs-410_8_7_147,"00:12:01,010","00:12:05,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,analytically we can also analyze
cs-410_8_7_148,"00:12:05,380","00:12:11,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,"So first, as I said,"
cs-410_8_7_149,"00:12:11,020","00:12:15,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,formula will give a higher score if there
cs-410_8_7_150,"00:12:15,802","00:12:17,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,So that's exactly what we want.
cs-410_8_7_151,"00:12:17,988","00:12:21,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,But if you analyze
cs-410_8_7_152,"00:12:21,170","00:12:24,286",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,then you also see there might
cs-410_8_7_153,"00:12:24,286","00:12:27,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,and specifically there
cs-410_8_7_154,"00:12:27,735","00:12:33,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,"First, it might favor matching"
cs-410_8_7_155,"00:12:33,935","00:12:35,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,over matching more distinct terms.
cs-410_8_7_156,"00:12:36,825","00:12:44,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,"And that is because in the dot product,"
cs-410_8_7_157,"00:12:44,300","00:12:50,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,element is shared by both contexts and
cs-410_8_7_158,"00:12:51,250","00:12:55,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,it might indeed make the score
cs-410_8_7_159,"00:12:55,710","00:13:01,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,where the two vectors actually have
cs-410_8_7_160,"00:13:01,150","00:13:06,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,But each term has a relatively low
cs-410_8_7_161,"00:13:06,878","00:13:09,586",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"Of course, this might be"
cs-410_8_7_162,"00:13:09,586","00:13:14,527",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"But in our case, we should intuitively"
cs-410_8_7_163,"00:13:14,527","00:13:19,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"more different terms in the context,"
cs-410_8_7_164,"00:13:19,645","00:13:24,253",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,in saying that the two words
cs-410_8_7_165,"00:13:24,253","00:13:27,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,If you only rely on one term and
cs-410_8_7_166,"00:13:27,020","00:13:32,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"that's a little bit questionable,"
cs-410_8_7_167,"00:13:34,675","00:13:38,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=814,Now the second problem is that it
cs-410_8_7_168,"00:13:38,795","00:13:42,131",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,So if you match a word like the and
cs-410_8_7_169,"00:13:42,131","00:13:47,443",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,it will be the same as
cs-410_8_7_170,"00:13:47,443","00:13:52,388",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=827,intuitively we know
cs-410_8_7_171,"00:13:52,388","00:13:57,816",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,surprising because the occurs everywhere.
cs-410_8_7_172,"00:13:57,816","00:14:02,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,So matching the is not as such
cs-410_8_7_173,"00:14:02,787","00:14:07,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,"a word like eats,"
cs-410_8_7_174,"00:14:07,956","00:14:11,216",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,So this is another
cs-410_8_7_175,"00:14:13,426","00:14:19,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,In the next chapter we are going to talk
cs-410_8_7_176,"00:14:19,003","00:14:29,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,[MUSIC]
cs-410_5_7_1,"00:00:06,440","00:00:11,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about the
cs-410_5_7_2,"00:00:12,020","00:00:14,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we are going"
cs-410_5_7_3,"00:00:14,100","00:00:16,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,to discuss textual
cs-410_5_7_4,"00:00:16,680","00:00:20,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,and discuss how natural
cs-410_5_7_5,"00:00:20,475","00:00:24,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,allow us to represent text
cs-410_5_7_6,"00:00:24,450","00:00:28,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,Let's take a look at this
cs-410_5_7_7,"00:00:28,590","00:00:33,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,We can represent this sentence
cs-410_5_7_8,"00:00:33,900","00:00:37,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"First, we can always"
cs-410_5_7_9,"00:00:37,680","00:00:42,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,represent such a sentence
cs-410_5_7_10,"00:00:42,090","00:00:45,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,This is true for
cs-410_5_7_11,"00:00:45,135","00:00:49,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,when we store them
cs-410_5_7_12,"00:00:49,310","00:00:53,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,When we store a natural
cs-410_5_7_13,"00:00:53,480","00:00:55,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"as a string of characters,"
cs-410_5_7_14,"00:00:55,805","00:01:00,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,we have perhaps the most general
cs-410_5_7_15,"00:01:00,350","00:01:02,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,since we always use
cs-410_5_7_16,"00:01:02,270","00:01:05,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,this approach to
cs-410_5_7_17,"00:01:05,360","00:01:10,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"But unfortunately, using"
cs-410_5_7_18,"00:01:10,070","00:01:12,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"help us to do semantic analysis,"
cs-410_5_7_19,"00:01:12,950","00:01:14,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,which is often needed
cs-410_5_7_20,"00:01:14,135","00:01:17,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,for many applications
cs-410_5_7_21,"00:01:17,600","00:01:21,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,The reason is because we're
cs-410_5_7_22,"00:01:21,650","00:01:22,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"So as a string,"
cs-410_5_7_23,"00:01:22,880","00:01:25,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,we're going to keep
cs-410_5_7_24,"00:01:25,100","00:01:29,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,and these ASCII symbols.
cs-410_5_7_25,"00:01:29,005","00:01:32,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,We can perhaps count what's
cs-410_5_7_26,"00:01:32,090","00:01:35,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,the most frequent character
cs-410_5_7_27,"00:01:35,225","00:01:38,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,or the correlation
cs-410_5_7_28,"00:01:38,960","00:01:43,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,but we can't really
cs-410_5_7_29,"00:01:43,075","00:01:47,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"Yet, this is the most"
cs-410_5_7_30,"00:01:47,300","00:01:49,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,text because we can use
cs-410_5_7_31,"00:01:49,250","00:01:53,045",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,this to represent any
cs-410_5_7_32,"00:01:53,045","00:01:55,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,If we try to do
cs-410_5_7_33,"00:01:55,220","00:01:57,635",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,a little bit more natural
cs-410_5_7_34,"00:01:57,635","00:02:00,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,"by doing word segmentation,"
cs-410_5_7_35,"00:02:00,540","00:02:05,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,then we can obtain a
cs-410_5_7_36,"00:02:05,210","00:02:08,315",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,but in the form of a
cs-410_5_7_37,"00:02:08,315","00:02:11,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,So here we see that
cs-410_5_7_38,"00:02:11,750","00:02:17,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,words like a dog is chasing etc.
cs-410_5_7_39,"00:02:17,230","00:02:20,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,Now with this level
cs-410_5_7_40,"00:02:20,600","00:02:23,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,we certainly can do
cs-410_5_7_41,"00:02:23,965","00:02:27,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,and this is mainly because
cs-410_5_7_42,"00:02:27,065","00:02:30,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,of human communication
cs-410_5_7_43,"00:02:30,275","00:02:33,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,so they are very powerful.
cs-410_5_7_44,"00:02:33,035","00:02:36,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,"By identifying words, we can for"
cs-410_5_7_45,"00:02:36,080","00:02:38,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,example easily count what are
cs-410_5_7_46,"00:02:38,780","00:02:40,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,the most frequent words in
cs-410_5_7_47,"00:02:40,820","00:02:45,185",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,this document or in
cs-410_5_7_48,"00:02:45,185","00:02:48,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,These words can be used to form
cs-410_5_7_49,"00:02:48,200","00:02:51,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,topics when we combine
cs-410_5_7_50,"00:02:51,940","00:02:54,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"and some words are positive,"
cs-410_5_7_51,"00:02:54,060","00:02:55,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"some words negative, so we can"
cs-410_5_7_52,"00:02:55,700","00:02:58,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,also do sentiment analysis.
cs-410_5_7_53,"00:02:58,660","00:03:02,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,So representing text data
cs-410_5_7_54,"00:03:02,690","00:03:06,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,opens up a lot of interesting
cs-410_5_7_55,"00:03:06,775","00:03:09,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"However, this level of"
cs-410_5_7_56,"00:03:09,060","00:03:12,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,representation is slightly
cs-410_5_7_57,"00:03:12,080","00:03:17,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,of characters because in
cs-410_5_7_58,"00:03:17,300","00:03:21,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,it's actually not
cs-410_5_7_59,"00:03:21,350","00:03:25,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,all the word boundaries
cs-410_5_7_60,"00:03:25,010","00:03:27,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,you see text as a sequence of
cs-410_5_7_61,"00:03:27,685","00:03:31,345",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,characters with
cs-410_5_7_62,"00:03:31,345","00:03:33,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,So you'll have to rely on
cs-410_5_7_63,"00:03:33,160","00:03:36,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,some special techniques
cs-410_5_7_64,"00:03:36,925","00:03:39,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"In such a language,"
cs-410_5_7_65,"00:03:39,940","00:03:43,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,we might make mistakes
cs-410_5_7_66,"00:03:43,600","00:03:46,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,So the sequence of
cs-410_5_7_67,"00:03:46,480","00:03:50,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,not as robust as
cs-410_5_7_68,"00:03:50,230","00:03:53,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"But in English, it's very"
cs-410_5_7_69,"00:03:53,230","00:03:56,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,easy to obtain this level
cs-410_5_7_70,"00:03:56,005","00:03:59,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,so we can do that all the time.
cs-410_5_7_71,"00:04:00,860","00:04:03,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"Now, if we go further"
cs-410_5_7_72,"00:04:03,295","00:04:04,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,to do naturally
cs-410_5_7_73,"00:04:04,645","00:04:08,125",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,we can add a part of speech tags.
cs-410_5_7_74,"00:04:08,125","00:04:09,955",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"Now once we do that,"
cs-410_5_7_75,"00:04:09,955","00:04:11,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"we can count, for example,"
cs-410_5_7_76,"00:04:11,850","00:04:14,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,the most frequent
cs-410_5_7_77,"00:04:14,680","00:04:18,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,nouns are associated with
cs-410_5_7_78,"00:04:18,220","00:04:19,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,So this opens up
cs-410_5_7_79,"00:04:19,480","00:04:23,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,a little bit more
cs-410_5_7_80,"00:04:23,020","00:04:24,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,for further analysis.
cs-410_5_7_81,"00:04:24,625","00:04:28,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,Note that I use a plus sign
cs-410_5_7_82,"00:04:28,270","00:04:32,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,representing text as a sequence
cs-410_5_7_83,"00:04:32,305","00:04:35,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,we don't necessarily replace
cs-410_5_7_84,"00:04:35,395","00:04:37,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,the original word
cs-410_5_7_85,"00:04:37,840","00:04:40,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"Instead, we add this as"
cs-410_5_7_86,"00:04:40,975","00:04:44,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,an additional way of
cs-410_5_7_87,"00:04:44,050","00:04:47,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,so that now the data is
cs-410_5_7_88,"00:04:47,230","00:04:50,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,of words and a sequence
cs-410_5_7_89,"00:04:50,965","00:04:54,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,This enriches the
cs-410_5_7_90,"00:04:54,055","00:04:59,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,and thus also enables
cs-410_5_7_91,"00:05:00,340","00:05:04,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"If we go further, then we'll"
cs-410_5_7_92,"00:05:04,040","00:05:08,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,often to obtain
cs-410_5_7_93,"00:05:08,020","00:05:09,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"Now this of course,"
cs-410_5_7_94,"00:05:09,700","00:05:12,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,further open up
cs-410_5_7_95,"00:05:12,230","00:05:14,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"of, for example,"
cs-410_5_7_96,"00:05:14,840","00:05:22,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,the writing styles or
cs-410_5_7_97,"00:05:22,520","00:05:26,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,If we go further for
cs-410_5_7_98,"00:05:26,440","00:05:31,684",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,then we might be able to
cs-410_5_7_99,"00:05:31,684","00:05:35,515",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,and we also can recognize
cs-410_5_7_100,"00:05:35,515","00:05:38,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,and playground as a location.
cs-410_5_7_101,"00:05:38,055","00:05:41,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,We can further analyze
cs-410_5_7_102,"00:05:41,335","00:05:45,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,dog is chasing the boy and
cs-410_5_7_103,"00:05:45,830","00:05:48,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,Now this will add
cs-410_5_7_104,"00:05:48,875","00:05:52,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,relations through
cs-410_5_7_105,"00:05:52,945","00:05:54,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"At this level,"
cs-410_5_7_106,"00:05:54,790","00:05:57,605",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,then we can do even more
cs-410_5_7_107,"00:05:57,605","00:05:59,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"For example, now we"
cs-410_5_7_108,"00:05:59,795","00:06:02,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,the most frequent person that's
cs-410_5_7_109,"00:06:02,360","00:06:06,284",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,mentioning this whole collection
cs-410_5_7_110,"00:06:06,284","00:06:09,205",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,or whenever you
cs-410_5_7_111,"00:06:09,205","00:06:13,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,you also tend to see mentioning
cs-410_5_7_112,"00:06:13,655","00:06:19,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,So this is a very
cs-410_5_7_113,"00:06:19,480","00:06:21,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,and it's also related to
cs-410_5_7_114,"00:06:21,830","00:06:24,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,the knowledge graph that
cs-410_5_7_115,"00:06:24,500","00:06:27,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,of that Google is doing as
cs-410_5_7_116,"00:06:27,620","00:06:31,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,a more semantic way of
cs-410_5_7_117,"00:06:31,690","00:06:39,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"However, it's also less robust"
cs-410_5_7_118,"00:06:39,080","00:06:42,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,even syntactical analysis
cs-410_5_7_119,"00:06:42,410","00:06:43,985",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,always easy to identify
cs-410_5_7_120,"00:06:43,985","00:06:46,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,all the entities with
cs-410_5_7_121,"00:06:46,160","00:06:47,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,"and we might make mistakes,"
cs-410_5_7_122,"00:06:47,735","00:06:50,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,and relations are
cs-410_5_7_123,"00:06:50,270","00:06:52,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,and we might make mistakes.
cs-410_5_7_124,"00:06:52,685","00:06:56,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,So this makes this level of
cs-410_5_7_125,"00:06:56,120","00:06:58,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,yet it's very useful.
cs-410_5_7_126,"00:06:58,190","00:07:01,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,Now if we move further
cs-410_5_7_127,"00:07:01,700","00:07:05,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,then we can have predicates
cs-410_5_7_128,"00:07:05,465","00:07:08,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"With inference rules, we can"
cs-410_5_7_129,"00:07:08,630","00:07:13,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,infer interesting derived
cs-410_5_7_130,"00:07:13,700","00:07:15,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,so that's very useful.
cs-410_5_7_131,"00:07:15,020","00:07:17,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"But unfortunately,"
cs-410_5_7_132,"00:07:17,420","00:07:19,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,representation is even less
cs-410_5_7_133,"00:07:19,940","00:07:22,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,robust and we can make
cs-410_5_7_134,"00:07:22,010","00:07:25,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,mistakes and we can't do
cs-410_5_7_135,"00:07:25,120","00:07:28,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,that all the time for
cs-410_5_7_136,"00:07:28,885","00:07:33,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,"Finally, speech acts would"
cs-410_5_7_137,"00:07:33,820","00:07:38,605",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,of repetition of the intent
cs-410_5_7_138,"00:07:38,605","00:07:40,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"So in this case,"
cs-410_5_7_139,"00:07:40,000","00:07:41,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,it might be a request.
cs-410_5_7_140,"00:07:41,485","00:07:44,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,So knowing that would
cs-410_5_7_141,"00:07:44,650","00:07:47,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,even more interesting
cs-410_5_7_142,"00:07:47,140","00:07:51,765",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,this observer or the author
cs-410_5_7_143,"00:07:51,765","00:07:53,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,What's the intention
cs-410_5_7_144,"00:07:53,895","00:07:57,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,What's scenarios? What kind
cs-410_5_7_145,"00:07:57,210","00:08:02,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So this is another level
cs-410_5_7_146,"00:08:02,740","00:08:05,755",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,of analysis that would
cs-410_5_7_147,"00:08:05,755","00:08:10,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,So this picture shows
cs-410_5_7_148,"00:08:10,250","00:08:12,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,we generally see
cs-410_5_7_149,"00:08:12,530","00:08:15,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,natural language processing
cs-410_5_7_150,"00:08:15,535","00:08:18,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"Unfortunately,"
cs-410_5_7_151,"00:08:18,080","00:08:20,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"require more human effort,"
cs-410_5_7_152,"00:08:20,330","00:08:23,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,and they are less accurate.
cs-410_5_7_153,"00:08:23,060","00:08:26,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,That means there are mistakes.
cs-410_5_7_154,"00:08:26,570","00:08:29,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,So if we add an texts that are at
cs-410_5_7_155,"00:08:29,945","00:08:32,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,the levels that are
cs-410_5_7_156,"00:08:32,240","00:08:34,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,representing deeper
cs-410_5_7_157,"00:08:34,970","00:08:37,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,then we have to
cs-410_5_7_158,"00:08:37,790","00:08:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,So that also means it's
cs-410_5_7_159,"00:08:42,380","00:08:46,835",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,such deep analysis with
cs-410_5_7_160,"00:08:46,835","00:08:48,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"for example, sequence of words."
cs-410_5_7_161,"00:08:48,695","00:08:50,675",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"On the right side,"
cs-410_5_7_162,"00:08:50,675","00:08:55,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,you'll see the arrow points
cs-410_5_7_163,"00:08:55,240","00:08:56,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"As we go down,"
cs-410_5_7_164,"00:08:56,960","00:08:59,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,we are representation
cs-410_5_7_165,"00:08:59,780","00:09:02,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,to knowledge representation
cs-410_5_7_166,"00:09:02,600","00:09:08,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,and need for solving
cs-410_5_7_167,"00:09:08,210","00:09:11,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,Now this is desirable because as
cs-410_5_7_168,"00:09:11,750","00:09:15,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,we can represent text at
cs-410_5_7_169,"00:09:15,110","00:09:17,315",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,we can easily extract
cs-410_5_7_170,"00:09:17,315","00:09:19,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,That's the purpose
cs-410_5_7_171,"00:09:19,280","00:09:21,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,So there is a trade-off
cs-410_5_7_172,"00:09:21,965","00:09:24,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,here between doing
cs-410_5_7_173,"00:09:24,920","00:09:27,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,might have errors but would give
cs-410_5_7_174,"00:09:27,260","00:09:31,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,us direct knowledge that
cs-410_5_7_175,"00:09:31,225","00:09:33,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"Doing shallow analysis, which"
cs-410_5_7_176,"00:09:33,910","00:09:37,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,is more robust but
cs-410_5_7_177,"00:09:37,010","00:09:42,665",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,give us the necessary deeper
cs-410_5_7_178,"00:09:42,665","00:09:45,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,I should also say that
cs-410_5_7_179,"00:09:45,740","00:09:49,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,humans and are meant to
cs-410_5_7_180,"00:09:49,085","00:09:52,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"So as a result, in"
cs-410_5_7_181,"00:09:52,340","00:09:56,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,text-mining humans play
cs-410_5_7_182,"00:09:56,090","00:09:58,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,they are always in the loop.
cs-410_5_7_183,"00:09:58,010","00:10:00,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,Meaning that we should optimize
cs-410_5_7_184,"00:10:00,650","00:10:03,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,the collaboration of
cs-410_5_7_185,"00:10:03,695","00:10:05,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,"So in that sense,"
cs-410_5_7_186,"00:10:05,540","00:10:08,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,it's okay that computers
cs-410_5_7_187,"00:10:08,480","00:10:12,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,to have compute accurately
cs-410_5_7_188,"00:10:12,920","00:10:15,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,and the patterns
cs-410_5_7_189,"00:10:15,290","00:10:18,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,from text data can be
cs-410_5_7_190,"00:10:18,035","00:10:20,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,and humans can
cs-410_5_7_191,"00:10:20,840","00:10:24,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,to do more accurate analysis
cs-410_5_7_192,"00:10:24,650","00:10:28,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,by providing features
cs-410_5_7_193,"00:10:28,640","00:10:33,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,learning programs to make
cs-410_4_7_1,"00:00:00,266","00:00:05,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_7_2,"00:00:10,218","00:00:13,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,So here are some specific examples of what
cs-410_4_7_3,"00:00:13,988","00:00:15,926",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,we can't do today and
cs-410_4_7_4,"00:00:15,926","00:00:21,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,part of speech tagging is still
cs-410_4_7_5,"00:00:21,970","00:00:27,938",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,"So in the example, he turned off the"
cs-410_4_7_6,"00:00:27,938","00:00:33,342",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,the two offs actually have somewhat
cs-410_4_7_7,"00:00:33,342","00:00:39,633",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,categories and also its very difficult
cs-410_4_7_8,"00:00:39,633","00:00:44,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Again, the example, a man saw a boy"
cs-410_4_7_9,"00:00:44,450","00:00:48,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,be very difficult to parse
cs-410_4_7_10,"00:00:48,400","00:00:52,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,Precise deep semantic
cs-410_4_7_11,"00:00:52,278","00:00:55,648",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"For example, to define the meaning of own,"
cs-410_4_7_12,"00:00:55,648","00:01:01,493",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,precisely is very difficult in
cs-410_4_7_13,"00:01:01,493","00:01:04,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,So the state of the off can
cs-410_4_7_14,"00:01:04,737","00:01:05,506",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,Robust and
cs-410_4_7_15,"00:01:05,506","00:01:11,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,general NLP tends to be shallow while
cs-410_4_7_16,"00:01:12,430","00:01:18,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"For this reason in this course,"
cs-410_4_7_17,"00:01:18,565","00:01:23,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"general, shallow techniques for"
cs-410_4_7_18,"00:01:23,610","00:01:29,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,mining text data and they are generally
cs-410_4_7_19,"00:01:29,630","00:01:34,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,So there are robust and
cs-410_4_7_20,"00:01:36,540","00:01:39,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,the in category of shallow analysis.
cs-410_4_7_21,"00:01:39,550","00:01:44,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,So such techniques have
cs-410_4_7_22,"00:01:44,320","00:01:49,099",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,applied to any text data in
cs-410_4_7_23,"00:01:49,099","00:01:55,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"But the downside is that, they don't"
cs-410_4_7_24,"00:01:55,425","00:01:59,159",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"For that, we have to rely on"
cs-410_4_7_25,"00:02:00,960","00:02:05,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,That typically would require
cs-410_4_7_26,"00:02:05,930","00:02:10,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,a lot of examples of analysis that would
cs-410_4_7_27,"00:02:10,940","00:02:16,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,machine learning techniques and learn from
cs-410_4_7_28,"00:02:16,120","00:02:21,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"So in practical applications, we generally"
cs-410_4_7_29,"00:02:21,880","00:02:29,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,with the general statistical and
cs-410_4_7_30,"00:02:29,150","00:02:32,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,These can be applied to any text data.
cs-410_4_7_31,"00:02:32,010","00:02:37,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"And on top of that, we're going to use"
cs-410_4_7_32,"00:02:37,060","00:02:42,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,to use supervised machine learning
cs-410_4_7_33,"00:02:42,770","00:02:48,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,especially for those important
cs-410_4_7_34,"00:02:48,640","00:02:55,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,to analyze text data more precisely.
cs-410_4_7_35,"00:02:55,170","00:03:00,036",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,But this course will cover
cs-410_4_7_36,"00:03:00,036","00:03:04,177",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"that generally,"
cs-410_4_7_37,"00:03:04,177","00:03:09,386",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"So they're practically,"
cs-410_4_7_38,"00:03:09,386","00:03:16,409",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,analysis techniques that require a lot of
cs-410_4_7_39,"00:03:16,409","00:03:21,302",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"So to summarize,"
cs-410_4_7_40,"00:03:21,302","00:03:24,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,is the foundation for text mining.
cs-410_4_7_41,"00:03:24,580","00:03:27,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"So obviously, the better we"
cs-410_4_7_42,"00:03:27,465","00:03:29,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,the better we can do text mining.
cs-410_4_7_43,"00:03:30,420","00:03:34,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,Computers today are far from being able
cs-410_4_7_44,"00:03:34,930","00:03:38,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,Deep NLP requires common sense
cs-410_4_7_45,"00:03:38,030","00:03:42,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Thus, only working for"
cs-410_4_7_46,"00:03:42,803","00:03:44,833",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,large scale text mining.
cs-410_4_7_47,"00:03:44,833","00:03:50,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,Shallow NLP based on statistical
cs-410_4_7_48,"00:03:50,003","00:03:52,543",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,is the main topic of this course and
cs-410_4_7_49,"00:03:52,543","00:03:56,763",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,they are generally applicable
cs-410_4_7_50,"00:03:56,763","00:04:02,081",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"They are in some sense also,"
cs-410_4_7_51,"00:04:02,081","00:04:06,834",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"In practice,"
cs-410_4_7_52,"00:04:06,834","00:04:11,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,we'll have humans for
cs-410_4_7_53,"00:04:11,810","00:04:21,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,[MUSIC]
cs-410_1_7_1,"00:00:00,012","00:00:06,665",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_7_2,"00:00:06,665","00:00:11,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,this lecture we give an overview
cs-410_1_7_3,"00:00:13,743","00:00:19,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"First, let's define the term text mining,"
cs-410_1_7_4,"00:00:19,830","00:00:24,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,The title of this course is
cs-410_1_7_5,"00:00:25,590","00:00:31,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"But the two terms text mining, and text"
cs-410_1_7_6,"00:00:32,670","00:00:36,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,So we are not really going to
cs-410_1_7_7,"00:00:36,370","00:00:38,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,we're going to use them interchangeably.
cs-410_1_7_8,"00:00:38,230","00:00:42,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,But the reason that we have chosen to use
cs-410_1_7_9,"00:00:42,880","00:00:47,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,both terms in the title is because
cs-410_1_7_10,"00:00:47,720","00:00:51,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,if you look at the two phrases literally.
cs-410_1_7_11,"00:00:52,110","00:00:55,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,Mining emphasizes more on the process.
cs-410_1_7_12,"00:00:55,640","00:01:01,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,So it gives us a error rate
cs-410_1_7_13,"00:01:01,683","00:01:06,359",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Analytics, on the other hand"
cs-410_1_7_14,"00:01:07,600","00:01:09,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,or having a problem in mind.
cs-410_1_7_15,"00:01:09,900","00:01:14,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,We are going to look at text
cs-410_1_7_16,"00:01:16,010","00:01:19,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"But again as I said, we can treat"
cs-410_1_7_17,"00:01:21,150","00:01:24,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,And I think in the literature
cs-410_1_7_18,"00:01:24,820","00:01:27,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So we're not going to really
cs-410_1_7_19,"00:01:29,850","00:01:35,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,Both text mining and
cs-410_1_7_20,"00:01:35,450","00:01:40,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,want to turn text data into high quality
cs-410_1_7_21,"00:01:42,570","00:01:44,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"So in both cases, we"
cs-410_1_7_22,"00:01:45,830","00:01:50,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,have the problem of dealing with
cs-410_1_7_23,"00:01:50,670","00:01:56,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,Turn these text data into something more
cs-410_1_7_24,"00:01:57,730","00:02:00,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,And here we distinguish
cs-410_1_7_25,"00:02:00,380","00:02:04,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"One is high-quality information,"
cs-410_1_7_26,"00:02:05,740","00:02:08,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,Sometimes the boundary between
cs-410_1_7_27,"00:02:09,850","00:02:11,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,But I also want to say a little bit about
cs-410_1_7_28,"00:02:12,780","00:02:17,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,these two different angles of
cs-410_1_7_29,"00:02:19,250","00:02:22,982",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"In the case of high quality information,"
cs-410_1_7_30,"00:02:22,982","00:02:27,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,concise information about the topic.
cs-410_1_7_31,"00:02:28,895","00:02:34,205",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,Which might be much easier for
cs-410_1_7_32,"00:02:34,205","00:02:37,045",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"For example, you might face"
cs-410_1_7_33,"00:02:38,260","00:02:42,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,A more concise form of information
cs-410_1_7_34,"00:02:42,700","00:02:46,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,of the major opinions about
cs-410_1_7_35,"00:02:46,570","00:02:50,874",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"Positive about,"
cs-410_1_7_36,"00:02:53,436","00:02:58,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,Now this kind of results are very useful
cs-410_1_7_37,"00:02:59,930","00:03:05,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,And so this is to minimize a human effort
cs-410_1_7_38,"00:03:06,250","00:03:09,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,The other kind of output
cs-410_1_7_39,"00:03:09,880","00:03:15,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,Here we emphasize the utility
cs-410_1_7_40,"00:03:15,300","00:03:17,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,knowledge we discover from text data.
cs-410_1_7_41,"00:03:18,270","00:03:23,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,It's actionable knowledge for some
cs-410_1_7_42,"00:03:24,990","00:03:31,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"For example, we might be able to determine"
cs-410_1_7_43,"00:03:31,510","00:03:36,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,or a better choice for
cs-410_1_7_44,"00:03:38,115","00:03:43,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Now, such an outcome could be"
cs-410_1_7_45,"00:03:43,190","00:03:49,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,because a consumer can take the knowledge
cs-410_1_7_46,"00:03:49,550","00:03:55,131",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"So, in this case text mining supplies"
cs-410_1_7_47,"00:03:55,131","00:03:59,424",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"But again, the two are not so"
cs-410_1_7_48,"00:03:59,424","00:04:03,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,we don't necessarily have
cs-410_1_7_49,"00:04:06,253","00:04:09,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,Text mining is also
cs-410_1_7_50,"00:04:09,821","00:04:14,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,which is a essential component
cs-410_1_7_51,"00:04:15,910","00:04:20,434",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"Now, text retrieval refers to"
cs-410_1_7_52,"00:04:20,434","00:04:22,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,a large amount of text data.
cs-410_1_7_53,"00:04:24,140","00:04:30,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,So I've taught another separate MOOC
cs-410_1_7_54,"00:04:31,710","00:04:34,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,Where we discussed various techniques for
cs-410_1_7_55,"00:04:36,360","00:04:41,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,"If you have taken that MOOC,"
cs-410_1_7_56,"00:04:42,120","00:04:46,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,And it will be useful To know
cs-410_1_7_57,"00:04:46,700","00:04:50,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,of understanding some of
cs-410_1_7_58,"00:04:51,750","00:04:54,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,"But, if you have not taken that MOOC,"
cs-410_1_7_59,"00:04:54,350","00:04:59,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,it's also fine because in this MOOC
cs-410_1_7_60,"00:04:59,440","00:05:03,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,going to repeat some of the key concepts
cs-410_1_7_61,"00:05:03,260","00:05:06,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,But they're at the high level and
cs-410_1_7_62,"00:05:06,540","00:05:10,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,they also explain the relation between
cs-410_1_7_63,"00:05:12,320","00:05:18,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,Text retrieval is very useful for
cs-410_1_7_64,"00:05:18,278","00:05:23,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"First, text retrieval can be"
cs-410_1_7_65,"00:05:23,200","00:05:27,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,Meaning that it can help
cs-410_1_7_66,"00:05:27,600","00:05:32,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,a relatively small amount
cs-410_1_7_67,"00:05:32,030","00:05:35,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,Which is often what's needed for
cs-410_1_7_68,"00:05:36,580","00:05:41,186",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"And in this sense, text retrieval"
cs-410_1_7_69,"00:05:43,323","00:05:46,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,Text retrieval is also needed for
cs-410_1_7_70,"00:05:46,365","00:05:50,976",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,And this roughly corresponds
cs-410_1_7_71,"00:05:50,976","00:05:56,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,mining as turning text data
cs-410_1_7_72,"00:05:56,350","00:05:58,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"Once we find the patterns in text data, or"
cs-410_1_7_73,"00:05:58,970","00:06:04,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"actionable knowledge, we generally"
cs-410_1_7_74,"00:06:04,040","00:06:06,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,By looking at the original text data.
cs-410_1_7_75,"00:06:06,580","00:06:11,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,So the users would have to have some text
cs-410_1_7_76,"00:06:11,110","00:06:16,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,text data to interpret the pattern or
cs-410_1_7_77,"00:06:16,010","00:06:19,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,to verify whether a pattern
cs-410_1_7_78,"00:06:19,910","00:06:23,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,So this is a high level introduction
cs-410_1_7_79,"00:06:23,830","00:06:29,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,and the relationship between
cs-410_1_7_80,"00:06:32,110","00:06:36,554",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"Next, let's talk about text"
cs-410_1_7_81,"00:06:39,689","00:06:45,607",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,Now it's interesting to
cs-410_1_7_82,"00:06:45,607","00:06:51,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,generated by humans as subjective sensors.
cs-410_1_7_83,"00:06:53,200","00:07:03,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,"So, this slide shows an analogy"
cs-410_1_7_84,"00:07:03,420","00:07:07,832",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,And between humans as
cs-410_1_7_85,"00:07:07,832","00:07:13,993",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,"physical sensors,"
cs-410_1_7_86,"00:07:16,292","00:07:21,377",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,So in general a sensor would
cs-410_1_7_87,"00:07:21,377","00:07:26,483",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,It would sense some signal
cs-410_1_7_88,"00:07:26,483","00:07:32,205",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"then would report the signal as data,"
cs-410_1_7_89,"00:07:32,205","00:07:38,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"For example, a thermometer would watch"
cs-410_1_7_90,"00:07:38,346","00:07:43,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,then we report the temperature
cs-410_1_7_91,"00:07:44,962","00:07:49,098",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"Similarly, a geo sensor would sense"
cs-410_1_7_92,"00:07:49,098","00:07:53,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"The location specification, for"
cs-410_1_7_93,"00:07:53,740","00:07:57,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"example, in the form of longitude"
cs-410_1_7_94,"00:07:57,140","00:08:02,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,A network sends over
cs-410_1_7_95,"00:08:02,580","00:08:04,873",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,or activities in the network and
cs-410_1_7_96,"00:08:04,873","00:08:09,477",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,Some digital format of data.
cs-410_1_7_97,"00:08:09,477","00:08:16,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,Similarly we can think of
cs-410_1_7_98,"00:08:16,460","00:08:22,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,That will observe the real world and
cs-410_1_7_99,"00:08:22,050","00:08:28,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And then humans will express what they
cs-410_1_7_100,"00:08:28,440","00:08:33,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"So, in this sense, human is actually"
cs-410_1_7_101,"00:08:33,330","00:08:36,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,sense what's happening in the world and
cs-410_1_7_102,"00:08:36,200","00:08:43,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,then express what's observed in the form
cs-410_1_7_103,"00:08:43,060","00:08:47,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,"Now, looking at the text data in"
cs-410_1_7_104,"00:08:47,350","00:08:50,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,able to integrate all
cs-410_1_7_105,"00:08:50,240","00:08:54,381",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,And that's indeed needed in
cs-410_1_7_106,"00:08:56,123","00:09:01,672",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,So here we are looking at
cs-410_1_7_107,"00:09:02,725","00:09:07,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,And in general we would Be
cs-410_1_7_108,"00:09:07,518","00:09:11,982",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,about our world that
cs-410_1_7_109,"00:09:11,982","00:09:17,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And in general it will be dealing with
cs-410_1_7_110,"00:09:17,180","00:09:21,348",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,And of course the non-text data
cs-410_1_7_111,"00:09:21,348","00:09:26,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,And those non-text data can
cs-410_1_7_112,"00:09:27,840","00:09:30,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"Numerical data, categorical,"
cs-410_1_7_113,"00:09:30,830","00:09:33,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,or multi-media data like video or speech.
cs-410_1_7_114,"00:09:36,360","00:09:41,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"So, these non text data are often"
cs-410_1_7_115,"00:09:41,900","00:09:45,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"But text data is also very important,"
cs-410_1_7_116,"00:09:45,590","00:09:50,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,mostly because they contain
cs-410_1_7_117,"00:09:50,960","00:09:55,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,And they often contain
cs-410_1_7_118,"00:09:55,930","00:09:58,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,especially preferences and
cs-410_1_7_119,"00:10:01,360","00:10:07,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"So, but by treating text data as"
cs-410_1_7_120,"00:10:07,990","00:10:14,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,we can treat all this data
cs-410_1_7_121,"00:10:14,510","00:10:18,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,So the data mining problem is
cs-410_1_7_122,"00:10:18,110","00:10:22,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,turn all the data in your actionable
cs-410_1_7_123,"00:10:22,960","00:10:26,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,of it to change the real
cs-410_1_7_124,"00:10:26,260","00:10:31,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,So this means the data mining problem is
cs-410_1_7_125,"00:10:31,490","00:10:37,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,basically taking a lot of data as input
cs-410_1_7_126,"00:10:37,450","00:10:42,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,"Inside of the data mining module,"
cs-410_1_7_127,"00:10:42,260","00:10:46,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,we have a number of different
cs-410_1_7_128,"00:10:46,510","00:10:49,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,"And this is because, for"
cs-410_1_7_129,"00:10:49,940","00:10:55,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,we generally need different algorithms for
cs-410_1_7_130,"00:10:56,390","00:10:57,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,"For example,"
cs-410_1_7_131,"00:10:57,100","00:11:01,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,video data might require computer
cs-410_1_7_132,"00:11:01,870","00:11:06,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,And that would facilitate
cs-410_1_7_133,"00:11:06,050","00:11:11,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,And we also have a lot of general
cs-410_1_7_134,"00:11:11,110","00:11:16,948",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"to all kinds of data and those algorithms,"
cs-410_1_7_135,"00:11:16,948","00:11:19,692",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"Although, for a particular kind of data,"
cs-410_1_7_136,"00:11:19,692","00:11:23,287",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,we generally want to also
cs-410_1_7_137,"00:11:23,287","00:11:27,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,So this course will cover
cs-410_1_7_138,"00:11:27,939","00:11:31,994",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,are particularly useful for
cs-410_1_7_139,"00:11:31,994","00:11:41,994",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,[MUSIC]
cs-410_9_7_1,"00:00:05,960","00:00:08,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,"In this lecture, we continue"
cs-410_9_7_2,"00:00:08,625","00:00:11,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,discussing Paradigmatical
cs-410_9_7_3,"00:00:11,565","00:00:14,175",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,Earlier we introduced
cs-410_9_7_4,"00:00:14,175","00:00:16,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,Expected Overlap of
cs-410_9_7_5,"00:00:16,935","00:00:21,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In this method, we"
cs-410_9_7_6,"00:00:21,090","00:00:23,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,a word vector that represents
cs-410_9_7_7,"00:00:23,040","00:00:26,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,the probability of a
cs-410_9_7_8,"00:00:26,490","00:00:30,345",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,We measure the similarity
cs-410_9_7_9,"00:00:30,345","00:00:34,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,which can be interpreted as
cs-410_9_7_10,"00:00:34,320","00:00:36,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,randomly picked words from
cs-410_9_7_11,"00:00:36,240","00:00:38,585",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,the two contexts are identical.
cs-410_9_7_12,"00:00:38,585","00:00:42,515",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,We also discussed
cs-410_9_7_13,"00:00:42,515","00:00:45,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,The first is that
cs-410_9_7_14,"00:00:45,920","00:00:47,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,one frequent term very well over
cs-410_9_7_15,"00:00:47,900","00:00:50,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,matching more distinct terms.
cs-410_9_7_16,"00:00:50,390","00:00:55,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,It put too much emphasis on
cs-410_9_7_17,"00:00:55,235","00:01:00,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,The second is that it
cs-410_9_7_18,"00:01:00,350","00:01:03,995",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,Even a common word like
cs-410_9_7_19,"00:01:03,995","00:01:08,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,equally as content
cs-410_9_7_20,"00:01:08,885","00:01:11,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,So now we are
cs-410_9_7_21,"00:01:11,270","00:01:13,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,going to talk about how
cs-410_9_7_22,"00:01:13,715","00:01:15,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"More specifically, we're"
cs-410_9_7_23,"00:01:15,965","00:01:19,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,some retrieval heuristics
cs-410_9_7_24,"00:01:19,790","00:01:23,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,These heuristics can effectively
cs-410_9_7_25,"00:01:23,900","00:01:26,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,as these problems also
cs-410_9_7_26,"00:01:26,975","00:01:30,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,when we match a query that
cs-410_9_7_27,"00:01:30,680","00:01:32,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,"So to address the first problem,"
cs-410_9_7_28,"00:01:32,920","00:01:36,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,we can use a sublinear
cs-410_9_7_29,"00:01:36,385","00:01:37,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"That is, we don't have to use"
cs-410_9_7_30,"00:01:37,970","00:01:39,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,the raw frequency count of
cs-410_9_7_31,"00:01:39,650","00:01:42,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,a term to represent the context.
cs-410_9_7_32,"00:01:42,140","00:01:44,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,We can transform
cs-410_9_7_33,"00:01:44,780","00:01:48,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,that wouldn't emphasize so
cs-410_9_7_34,"00:01:48,025","00:01:50,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,To address the
cs-410_9_7_35,"00:01:50,130","00:01:53,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,we can put more weight
cs-410_9_7_36,"00:01:53,195","00:01:56,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,That is we can reward
cs-410_9_7_37,"00:01:56,330","00:01:58,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,This heuristic is called the IDF
cs-410_9_7_38,"00:01:58,760","00:02:01,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,term weighting in text retrieval.
cs-410_9_7_39,"00:02:01,135","00:02:05,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,IDF stands for
cs-410_9_7_40,"00:02:05,085","00:02:07,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So now, we're going to talk about"
cs-410_9_7_41,"00:02:07,010","00:02:10,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,the two heuristics
cs-410_9_7_42,"00:02:10,130","00:02:13,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,First let's talk about
cs-410_9_7_43,"00:02:13,930","00:02:16,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,That is to convert
cs-410_9_7_44,"00:02:16,400","00:02:19,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,a word in the document
cs-410_9_7_45,"00:02:19,565","00:02:23,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,that reflects our belief
cs-410_9_7_46,"00:02:23,195","00:02:27,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,about how important
cs-410_9_7_47,"00:02:27,200","00:02:32,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,So that will be
cs-410_9_7_48,"00:02:32,370","00:02:36,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,That's shown in the y-axis.
cs-410_9_7_49,"00:02:36,415","00:02:39,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"Now, in general, there are"
cs-410_9_7_50,"00:02:39,920","00:02:44,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,Let's first look at
cs-410_9_7_51,"00:02:44,250","00:02:47,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,"In this case, we're"
cs-410_9_7_52,"00:02:47,920","00:02:51,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,any non-zero counts
cs-410_9_7_53,"00:02:51,510","00:02:55,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,one and the zero count
cs-410_9_7_54,"00:02:55,450","00:02:56,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So with this mapping
cs-410_9_7_55,"00:02:56,990","00:02:59,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,all the frequencies will be
cs-410_9_7_56,"00:02:59,240","00:03:02,605",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,mapped to only two
cs-410_9_7_57,"00:03:02,605","00:03:11,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,The mapping function is shown
cs-410_9_7_58,"00:03:11,015","00:03:14,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"Now, this is naive"
cs-410_9_7_59,"00:03:14,030","00:03:16,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,because it's not
cs-410_9_7_60,"00:03:16,660","00:03:20,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"However, this actually"
cs-410_9_7_61,"00:03:20,195","00:03:25,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,emphasizing matching all
cs-410_9_7_62,"00:03:25,700","00:03:27,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So it does not allow
cs-410_9_7_63,"00:03:27,725","00:03:30,505",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,a frequency of word to
cs-410_9_7_64,"00:03:30,505","00:03:32,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"Now, the approach"
cs-410_9_7_65,"00:03:32,930","00:03:36,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,earlier in the expected
cs-410_9_7_66,"00:03:36,650","00:03:38,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,is a linear transformation.
cs-410_9_7_67,"00:03:38,225","00:03:41,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"We basically, take"
cs-410_9_7_68,"00:03:41,870","00:03:45,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,So we use the raw count
cs-410_9_7_69,"00:03:45,445","00:03:48,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,That created the problem
cs-410_9_7_70,"00:03:48,140","00:03:50,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,that we just talked about namely;
cs-410_9_7_71,"00:03:50,360","00:03:54,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,it emphasize too much on just
cs-410_9_7_72,"00:03:54,935","00:03:58,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,Matching one frequent term
cs-410_9_7_73,"00:03:58,520","00:04:02,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,So we can have a lot
cs-410_9_7_74,"00:04:02,750","00:04:04,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,of other interesting
cs-410_9_7_75,"00:04:04,475","00:04:06,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"in between the two extremes,"
cs-410_9_7_76,"00:04:06,875","00:04:10,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,and they generally form
cs-410_9_7_77,"00:04:10,640","00:04:13,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"So for example,"
cs-410_9_7_78,"00:04:13,340","00:04:16,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"logarithm of the raw count,"
cs-410_9_7_79,"00:04:16,080","00:04:19,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,and this will give us curve
cs-410_9_7_80,"00:04:19,400","00:04:21,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,that you are seeing here.
cs-410_9_7_81,"00:04:21,260","00:04:25,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"In this case, you can see"
cs-410_9_7_82,"00:04:25,295","00:04:29,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,The high counts are
cs-410_9_7_83,"00:04:29,330","00:04:33,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,so the curve is a sublinear
cs-410_9_7_84,"00:04:33,470","00:04:39,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,the weight of
cs-410_9_7_85,"00:04:39,240","00:04:42,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"This is what we want,"
cs-410_9_7_86,"00:04:42,875","00:04:47,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,terms from dominating
cs-410_9_7_87,"00:04:48,620","00:04:50,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"Now, there is also"
cs-410_9_7_88,"00:04:50,870","00:04:52,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,another interesting
cs-410_9_7_89,"00:04:52,760","00:04:55,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,a BM25 transformation which
cs-410_9_7_90,"00:04:55,430","00:04:59,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,has been shown to be very
cs-410_9_7_91,"00:04:59,945","00:05:02,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"In this transformation, we have"
cs-410_9_7_92,"00:05:02,735","00:05:07,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,a form that looks like this.
cs-410_9_7_93,"00:05:07,225","00:05:11,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,So it's k plus one multiplied
cs-410_9_7_94,"00:05:11,640","00:05:13,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"where k is a parameter,"
cs-410_9_7_95,"00:05:13,800","00:05:16,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"x is the count,"
cs-410_9_7_96,"00:05:16,485","00:05:18,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,the raw count of a word.
cs-410_9_7_97,"00:05:18,690","00:05:22,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"Now, the transformation"
cs-410_9_7_98,"00:05:22,190","00:05:25,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,that it can actually go from
cs-410_9_7_99,"00:05:25,430","00:05:28,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,one extreme to the other
cs-410_9_7_100,"00:05:28,910","00:05:34,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,k. It also interesting
cs-410_9_7_101,"00:05:34,725","00:05:37,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,k plus one in this case.
cs-410_9_7_102,"00:05:37,135","00:05:41,435",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,So this puts
cs-410_9_7_103,"00:05:41,435","00:05:43,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"on high frequency terms,"
cs-410_9_7_104,"00:05:43,040","00:05:46,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,because their weight would
cs-410_9_7_105,"00:05:46,460","00:05:50,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,"As we vary k, if we can"
cs-410_9_7_106,"00:05:50,900","00:05:52,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"So when k is set to zero,"
cs-410_9_7_107,"00:05:52,590","00:05:55,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"we roughly have the 0,1 vector."
cs-410_9_7_108,"00:05:55,680","00:05:59,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,Whereas when we set k
cs-410_9_7_109,"00:05:59,090","00:06:02,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,it will behave more like
cs-410_9_7_110,"00:06:02,075","00:06:05,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,So this transformation
cs-410_9_7_111,"00:06:05,270","00:06:07,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,far the most effective
cs-410_9_7_112,"00:06:07,880","00:06:10,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,text retrieval and it also makes
cs-410_9_7_113,"00:06:10,880","00:06:14,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,sense for our problem setup.
cs-410_9_7_114,"00:06:14,285","00:06:17,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,So we just talked about how
cs-410_9_7_115,"00:06:17,195","00:06:20,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,overemphasizing a frequency term
cs-410_9_7_116,"00:06:20,660","00:06:22,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,Now let's look at
cs-410_9_7_117,"00:06:22,850","00:06:26,585",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,and that is how we can
cs-410_9_7_118,"00:06:26,585","00:06:28,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Matching ""the"" is not surprising,"
cs-410_9_7_119,"00:06:28,935","00:06:30,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,"because ""the"" occurs everywhere."
cs-410_9_7_120,"00:06:30,645","00:06:33,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,"But matching ""eats"""
cs-410_9_7_121,"00:06:33,020","00:06:35,105",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,So how can we address
cs-410_9_7_122,"00:06:35,105","00:06:38,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,"Now in this case, we can"
cs-410_9_7_123,"00:06:38,965","00:06:42,095",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,That's commonly
cs-410_9_7_124,"00:06:42,095","00:06:45,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,IDF stands for
cs-410_9_7_125,"00:06:45,065","00:06:47,675",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,Document frequency
cs-410_9_7_126,"00:06:47,675","00:06:49,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,of the total number of
cs-410_9_7_127,"00:06:49,370","00:06:52,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,documents that contain
cs-410_9_7_128,"00:06:52,235","00:06:57,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,So here we show that the IDF
cs-410_9_7_129,"00:06:57,200","00:07:00,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,a logarithm function
cs-410_9_7_130,"00:07:00,230","00:07:05,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,of documents that match a
cs-410_9_7_131,"00:07:05,065","00:07:08,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,So K is the number of
cs-410_9_7_132,"00:07:08,870","00:07:11,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,document frequency and M
cs-410_9_7_133,"00:07:11,630","00:07:14,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,here is the total number of
cs-410_9_7_134,"00:07:14,615","00:07:21,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,The IDF function is giving
cs-410_9_7_135,"00:07:21,200","00:07:24,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,meaning that it
cs-410_9_7_136,"00:07:24,815","00:07:28,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,The maximum value is
cs-410_9_7_137,"00:07:28,805","00:07:33,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,That's when the word occurred
cs-410_9_7_138,"00:07:33,650","00:07:37,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"So that's a very rare term,"
cs-410_9_7_139,"00:07:37,235","00:07:40,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,the rare is term in
cs-410_9_7_140,"00:07:40,745","00:07:46,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,The lowest value you can
cs-410_9_7_141,"00:07:46,700","00:07:49,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,its maximum which would be M.
cs-410_9_7_142,"00:07:49,115","00:07:53,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,So that would be
cs-410_9_7_143,"00:07:53,990","00:07:57,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,close to zero in fact.
cs-410_9_7_144,"00:07:57,470","00:08:02,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So this of course measure
cs-410_9_7_145,"00:08:02,360","00:08:06,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,is used in search where we
cs-410_9_7_146,"00:08:06,740","00:08:09,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"In our case, what would"
cs-410_9_7_147,"00:08:09,960","00:08:13,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,"Well, we can also"
cs-410_9_7_148,"00:08:13,040","00:08:16,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,we can collect all the words
cs-410_9_7_149,"00:08:16,610","00:08:18,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"That is to say,"
cs-410_9_7_150,"00:08:18,590","00:08:22,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,a word that's popular in
cs-410_9_7_151,"00:08:22,225","00:08:25,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,would also have a low IDF.
cs-410_9_7_152,"00:08:25,650","00:08:29,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,"Because depending on the dataset,"
cs-410_9_7_153,"00:08:29,445","00:08:35,105",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,we can construct the context
cs-410_9_7_154,"00:08:35,105","00:08:38,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,But in the end if a term is
cs-410_9_7_155,"00:08:38,010","00:08:41,024",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,very frequent in
cs-410_9_7_156,"00:08:41,024","00:08:43,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,then it will still be frequent
cs-410_9_7_157,"00:08:43,210","00:08:47,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,in the collective
cs-410_9_7_158,"00:08:47,620","00:08:52,355",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So how can we add
cs-410_9_7_159,"00:08:52,355","00:08:56,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,improve our similarity function?
cs-410_9_7_160,"00:08:56,910","00:08:58,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"Well, here's one way"
cs-410_9_7_161,"00:08:58,565","00:09:00,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,many other ways
cs-410_9_7_162,"00:09:00,920","00:09:02,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"But this is a reasonable way,"
cs-410_9_7_163,"00:09:02,520","00:09:05,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,where we can adapt
cs-410_9_7_164,"00:09:05,825","00:09:09,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,for paradigmatical
cs-410_9_7_165,"00:09:14,120","00:09:20,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"In this case, we define the"
cs-410_9_7_166,"00:09:20,555","00:09:26,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,elements representing
cs-410_9_7_167,"00:09:26,825","00:09:29,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,So in this
cs-410_9_7_168,"00:09:29,810","00:09:36,985",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,we take sum over all
cs-410_9_7_169,"00:09:36,985","00:09:42,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,normalize the weight of
cs-410_9_7_170,"00:09:42,155","00:09:48,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,of the weights of all the words.
cs-410_9_7_171,"00:09:48,210","00:09:51,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,This is to again ensure all the
cs-410_9_7_172,"00:09:51,030","00:09:53,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,xi's will sum to
cs-410_9_7_173,"00:09:53,975","00:09:57,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,So this would be very similar
cs-410_9_7_174,"00:09:57,800","00:09:59,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,in that this vector is
cs-410_9_7_175,"00:09:59,420","00:10:04,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,actually something similar
cs-410_9_7_176,"00:10:04,015","00:10:06,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,all the xi's will sum to one.
cs-410_9_7_177,"00:10:06,685","00:10:13,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"Now, the weight of BM25 for"
cs-410_9_7_178,"00:10:14,460","00:10:18,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,If you compare this with
cs-410_9_7_179,"00:10:18,940","00:10:22,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,have a normalized count
cs-410_9_7_180,"00:10:22,930","00:10:26,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,So we only have this one
cs-410_9_7_181,"00:10:26,320","00:10:31,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,the total counts of words in
cs-410_9_7_182,"00:10:31,090","00:10:33,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,and that's what we had before.
cs-410_9_7_183,"00:10:33,430","00:10:36,039",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,But now with the BM25
cs-410_9_7_184,"00:10:36,039","00:10:38,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,we introduced something else.
cs-410_9_7_185,"00:10:38,335","00:10:42,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"First, of course,"
cs-410_9_7_186,"00:10:42,040","00:10:43,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,this count is just to
cs-410_9_7_187,"00:10:43,420","00:10:46,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,achieve the sub-linear
cs-410_9_7_188,"00:10:46,075","00:10:50,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,But we also see we introduced
cs-410_9_7_189,"00:10:50,155","00:10:56,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,and this parameter is
cs-410_9_7_190,"00:10:56,110","00:10:58,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,although zero is also possible.
cs-410_9_7_191,"00:10:58,810","00:11:02,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,But this controls
cs-410_9_7_192,"00:11:02,950","00:11:06,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,and also controls to what extent
cs-410_9_7_193,"00:11:06,535","00:11:11,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,it simulates the
cs-410_9_7_194,"00:11:11,250","00:11:14,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"So this is one parameter,"
cs-410_9_7_195,"00:11:14,830","00:11:17,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,but we also see there is
cs-410_9_7_196,"00:11:17,140","00:11:21,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,"b, and this would be"
cs-410_9_7_197,"00:11:21,115","00:11:25,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,This is a parameter to
cs-410_9_7_198,"00:11:25,405","00:11:27,294",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,"In this case,"
cs-410_9_7_199,"00:11:27,294","00:11:29,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,the normalization formula has
cs-410_9_7_200,"00:11:29,200","00:11:31,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,a average document lens here.
cs-410_9_7_201,"00:11:31,885","00:11:35,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,This is computed up
cs-410_9_7_202,"00:11:35,770","00:11:39,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,of the lenses of all the
cs-410_9_7_203,"00:11:39,880","00:11:41,605",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,"In this case, all the lenses of"
cs-410_9_7_204,"00:11:41,605","00:11:45,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,all the context of documents
cs-410_9_7_205,"00:11:45,340","00:11:48,175",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,So this average documents
cs-410_9_7_206,"00:11:48,175","00:11:50,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,will be a constant for
cs-410_9_7_207,"00:11:50,425","00:11:52,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,So it actually is only
cs-410_9_7_208,"00:11:52,795","00:11:56,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,affecting the effect
cs-410_9_7_209,"00:11:56,530","00:12:01,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,"b, here because"
cs-410_9_7_210,"00:12:01,180","00:12:07,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,But I kept it here because
cs-410_9_7_211,"00:12:07,780","00:12:10,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,for in retrieval where it would
cs-410_9_7_212,"00:12:10,840","00:12:14,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,give us a stabilized
cs-410_9_7_213,"00:12:14,770","00:12:16,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,"But for our purpose,"
cs-410_9_7_214,"00:12:16,570","00:12:21,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,this will be a constant so
cs-410_9_7_215,"00:12:21,430","00:12:28,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,the lens normalization
cs-410_9_7_216,"00:12:29,400","00:12:33,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"Now, with this definition then,"
cs-410_9_7_217,"00:12:33,295","00:12:37,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,we have a new way to define
cs-410_9_7_218,"00:12:37,810","00:12:41,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,and we can compute
cs-410_9_7_219,"00:12:41,785","00:12:43,255",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,The difference is that
cs-410_9_7_220,"00:12:43,255","00:12:44,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,the high-frequency terms will now
cs-410_9_7_221,"00:12:44,950","00:12:46,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,have a somewhat lower weights.
cs-410_9_7_222,"00:12:46,930","00:12:49,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,This would help us control
cs-410_9_7_223,"00:12:49,690","00:12:53,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,the inference of
cs-410_9_7_224,"00:12:53,575","00:12:58,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,"Now, the idea can be added"
cs-410_9_7_225,"00:12:58,000","00:12:59,905",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,That means we'll
cs-410_9_7_226,"00:12:59,905","00:13:01,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,for matching each term.
cs-410_9_7_227,"00:13:01,990","00:13:05,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,So you may recall
cs-410_9_7_228,"00:13:05,650","00:13:08,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,all the possible words
cs-410_9_7_229,"00:13:08,305","00:13:11,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,overlap between the two contexts.
cs-410_9_7_230,"00:13:11,365","00:13:15,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,The x_i and the y_i
cs-410_9_7_231,"00:13:15,790","00:13:20,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,of picking the word
cs-410_9_7_232,"00:13:20,245","00:13:22,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,"Therefore, it"
cs-410_9_7_233,"00:13:22,330","00:13:24,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,we'll see a match on this word.
cs-410_9_7_234,"00:13:24,805","00:13:26,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,"Now, IDF would give us"
cs-410_9_7_235,"00:13:26,695","00:13:29,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,the importance of
cs-410_9_7_236,"00:13:29,200","00:13:33,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,A common word will be worth
cs-410_9_7_237,"00:13:33,700","00:13:36,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So we emphasize more on
cs-410_9_7_238,"00:13:36,715","00:13:38,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=816,"So with this modification,"
cs-410_9_7_239,"00:13:38,785","00:13:40,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,then the new function will
cs-410_9_7_240,"00:13:40,660","00:13:43,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,likely address
cs-410_9_7_241,"00:13:43,270","00:13:45,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"Now, interestingly"
cs-410_9_7_242,"00:13:45,310","00:13:49,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,this approach to discover
cs-410_9_7_243,"00:13:49,825","00:13:57,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,"In general, when we re-brand"
cs-410_9_7_244,"00:13:57,430","00:13:59,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,"a context with a term vector,"
cs-410_9_7_245,"00:13:59,365","00:14:01,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,we would likely see
cs-410_9_7_246,"00:14:01,900","00:14:04,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,some terms have high weights
cs-410_9_7_247,"00:14:04,135","00:14:06,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,and other terms have low weights.
cs-410_9_7_248,"00:14:06,040","00:14:09,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,Depending on how we assign
cs-410_9_7_249,"00:14:09,490","00:14:11,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,we might be able to
cs-410_9_7_250,"00:14:11,650","00:14:13,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,discover the words that
cs-410_9_7_251,"00:14:13,720","00:14:15,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,are strongly associated with
cs-410_9_7_252,"00:14:15,700","00:14:18,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,the candidate word
cs-410_9_7_253,"00:14:18,400","00:14:20,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=858,So let's take a look at
cs-410_9_7_254,"00:14:20,560","00:14:23,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,the term vector in
cs-410_9_7_255,"00:14:23,815","00:14:29,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,We have each x_i
cs-410_9_7_256,"00:14:29,885","00:14:33,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,defined as the normalized
cs-410_9_7_257,"00:14:33,610","00:14:37,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,"Now, this weight alone only"
cs-410_9_7_258,"00:14:37,420","00:14:41,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,reflects how frequent the word
cs-410_9_7_259,"00:14:41,110","00:14:43,345",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,But we can't just say
cs-410_9_7_260,"00:14:43,345","00:14:44,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,any frequent term in
cs-410_9_7_261,"00:14:44,500","00:14:46,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,the context that would
cs-410_9_7_262,"00:14:46,560","00:14:50,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,the candidate word because
cs-410_9_7_263,"00:14:50,235","00:14:51,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,many common words like 'the' will
cs-410_9_7_264,"00:14:51,990","00:14:54,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,occur frequently in
cs-410_9_7_265,"00:14:54,540","00:14:59,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,But if we apply IDF
cs-410_9_7_266,"00:14:59,645","00:15:07,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,we can then re-weight
cs-410_9_7_267,"00:15:07,090","00:15:09,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,That means the words that are
cs-410_9_7_268,"00:15:09,220","00:15:11,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,common like 'the'
cs-410_9_7_269,"00:15:11,920","00:15:14,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,So now the highest
cs-410_9_7_270,"00:15:14,920","00:15:18,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,those common terms because
cs-410_9_7_271,"00:15:18,220","00:15:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,"Instead, those terms would"
cs-410_9_7_272,"00:15:20,980","00:15:23,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,be the terms that are
cs-410_9_7_273,"00:15:23,920","00:15:26,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,but not frequent
cs-410_9_7_274,"00:15:26,080","00:15:29,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,So those are clearly the words
cs-410_9_7_275,"00:15:29,590","00:15:33,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=929,the context of the candidate
cs-410_9_7_276,"00:15:33,820","00:15:35,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,"So for this reason,"
cs-410_9_7_277,"00:15:35,365","00:15:39,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,the highly weighted terms in
cs-410_9_7_278,"00:15:39,865","00:15:42,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,can also be assumed to
cs-410_9_7_279,"00:15:42,310","00:15:45,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,be candidates for
cs-410_9_7_280,"00:15:45,940","00:15:48,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,"Now, of course, this is"
cs-410_9_7_281,"00:15:48,895","00:15:53,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,our approach for discovering
cs-410_9_7_282,"00:15:53,560","00:15:57,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,"In the next lecture, we're"
cs-410_9_7_283,"00:15:57,025","00:16:01,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,how to discover
cs-410_9_7_284,"00:16:02,280","00:16:05,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,But it clearly shows the relation
cs-410_9_7_285,"00:16:05,305","00:16:08,995",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,between discovering
cs-410_9_7_286,"00:16:08,995","00:16:12,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,Indeed they can be discovered in
cs-410_9_7_287,"00:16:12,670","00:16:18,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,a joint manner by leveraging
cs-410_9_7_288,"00:16:18,340","00:16:22,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,"So to summarize,"
cs-410_9_7_289,"00:16:22,600","00:16:26,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,paradigmatic relations is to
cs-410_9_7_290,"00:16:26,050","00:16:27,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,collect the context of
cs-410_9_7_291,"00:16:27,610","00:16:30,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=987,a candidate word to
cs-410_9_7_292,"00:16:30,460","00:16:33,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=990,This is typically represented
cs-410_9_7_293,"00:16:33,685","00:16:35,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,Then compute the similarity of
cs-410_9_7_294,"00:16:35,890","00:16:38,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,the corresponding
cs-410_9_7_295,"00:16:38,005","00:16:40,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,of two candidate words.
cs-410_9_7_296,"00:16:40,540","00:16:45,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,Then we can take
cs-410_9_7_297,"00:16:45,910","00:16:50,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,and treat them as having
cs-410_9_7_298,"00:16:50,305","00:16:53,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,These are the words that
cs-410_9_7_299,"00:16:53,395","00:16:55,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,There are many different ways to
cs-410_9_7_300,"00:16:55,540","00:16:58,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,implement this general idea.
cs-410_9_7_301,"00:16:58,090","00:17:01,435",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,We just talked about
cs-410_9_7_302,"00:17:01,435","00:17:04,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,"More specifically, we"
cs-410_9_7_303,"00:17:04,510","00:17:07,765",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1024,text retrieval models to help us
cs-410_9_7_304,"00:17:07,765","00:17:10,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,design effective
cs-410_9_7_305,"00:17:10,690","00:17:15,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,compute the
cs-410_9_7_306,"00:17:15,960","00:17:19,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"More specifically, we have used"
cs-410_9_7_307,"00:17:19,330","00:17:23,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1039,the BM25 and IDF weighting
cs-410_9_7_308,"00:17:23,020","00:17:27,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,to discover
cs-410_9_7_309,"00:17:27,250","00:17:30,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,These approaches also represent
cs-410_9_7_310,"00:17:30,100","00:17:33,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,the state of the art in
cs-410_9_7_311,"00:17:33,310","00:17:37,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1053,"Finally, syntagmatic relations"
cs-410_9_7_312,"00:17:37,165","00:17:42,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1057,as a by-product when we discover
cs-410_6_2_1,"00:00:00,000","00:00:07,148",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_6_2_2,"00:00:07,148","00:00:12,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about how to do faster
cs-410_6_2_3,"00:00:14,710","00:00:19,487",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we're going to continue"
cs-410_6_2_4,"00:00:19,487","00:00:20,583",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In particular,"
cs-410_6_2_5,"00:00:20,583","00:00:25,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,we're going to talk about how to support
cs-410_6_2_6,"00:00:26,906","00:00:31,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,So let's think about what a general
cs-410_6_2_7,"00:00:32,730","00:00:37,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"Now of course, the vector space"
cs-410_6_2_8,"00:00:37,260","00:00:40,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,we can imagine many other retrieval
cs-410_6_2_9,"00:00:42,390","00:00:44,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,So the form of this
cs-410_6_2_10,"00:00:46,060","00:00:49,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,We see this scoring function
cs-410_6_2_11,"00:00:49,870","00:00:55,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,a query Q is defined as
cs-410_6_2_12,"00:00:55,260","00:01:00,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,that adjustment a function that
cs-410_6_2_13,"00:01:00,350","00:01:05,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"That I'll assume here at the end,"
cs-410_6_2_14,"00:01:05,425","00:01:09,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,f sub d of d and f sub q of q.
cs-410_6_2_15,"00:01:09,100","00:01:14,467",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,These are adjustment factors
cs-410_6_2_16,"00:01:14,467","00:01:19,387",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,so they are at the level of a document and
cs-410_6_2_17,"00:01:19,387","00:01:22,719",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"So and then inside of this function,"
cs-410_6_2_18,"00:01:22,719","00:01:27,127",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,we also see there's
cs-410_6_2_19,"00:01:27,127","00:01:33,102",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So this is the main part
cs-410_6_2_20,"00:01:33,102","00:01:38,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,these as I just said of
cs-410_6_2_21,"00:01:38,365","00:01:43,931",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,the level of the whole document and
cs-410_6_2_22,"00:01:43,931","00:01:47,521",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"For example, document [INAUDIBLE] and"
cs-410_6_2_23,"00:01:47,521","00:01:52,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,this aggregate punching would
cs-410_6_2_24,"00:01:52,805","00:01:55,847",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,"Now inside this h function,"
cs-410_6_2_25,"00:01:55,847","00:02:01,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,there are functions that
cs-410_6_2_26,"00:02:01,300","00:02:06,384",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,of the contribution of
cs-410_6_2_27,"00:02:08,475","00:02:14,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"So this g,"
cs-410_6_2_28,"00:02:14,305","00:02:19,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,of a matched query term ti in document d.
cs-410_6_2_29,"00:02:23,710","00:02:28,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And this h function would then
cs-410_6_2_30,"00:02:28,365","00:02:34,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"So for example,"
cs-410_6_2_31,"00:02:36,110","00:02:39,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,but it can also be a product or it could
cs-410_6_2_32,"00:02:41,250","00:02:46,207",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"And then finally, this adjustment"
cs-410_6_2_33,"00:02:46,207","00:02:51,162",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,the document level or query level
cs-410_6_2_34,"00:02:51,162","00:02:53,697",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"for example, document [INAUDIBLE]."
cs-410_6_2_35,"00:02:53,697","00:02:58,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"So, this general form would cover"
cs-410_6_2_36,"00:02:58,960","00:03:06,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,Let's look at how we can score documents
cs-410_6_2_37,"00:03:07,610","00:03:10,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"So, here's a general algorithm"
cs-410_6_2_38,"00:03:10,930","00:03:14,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,First this query level and
cs-410_6_2_39,"00:03:14,670","00:03:19,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,document level factors can be
cs-410_6_2_40,"00:03:19,540","00:03:22,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,"Of course, for the query we have to"
cs-410_6_2_41,"00:03:22,810","00:03:28,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"document, for example,"
cs-410_6_2_42,"00:03:28,180","00:03:32,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"And then, we maintain a score accumulator"
cs-410_6_2_43,"00:03:34,710","00:03:39,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,An h is an aggregation function
cs-410_6_2_44,"00:03:39,440","00:03:40,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,So how do we do that?
cs-410_6_2_45,"00:03:40,530","00:03:45,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,For each period term we're going to
cs-410_6_2_46,"00:03:45,770","00:03:47,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,from the invert index.
cs-410_6_2_47,"00:03:47,130","00:03:51,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,This will give us all the documents
cs-410_6_2_48,"00:03:52,850","00:03:57,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"and that includes d1, f1 and so dn fn."
cs-410_6_2_49,"00:03:57,640","00:04:03,394",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,So each pair is a document ID and
cs-410_6_2_50,"00:04:03,394","00:04:08,268",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,Then for each entry d sub j and
cs-410_6_2_51,"00:04:08,268","00:04:12,436",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,of the term in this
cs-410_6_2_52,"00:04:12,436","00:04:17,739",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,We'll going to compute the function
cs-410_6_2_53,"00:04:17,739","00:04:19,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,"weight of this term, so"
cs-410_6_2_54,"00:04:19,370","00:04:26,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,we're computing the weight completion of
cs-410_6_2_55,"00:04:26,170","00:04:31,152",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"And then, we're going to update"
cs-410_6_2_56,"00:04:31,152","00:04:35,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,this document and
cs-410_6_2_57,"00:04:35,820","00:04:41,144",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,accumulator that would
cs-410_6_2_58,"00:04:41,144","00:04:46,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So this is basically a general
cs-410_6_2_59,"00:04:46,640","00:04:51,288",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,functions of this form by
cs-410_6_2_60,"00:04:51,288","00:04:54,621",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,Note that we don't have to
cs-410_6_2_61,"00:04:54,621","00:04:56,906",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,that didn't match any query term.
cs-410_6_2_62,"00:04:56,906","00:04:59,096",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"Well, this is why it's fast,"
cs-410_6_2_63,"00:04:59,096","00:05:04,418",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,we only need to process the documents
cs-410_6_2_64,"00:05:04,418","00:05:09,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"In the end, then we're going to adjust"
cs-410_6_2_65,"00:05:09,415","00:05:11,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,sub a and then we can sort.
cs-410_6_2_66,"00:05:11,600","00:05:14,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,So let's take a look
cs-410_6_2_67,"00:05:14,270","00:05:17,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"In this case, let's assume the scoring"
cs-410_6_2_68,"00:05:17,880","00:05:24,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,"it just takes the sum of t f, the role of"
cs-410_6_2_69,"00:05:25,830","00:05:31,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,This simplification would help
cs-410_6_2_70,"00:05:31,340","00:05:36,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,It's very easy to extend the computation
cs-410_6_2_71,"00:05:36,640","00:05:43,422",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"the transformation of tf, or [INAUDIBLE]"
cs-410_6_2_72,"00:05:43,422","00:05:47,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"So let's take a look at specific example,"
cs-410_6_2_73,"00:05:48,980","00:05:54,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,and it show some entries of
cs-410_6_2_74,"00:05:54,600","00:05:56,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Information occurred in four documents and
cs-410_6_2_75,"00:05:56,800","00:06:01,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"their frequencies are also there,"
cs-410_6_2_76,"00:06:01,260","00:06:07,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"So let's see how the arrows works, so"
cs-410_6_2_77,"00:06:07,210","00:06:09,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"and we fetch the first query then,"
cs-410_6_2_78,"00:06:09,580","00:06:12,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"That's information, right?"
cs-410_6_2_79,"00:06:12,260","00:06:16,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And imagine we have all these
cs-410_6_2_80,"00:06:17,800","00:06:19,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,scores for these documents.
cs-410_6_2_81,"00:06:19,800","00:06:21,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,We can imagine there will be other but
cs-410_6_2_82,"00:06:21,740","00:06:24,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,then they will only be
cs-410_6_2_83,"00:06:24,660","00:06:28,681",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,"So before we do any waiting of terms,"
cs-410_6_2_84,"00:06:28,681","00:06:32,979",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,we don't even need a score of.
cs-410_6_2_85,"00:06:32,979","00:06:36,859",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,That comes actually we have these score
cs-410_6_2_86,"00:06:38,260","00:06:43,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,So lets fetch the interest from
cs-410_6_2_87,"00:06:43,110","00:06:45,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,"information, that the first one."
cs-410_6_2_88,"00:06:46,260","00:06:50,809",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,So these four accumulators obviously
cs-410_6_2_89,"00:06:51,830","00:06:54,418",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,"So, the first entry is d1 and 3,"
cs-410_6_2_90,"00:06:54,418","00:06:58,357",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,3 is occurrences of
cs-410_6_2_91,"00:06:58,357","00:07:03,617",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,Since our scoring function assume that the
cs-410_6_2_92,"00:07:03,617","00:07:09,178",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We just need to add a 3 to the score
cs-410_6_2_93,"00:07:09,178","00:07:16,388",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,the increase of score due to matching
cs-410_6_2_94,"00:07:16,388","00:07:19,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"And then, we go to the next entry,"
cs-410_6_2_95,"00:07:19,679","00:07:22,493",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,then we add a 4 to the score
cs-410_6_2_96,"00:07:22,493","00:07:27,614",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"Of course, at this point, that we will"
cs-410_6_2_97,"00:07:27,614","00:07:33,427",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,"And so at this point, we allocated"
cs-410_6_2_98,"00:07:33,427","00:07:39,174",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"and we add one, we allocate another"
cs-410_6_2_99,"00:07:39,174","00:07:44,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"And then finally,"
cs-410_6_2_100,"00:07:44,370","00:07:50,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,information occurred five
cs-410_6_2_101,"00:07:50,450","00:07:55,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"Okay, so this completes the processing of"
cs-410_6_2_102,"00:07:55,310","00:07:56,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,information.
cs-410_6_2_103,"00:07:56,500","00:08:00,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,It processed all the contributions
cs-410_6_2_104,"00:08:00,080","00:08:00,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,four documents.
cs-410_6_2_105,"00:08:01,830","00:08:06,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"So now, our error will go to"
cs-410_6_2_106,"00:08:06,900","00:08:09,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"So, we're going to fetch all"
cs-410_6_2_107,"00:08:10,830","00:08:11,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"So, in this case,"
cs-410_6_2_108,"00:08:11,520","00:08:15,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"there are three entries, and"
cs-410_6_2_109,"00:08:15,700","00:08:18,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,The first is d2 and 3 and
cs-410_6_2_110,"00:08:18,410","00:08:22,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,that means security occur three
cs-410_6_2_111,"00:08:22,890","00:08:26,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"Well, we do exactly the same,"
cs-410_6_2_112,"00:08:26,300","00:08:31,557",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"So, this time we're going to change the"
cs-410_6_2_113,"00:08:31,557","00:08:36,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,allocated and
cs-410_6_2_114,"00:08:36,390","00:08:40,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"value which is a 4, so"
cs-410_6_2_115,"00:08:41,530","00:08:46,333",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,D2 score is increased because the match
cs-410_6_2_116,"00:08:46,333","00:08:47,382",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,the security.
cs-410_6_2_117,"00:08:47,382","00:08:53,721",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Go to the next entry, that's d4 and"
cs-410_6_2_118,"00:08:53,721","00:08:59,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"d4 and again, we add 1 to d4 so"
cs-410_6_2_119,"00:08:59,040","00:09:02,449",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,"Finally, we process d5 and a 3."
cs-410_6_2_120,"00:09:02,449","00:09:07,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,Since we have not yet allocated a score
cs-410_6_2_121,"00:09:07,679","00:09:12,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"we're going to allocate 1 for d5,"
cs-410_6_2_122,"00:09:12,190","00:09:19,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,"So, those scores, of the last rule,"
cs-410_6_2_123,"00:09:20,480","00:09:25,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,If our scoring function is just
cs-410_6_2_124,"00:09:27,080","00:09:31,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"Now, what if we, actually,"
cs-410_6_2_125,"00:09:31,600","00:09:35,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"Well, we going to do the [INAUDIBLE]"
cs-410_6_2_126,"00:09:36,490","00:09:40,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"So, to summarize this,"
cs-410_6_2_127,"00:09:40,020","00:09:44,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,we first process the information
cs-410_6_2_128,"00:09:44,660","00:09:49,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,we processed all the entries
cs-410_6_2_129,"00:09:49,520","00:09:54,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"Then we process the security,"
cs-410_6_2_130,"00:09:54,775","00:10:00,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,what should be the order of processing
cs-410_6_2_131,"00:10:00,916","00:10:05,677",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,It might make a difference especially
cs-410_6_2_132,"00:10:05,677","00:10:07,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,the score accumulators.
cs-410_6_2_133,"00:10:07,670","00:10:12,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"Let's say, we only want to keep"
cs-410_6_2_134,"00:10:12,226","00:10:15,601",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,What do you think would be
cs-410_6_2_135,"00:10:15,601","00:10:22,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Would you process a common term first or
cs-410_6_2_136,"00:10:24,460","00:10:30,597",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,The answers is we just go to who
cs-410_6_2_137,"00:10:30,597","00:10:35,531",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"A rare term would match a few documents,"
cs-410_6_2_138,"00:10:35,531","00:10:38,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"be higher,"
cs-410_6_2_139,"00:10:38,910","00:10:44,933",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"And then, it allows us to attach"
cs-410_6_2_140,"00:10:44,933","00:10:48,042",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"So, it helps pruning"
cs-410_6_2_141,"00:10:48,042","00:10:51,901",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,if we don't need so
cs-410_6_2_142,"00:10:51,901","00:10:55,474",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,So those are all heuristics for
cs-410_6_2_143,"00:10:55,474","00:10:59,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,Here you can also see how we can
cs-410_6_2_144,"00:10:59,850","00:11:03,192",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So they can [INAUDIBLE] when we
cs-410_6_2_145,"00:11:03,192","00:11:04,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,each query time.
cs-410_6_2_146,"00:11:04,700","00:11:08,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,When we fetch the inverted index we
cs-410_6_2_147,"00:11:08,420","00:11:09,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,then we can compute IDF.
cs-410_6_2_148,"00:11:09,990","00:11:13,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,Or maybe perhaps the IDF value
cs-410_6_2_149,"00:11:13,710","00:11:16,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,when we indexed the documents.
cs-410_6_2_150,"00:11:16,890","00:11:22,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"At that time, we already computed"
cs-410_6_2_151,"00:11:22,780","00:11:26,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,so all these can be done at this time.
cs-410_6_2_152,"00:11:26,570","00:11:29,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,So that would mean when we process
cs-410_6_2_153,"00:11:29,820","00:11:35,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,these words would be adjusted by the same
cs-410_6_2_154,"00:11:36,590","00:11:39,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,So this is the basic idea of using
cs-410_6_2_155,"00:11:39,580","00:11:44,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,it works well for all kinds of
cs-410_6_2_156,"00:11:44,770","00:11:49,726",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"And this generally,"
cs-410_6_2_157,"00:11:49,726","00:11:53,397",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,most state of art retrieval functions.
cs-410_6_2_158,"00:11:53,397","00:11:58,708",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,So there are some tricks to
cs-410_6_2_159,"00:11:58,708","00:12:02,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,some general techniques
cs-410_6_2_160,"00:12:02,988","00:12:07,756",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,This is we just store some results of
cs-410_6_2_161,"00:12:07,756","00:12:12,291",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"when you see the same query,"
cs-410_6_2_162,"00:12:12,291","00:12:17,781",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,"Similarly, you can also slow the list"
cs-410_6_2_163,"00:12:17,781","00:12:19,041",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,a popular term.
cs-410_6_2_164,"00:12:19,041","00:12:21,268",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,"And if the query term is popular likely,"
cs-410_6_2_165,"00:12:21,268","00:12:25,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,you will soon need to factor the inverted
cs-410_6_2_166,"00:12:25,620","00:12:30,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,"So keeping it in the memory would help,"
cs-410_6_2_167,"00:12:30,569","00:12:32,206",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,improving efficiency.
cs-410_6_2_168,"00:12:32,206","00:12:36,694",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,We can also keep only the most promising
cs-410_6_2_169,"00:12:36,694","00:12:39,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,doesn't want to examine so many documents.
cs-410_6_2_170,"00:12:39,281","00:12:44,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,We only need to return high
cs-410_6_2_171,"00:12:44,092","00:12:46,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,likely are ranked on the top.
cs-410_6_2_172,"00:12:47,900","00:12:51,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,"For that purpose,"
cs-410_6_2_173,"00:12:51,860","00:12:53,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,We don't have to store
cs-410_6_2_174,"00:12:53,810","00:12:59,936",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,"At some point, we just keep"
cs-410_6_2_175,"00:12:59,936","00:13:06,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,Another technique is to do parallel
cs-410_6_2_176,"00:13:06,257","00:13:11,731",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,really process in such a large
cs-410_6_2_177,"00:13:11,731","00:13:15,869",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,And you scale up to
cs-410_6_2_178,"00:13:15,869","00:13:20,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,the special techniques you
cs-410_6_2_179,"00:13:20,628","00:13:25,609",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,to distribute the storage
cs-410_6_2_180,"00:13:25,609","00:13:31,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,So here is a list of some text retrieval
cs-410_6_2_181,"00:13:31,850","00:13:37,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=811,You can find more information
cs-410_6_2_182,"00:13:37,160","00:13:42,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"And here, I listed your four here,"
cs-410_6_2_183,"00:13:42,510","00:13:48,361",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,that can support a lot of applications and
cs-410_6_2_184,"00:13:48,361","00:13:51,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,You can use it to build a search
cs-410_6_2_185,"00:13:51,900","00:13:55,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,The downside is that it's not
cs-410_6_2_186,"00:13:55,555","00:14:01,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,the algorithms implemented they are also
cs-410_6_2_187,"00:14:01,500","00:14:06,294",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,Lemur or Indri is another
cs-410_6_2_188,"00:14:06,294","00:14:10,068",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,a nice support web
cs-410_6_2_189,"00:14:10,068","00:14:16,094",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,it has many advanced search algorithms and
cs-410_6_2_190,"00:14:16,094","00:14:20,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,Terrier is yet another toolkit
cs-410_6_2_191,"00:14:20,735","00:14:25,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,application capability and
cs-410_6_2_192,"00:14:25,108","00:14:30,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,So that's maybe in between Lemur or
cs-410_6_2_193,"00:14:30,008","00:14:34,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,maybe rather combining
cs-410_6_2_194,"00:14:34,663","00:14:38,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,so that's also useful tool kit.
cs-410_6_2_195,"00:14:38,110","00:14:41,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,MeTA is a toolkit that we will use for
cs-410_6_2_196,"00:14:41,920","00:14:46,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,the problem assignment and
cs-410_6_2_197,"00:14:47,690","00:14:54,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,a combination of both text retrieval
cs-410_6_2_198,"00:14:54,250","00:15:01,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,And so talking models are implement they
cs-410_6_2_199,"00:15:01,390","00:15:06,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=901,implemented in the toolkit as
cs-410_6_2_200,"00:15:06,720","00:15:10,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=906,So to summarize all the discussion
cs-410_6_2_201,"00:15:11,600","00:15:14,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,here are the major takeaway points.
cs-410_6_2_202,"00:15:14,700","00:15:20,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,Inverted index is the primary data
cs-410_6_2_203,"00:15:20,760","00:15:25,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,and that's the key to enable
cs-410_6_2_204,"00:15:26,350","00:15:31,116",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,And the basic idea is to preprocess
cs-410_6_2_205,"00:15:31,116","00:15:34,491",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,we want to do compression
cs-410_6_2_206,"00:15:34,491","00:15:39,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,So that we can save disk space and
cs-410_6_2_207,"00:15:39,625","00:15:43,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,processing of inverted index in general.
cs-410_6_2_208,"00:15:43,840","00:15:48,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,We talked about how to construct the
cs-410_6_2_209,"00:15:48,400","00:15:49,179",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,the memory.
cs-410_6_2_210,"00:15:49,179","00:15:54,374",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=949,And then we talk about faster search using
cs-410_6_2_211,"00:15:54,374","00:15:59,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=954,the invective index to accumulate a scores
cs-410_6_2_212,"00:15:59,960","00:16:03,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,And we exploit the Zipf's law to
cs-410_6_2_213,"00:16:03,760","00:16:06,052",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,that don't match any query term and
cs-410_6_2_214,"00:16:06,052","00:16:11,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,this algorithm can actually support
cs-410_6_2_215,"00:16:13,400","00:16:17,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,So these basic techniques
cs-410_6_2_216,"00:16:17,630","00:16:23,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=977,further scaling up using distributed file
cs-410_6_2_217,"00:16:23,570","00:16:28,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,Here are two additional readings you
cs-410_6_2_218,"00:16:28,410","00:16:31,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,you are interested in
cs-410_6_2_219,"00:16:31,040","00:16:38,156",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,The first one is a classical
cs-410_6_2_220,"00:16:38,156","00:16:41,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,o inverted index and
cs-410_6_2_221,"00:16:41,590","00:16:46,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,"And how to,"
cs-410_6_2_222,"00:16:46,035","00:16:49,811",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,"any inputs of the space,"
cs-410_6_2_223,"00:16:49,811","00:16:54,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,The second one is a newer textbook that
cs-410_6_2_224,"00:16:54,802","00:16:56,675",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,evaluating search engines.
cs-410_6_2_225,"00:16:58,835","00:17:08,835",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,[MUSIC]
cs-410_4_2_1,"00:00:00,248","00:00:06,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_4_2_2,"00:00:06,368","00:00:10,308",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about the implementation
cs-410_4_2_3,"00:00:12,878","00:00:17,327",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,In this lecture we will discuss
cs-410_4_2_4,"00:00:17,327","00:00:20,768",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,retrieval method to build a search engine.
cs-410_4_2_5,"00:00:20,768","00:00:24,753",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,The main challenge is to
cs-410_4_2_6,"00:00:24,753","00:00:30,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,to enable a query to be answered very
cs-410_4_2_7,"00:00:30,698","00:00:34,858",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,This is a typical text
cs-410_4_2_8,"00:00:34,858","00:00:39,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,We can see the documents are first
cs-410_4_2_9,"00:00:39,805","00:00:43,498",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"get tokenized units, for example, words."
cs-410_4_2_10,"00:00:43,498","00:00:48,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,"And then, these words, or"
cs-410_4_2_11,"00:00:48,058","00:00:53,188",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"a indexer that will create a index,"
cs-410_4_2_12,"00:00:53,188","00:00:57,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,the search engine to use
cs-410_4_2_13,"00:00:57,280","00:01:01,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,And the query would be going
cs-410_4_2_14,"00:01:01,830","00:01:05,761",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,So the Tokenizer would be
cs-410_4_2_15,"00:01:05,761","00:01:09,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,so that the text can be
cs-410_4_2_16,"00:01:09,200","00:01:12,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,The same units would be
cs-410_4_2_17,"00:01:12,960","00:01:17,604",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,The query's representation would
cs-410_4_2_18,"00:01:17,604","00:01:22,506",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,which would use the index to quickly
cs-410_4_2_19,"00:01:22,506","00:01:25,268",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,the documents and then ranking them.
cs-410_4_2_20,"00:01:25,268","00:01:27,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,The results will be given to the user.
cs-410_4_2_21,"00:01:27,628","00:01:32,033",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,And then the user can look at the results
cs-410_4_2_22,"00:01:32,033","00:01:35,126",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,explicit judgements of both
cs-410_4_2_23,"00:01:35,126","00:01:36,603",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,which documents are bad.
cs-410_4_2_24,"00:01:36,603","00:01:43,353",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,Or implicit feedback such as so that
cs-410_4_2_25,"00:01:43,353","00:01:46,187",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"End user will just look at the results,"
cs-410_4_2_26,"00:01:46,187","00:01:49,193",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"skip some, and"
cs-410_4_2_27,"00:01:49,193","00:01:55,353",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,So these interacting signals can be used
cs-410_4_2_28,"00:01:55,353","00:02:01,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,accuracy by assuming that viewed documents
cs-410_4_2_29,"00:02:01,718","00:02:05,678",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,So a search engine system then
cs-410_4_2_30,"00:02:05,678","00:02:10,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"The first part is the indexer, and"
cs-410_4_2_31,"00:02:10,738","00:02:16,458",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"responds to the users query, and"
cs-410_4_2_32,"00:02:16,458","00:02:21,072",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"Now typically, the Indexer is"
cs-410_4_2_33,"00:02:21,072","00:02:24,179",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,you can pre-process the correct data and
cs-410_4_2_34,"00:02:24,179","00:02:29,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"to build the inventory index,"
cs-410_4_2_35,"00:02:29,168","00:02:34,819",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And this data structure can then be used
cs-410_4_2_36,"00:02:34,819","00:02:40,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,to process a user's query dynamically and
cs-410_4_2_37,"00:02:40,668","00:02:45,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,The feedback mechanism can be done online
cs-410_4_2_38,"00:02:45,368","00:02:50,367",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,The implementation of the indexer and
cs-410_4_2_39,"00:02:50,367","00:02:55,378",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,and this is the main topic of this
cs-410_4_2_40,"00:02:55,378","00:02:59,843",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"The feedback mechanism,"
cs-410_4_2_41,"00:02:59,843","00:03:02,378",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,it depends on which method is used.
cs-410_4_2_42,"00:03:02,378","00:03:08,818",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,So that is usually done in
cs-410_4_2_43,"00:03:08,818","00:03:11,538",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,Let's first talk about the tokenizer.
cs-410_4_2_44,"00:03:11,538","00:03:16,578",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,Tokernization is a normalized lexical
cs-410_4_2_45,"00:03:16,578","00:03:21,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,so that semantically similar words
cs-410_4_2_46,"00:03:21,368","00:03:25,133",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,"Now, in the language like English,"
cs-410_4_2_47,"00:03:25,133","00:03:29,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,this will map all the inflectional
cs-410_4_2_48,"00:03:29,548","00:03:33,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"So for example, computer, computation, and"
cs-410_4_2_49,"00:03:33,047","00:03:37,078",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,computing can all be matched
cs-410_4_2_50,"00:03:37,078","00:03:43,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,This way all these different forms of
cs-410_4_2_51,"00:03:43,628","00:03:46,433",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,"Now normally, this is a good idea,"
cs-410_4_2_52,"00:03:46,433","00:03:52,337",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,to increase the coverage of documents
cs-410_4_2_53,"00:03:52,337","00:03:55,553",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"But it's also not always beneficial,"
cs-410_4_2_54,"00:03:55,553","00:04:00,914",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,because sometimes the subtlest
cs-410_4_2_55,"00:04:00,914","00:04:07,558",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,computation might still suggest the
cs-410_4_2_56,"00:04:07,558","00:04:13,398",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"But in most cases,"
cs-410_4_2_57,"00:04:13,398","00:04:19,363",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,When we tokenize the text in some other
cs-410_4_2_58,"00:04:19,363","00:04:25,338",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,face some special challenges in segmenting
cs-410_4_2_59,"00:04:25,338","00:04:29,697",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,Because it's not obvious
cs-410_4_2_60,"00:04:29,697","00:04:32,928",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,there's no space to separate them.
cs-410_4_2_61,"00:04:32,928","00:04:41,638",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,"So here of course, we have to use some"
cs-410_4_2_62,"00:04:41,638","00:04:47,144",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"Once we do tokenization, then we would"
cs-410_4_2_63,"00:04:47,144","00:04:52,748",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,convert the documents and do some data
cs-410_4_2_64,"00:04:52,748","00:04:58,298",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,The basic idea is to precompute
cs-410_4_2_65,"00:04:58,298","00:05:02,848",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,So the most commonly used index
cs-410_4_2_66,"00:05:02,848","00:05:06,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,And this has been used
cs-410_4_2_67,"00:05:06,555","00:05:09,768",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,to support basic search algorithms.
cs-410_4_2_68,"00:05:09,768","00:05:13,426",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"Sometimes the other indices, for example,"
cs-410_4_2_69,"00:05:13,426","00:05:19,498",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,document index might be needed in order
cs-410_4_2_70,"00:05:19,498","00:05:24,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,And these kind of techniques
cs-410_4_2_71,"00:05:24,106","00:05:28,828",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,that they vary a lot according
cs-410_4_2_72,"00:05:28,828","00:05:34,549",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,To understand why we want to use
cs-410_4_2_73,"00:05:34,549","00:05:40,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,you to think about how you would
cs-410_4_2_74,"00:05:40,698","00:05:44,938",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,So if you want to use more time to
cs-410_4_2_75,"00:05:44,938","00:05:49,584",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,So think about how you can
cs-410_4_2_76,"00:05:49,584","00:05:54,768",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,that you can quickly respond
cs-410_4_2_77,"00:05:54,768","00:05:58,466",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Where if you have thought
cs-410_4_2_78,"00:05:58,466","00:06:02,811",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,you might realize that where
cs-410_4_2_79,"00:06:02,811","00:06:07,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,the list of documents that match
cs-410_4_2_80,"00:06:07,718","00:06:11,788",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"In this way, you can basically"
cs-410_4_2_81,"00:06:11,788","00:06:17,503",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,So when you see a term you can simply just
cs-410_4_2_82,"00:06:17,503","00:06:20,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,that term and return the list to the user.
cs-410_4_2_83,"00:06:20,508","00:06:24,928",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,So that's the fastest way to
cs-410_4_2_84,"00:06:24,928","00:06:30,468",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,Now the idea of the invert index
cs-410_4_2_85,"00:06:30,468","00:06:36,017",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,We're going to do pre-constructed
cs-410_4_2_86,"00:06:36,017","00:06:41,388",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,us to quickly find all the documents
cs-410_4_2_87,"00:06:41,388","00:06:43,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,So let's take a look at this example.
cs-410_4_2_88,"00:06:43,878","00:06:45,439",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,"We have three documents here,"
cs-410_4_2_89,"00:06:45,439","00:06:49,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,and these are the documents that you
cs-410_4_2_90,"00:06:49,168","00:06:52,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,Suppose that we want to create
cs-410_4_2_91,"00:06:52,916","00:06:57,502",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Then we want to maintain a dictionary, in"
cs-410_4_2_92,"00:06:57,502","00:07:01,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,each term and we're going to store
cs-410_4_2_93,"00:07:01,628","00:07:05,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"For example, the number of"
cs-410_4_2_94,"00:07:05,960","00:07:09,458",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,the total number of code or
cs-410_4_2_95,"00:07:09,458","00:07:14,148",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,which means we would kind of duplicate
cs-410_4_2_96,"00:07:14,148","00:07:17,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,"And so, for example, news,"
cs-410_4_2_97,"00:07:17,415","00:07:22,253",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,this term occur in all
cs-410_4_2_98,"00:07:22,253","00:07:26,198",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,so the count of documents is three.
cs-410_4_2_99,"00:07:26,198","00:07:32,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,And you might also realize we needed this
cs-410_4_2_100,"00:07:32,820","00:07:38,002",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,for computing some statistics to
cs-410_4_2_101,"00:07:38,002","00:07:42,422",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,Can you think of that?
cs-410_4_2_102,"00:07:42,422","00:07:49,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,So what weighting heuristic
cs-410_4_2_103,"00:07:49,862","00:07:53,622",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"Well, that's the idea, right,"
cs-410_4_2_104,"00:07:53,622","00:07:58,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"So, IDF is the property of a term,"
cs-410_4_2_105,"00:07:58,300","00:08:03,291",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"So, with the document that count here,"
cs-410_4_2_106,"00:08:03,291","00:08:06,556",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,"either at this time, or"
cs-410_4_2_107,"00:08:06,556","00:08:10,134",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,At random time when we see a query.
cs-410_4_2_108,"00:08:10,134","00:08:13,641",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"Now in addition to these basic statistics,"
cs-410_4_2_109,"00:08:13,641","00:08:18,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,we'll also store all the documents
cs-410_4_2_110,"00:08:18,380","00:08:23,049",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,and these entries are stored
cs-410_4_2_111,"00:08:24,150","00:08:27,595",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,So in this case it matched
cs-410_4_2_112,"00:08:27,595","00:08:31,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,we store information about
cs-410_4_2_113,"00:08:31,680","00:08:38,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"This is the document id,"
cs-410_4_2_114,"00:08:38,160","00:08:45,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,"The tf is one for news, in the second"
cs-410_4_2_115,"00:08:45,240","00:08:50,864",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So from this list, we can get all"
cs-410_4_2_116,"00:08:50,864","00:08:55,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,we can also know the frequency
cs-410_4_2_117,"00:08:55,320","00:08:58,214",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"So, if the query has just one word,"
cs-410_4_2_118,"00:08:58,214","00:09:01,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,we have easily look up to this
cs-410_4_2_119,"00:09:01,628","00:09:06,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,go quicker into the postings to fetch
cs-410_4_2_120,"00:09:06,780","00:09:08,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,"So, let's take a look at another term."
cs-410_4_2_121,"00:09:09,280","00:09:12,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"This time, let's take a look"
cs-410_4_2_122,"00:09:14,130","00:09:17,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"This would occur in only one document,"
cs-410_4_2_123,"00:09:17,950","00:09:23,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,So the document frequency is 1 but
cs-410_4_2_124,"00:09:23,490","00:09:29,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"So the frequency count is two, and"
cs-410_4_2_125,"00:09:29,210","00:09:33,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,some other reachable method where
cs-410_4_2_126,"00:09:34,490","00:09:38,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,assess the popularity of
cs-410_4_2_127,"00:09:38,770","00:09:42,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,Similarly we'll have a pointer
cs-410_4_2_128,"00:09:42,930","00:09:47,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,"and in this case,"
cs-410_4_2_129,"00:09:48,900","00:09:53,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,the term occurred in just one document and
cs-410_4_2_130,"00:09:53,570","00:09:57,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,The document id is 3 and
cs-410_4_2_131,"00:09:59,610","00:10:02,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,So this is the basic
cs-410_4_2_132,"00:10:02,550","00:10:04,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,"It's actually pretty simple, right?"
cs-410_4_2_133,"00:10:06,580","00:10:12,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,With this structure we can easily fetch
cs-410_4_2_134,"00:10:12,370","00:10:15,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,And this will be the basis for
cs-410_4_2_135,"00:10:15,760","00:10:23,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Now sometimes we also want to store
cs-410_4_2_136,"00:10:25,220","00:10:31,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,So in many of these cases the term
cs-410_4_2_137,"00:10:31,960","00:10:34,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,So there's only one position for
cs-410_4_2_138,"00:10:35,810","00:10:40,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"But in this case, the term occurred"
cs-410_4_2_139,"00:10:40,990","00:10:44,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,Now the position information is very
cs-410_4_2_140,"00:10:44,690","00:10:48,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,the matching of query terms is
cs-410_4_2_141,"00:10:48,400","00:10:51,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"let's say, five words or ten words."
cs-410_4_2_142,"00:10:52,410","00:11:00,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,"Or, whether the matching of the two query"
cs-410_4_2_143,"00:11:00,700","00:11:04,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,That this can all be checked quickly
cs-410_4_2_144,"00:11:05,920","00:11:10,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,"So, why is inverted index good for"
cs-410_4_2_145,"00:11:10,160","00:11:16,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,"Well, we just talked about the possibility"
cs-410_4_2_146,"00:11:16,349","00:11:17,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,And that's very easy.
cs-410_4_2_147,"00:11:17,990","00:11:19,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,What about the multiple term queries?
cs-410_4_2_148,"00:11:19,910","00:11:23,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,Well let's first look at the some
cs-410_4_2_149,"00:11:23,800","00:11:27,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,A Boolean query is basically
cs-410_4_2_150,"00:11:27,740","00:11:36,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,So I want the value in the document
cs-410_4_2_151,"00:11:36,290","00:11:38,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,So that's one conjunctive query.
cs-410_4_2_152,"00:11:38,770","00:11:45,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,Or I want the web documents
cs-410_4_2_153,"00:11:45,440","00:11:46,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,That's a disjunctive query.
cs-410_4_2_154,"00:11:46,540","00:11:51,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,But how can we answer such
cs-410_4_2_155,"00:11:52,090","00:11:53,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"Well if you think a bit about it,"
cs-410_4_2_156,"00:11:53,860","00:11:58,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,it would be obvious because
cs-410_4_2_157,"00:11:58,130","00:12:03,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,the documents that match term A and also
cs-410_4_2_158,"00:12:03,170","00:12:08,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,And then just take the intersection
cs-410_4_2_159,"00:12:08,160","00:12:13,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,Or to take the union to
cs-410_4_2_160,"00:12:13,050","00:12:16,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,So this is all very easy to answer.
cs-410_4_2_161,"00:12:16,020","00:12:17,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,It's going to be very quick.
cs-410_4_2_162,"00:12:17,780","00:12:20,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,Now what about the multi-term
cs-410_4_2_163,"00:12:20,850","00:12:24,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,We talked about the vector space model for
cs-410_4_2_164,"00:12:24,390","00:12:28,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,we will do a match such query with
cs-410_4_2_165,"00:12:28,940","00:12:32,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,And the score is based on
cs-410_4_2_166,"00:12:32,330","00:12:35,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,So in this case it's not
cs-410_4_2_167,"00:12:35,670","00:12:38,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,the scoring can be actually
cs-410_4_2_168,"00:12:38,770","00:12:42,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,Basically it's similar to
cs-410_4_2_169,"00:12:42,430","00:12:45,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,"Basically, it's like A or B."
cs-410_4_2_170,"00:12:45,140","00:12:50,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,We take the union of all the documents
cs-410_4_2_171,"00:12:50,680","00:12:53,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,then we would aggregate the term weights.
cs-410_4_2_172,"00:12:53,320","00:13:01,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,So this is a basic idea of using inverted
cs-410_4_2_173,"00:13:01,420","00:13:05,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,And we're going to talk about
cs-410_4_2_174,"00:13:05,210","00:13:06,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,"But for now,"
cs-410_4_2_175,"00:13:06,000","00:13:12,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,let's just look at the question
cs-410_4_2_176,"00:13:12,210","00:13:17,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,Basically why is more efficient than
cs-410_4_2_177,"00:13:17,470","00:13:20,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,This is the obvious approach.
cs-410_4_2_178,"00:13:20,770","00:13:27,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,You can just compute a score for each
cs-410_4_2_179,"00:13:27,518","00:13:29,936",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,And this is a straightforward method but
cs-410_4_2_180,"00:13:29,936","00:13:34,496",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,this is going to be very slow imagine
cs-410_4_2_181,"00:13:34,496","00:13:39,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=814,If you do this then it will take
cs-410_4_2_182,"00:13:39,620","00:13:44,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,So the question now is why would
cs-410_4_2_183,"00:13:44,975","00:13:48,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,Well it has to do is the word
cs-410_4_2_184,"00:13:48,780","00:13:54,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,"So, here's some common phenomena"
cs-410_4_2_185,"00:13:54,010","00:13:58,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,There are some languages independent
cs-410_4_2_186,"00:14:00,300","00:14:07,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,And these patterns are basically
cs-410_4_2_187,"00:14:07,690","00:14:10,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,A few words like the common
cs-410_4_2_188,"00:14:10,830","00:14:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,"we occur very, very frequently in text."
cs-410_4_2_189,"00:14:14,780","00:14:18,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,So they account for
cs-410_4_2_190,"00:14:19,405","00:14:22,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,But most words would occur just rarely.
cs-410_4_2_191,"00:14:22,885","00:14:25,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,"There are many words that occur just once,"
cs-410_4_2_192,"00:14:25,615","00:14:29,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"let's say, in a document or"
cs-410_4_2_193,"00:14:29,790","00:14:33,306",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,And there are many such.
cs-410_4_2_194,"00:14:33,306","00:14:37,977",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,It's also true that the most
cs-410_4_2_195,"00:14:37,977","00:14:40,462",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,they have to be rare in another.
cs-410_4_2_196,"00:14:40,462","00:14:45,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,That means although the general
cs-410_4_2_197,"00:14:45,800","00:14:51,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,was observed in many cases that
cs-410_4_2_198,"00:14:51,060","00:14:54,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,may vary from context to context.
cs-410_4_2_199,"00:14:54,770","00:14:59,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,So this phenomena is characterized
cs-410_4_2_200,"00:14:59,450","00:15:02,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,This law says that the rank of a word
cs-410_4_2_201,"00:15:02,210","00:15:06,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,multiplied by the frequency of
cs-410_4_2_202,"00:15:07,450","00:15:13,045",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,So formally if we use F(w)
cs-410_4_2_203,"00:15:13,045","00:15:16,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,r(w) to denote the rank of a word.
cs-410_4_2_204,"00:15:16,310","00:15:17,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,Then this is the formula.
cs-410_4_2_205,"00:15:17,390","00:15:21,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,"It basically says the same thing,"
cs-410_4_2_206,"00:15:21,300","00:15:28,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,Where C is basically a constant and
cs-410_4_2_207,"00:15:28,510","00:15:34,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,"alpha, that might be adjusted to"
cs-410_4_2_208,"00:15:34,180","00:15:38,128",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,So if I plot the word
cs-410_4_2_209,"00:15:38,128","00:15:40,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=938,then you can see this more easily.
cs-410_4_2_210,"00:15:40,980","00:15:43,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,The x axis is basically the word rank.
cs-410_4_2_211,"00:15:43,660","00:15:50,393",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,This is r(w) and
cs-410_4_2_212,"00:15:50,393","00:15:57,448",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,Now this curve shows that the product
cs-410_4_2_213,"00:15:57,448","00:16:02,524",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,"Now if you look at these words, we can see"
cs-410_4_2_214,"00:16:02,524","00:16:06,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,"In the middle,"
cs-410_4_2_215,"00:16:06,870","00:16:11,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,These words tend to occur
cs-410_4_2_216,"00:16:11,440","00:16:14,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,they are not like those
cs-410_4_2_217,"00:16:14,890","00:16:17,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,And they are also not very rare.
cs-410_4_2_218,"00:16:18,190","00:16:21,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,So they tend to be often used in
cs-410_4_2_219,"00:16:22,700","00:16:28,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,queries and they also tend
cs-410_4_2_220,"00:16:28,240","00:16:31,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,These intermediate frequency words.
cs-410_4_2_221,"00:16:31,290","00:16:34,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,But if you look at the left
cs-410_4_2_222,"00:16:35,820","00:16:38,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,these are the highest frequency words.
cs-410_4_2_223,"00:16:38,330","00:16:39,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,They are covered very frequently.
cs-410_4_2_224,"00:16:39,620","00:16:45,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=999,"They are usually words,"
cs-410_4_2_225,"00:16:45,540","00:16:49,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,"Those words are very, very frequent and"
cs-410_4_2_226,"00:16:49,440","00:16:54,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,"discriminated, and they are generally"
cs-410_4_2_227,"00:16:54,226","00:17:01,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,So they are often removed and
cs-410_4_2_228,"00:17:01,900","00:17:06,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,So you can use pretty much just the kind
cs-410_4_2_229,"00:17:06,960","00:17:09,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1026,infer what words might be stop words.
cs-410_4_2_230,"00:17:09,620","00:17:12,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1029,Those are basically
cs-410_4_2_231,"00:17:13,780","00:17:18,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,And they also occupy a lot of
cs-410_4_2_232,"00:17:18,500","00:17:23,048",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,You can imagine the posting entries for
cs-410_4_2_233,"00:17:23,048","00:17:24,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,"And then therefore,"
cs-410_4_2_234,"00:17:24,370","00:17:28,299",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,if you can remove such words you can save
cs-410_4_2_235,"00:17:29,890","00:17:35,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,"We also show the tail part,"
cs-410_4_2_236,"00:17:35,100","00:17:38,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"Those words don't occur very frequently,"
cs-410_4_2_237,"00:17:39,680","00:17:41,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1059,Those words are actually very useful for
cs-410_4_2_238,"00:17:41,330","00:17:45,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1061,"search also, if a user happens to"
cs-410_4_2_239,"00:17:45,630","00:17:49,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1065,"But because they're rare,"
cs-410_4_2_240,"00:17:49,730","00:17:54,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1069,aren't necessarily
cs-410_4_2_241,"00:17:54,030","00:17:58,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1074,But retain them would allow us to
cs-410_4_2_242,"00:17:58,970","00:18:00,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1078,They generally have very high IDF.
cs-410_4_2_243,"00:18:05,559","00:18:10,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,So what kind of data structures should
cs-410_4_2_244,"00:18:10,840","00:18:11,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1090,"Well, it has two parts, right."
cs-410_4_2_245,"00:18:11,970","00:18:16,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,"If you recall, we have a dictionary and"
cs-410_4_2_246,"00:18:16,720","00:18:21,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1096,"The dictionary has modest size, although"
cs-410_4_2_247,"00:18:21,810","00:18:24,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1101,large but compare it with
cs-410_4_2_248,"00:18:26,220","00:18:29,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,And we also need to have fast
cs-410_4_2_249,"00:18:29,710","00:18:32,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1109,because we're going to look up
cs-410_4_2_250,"00:18:32,940","00:18:39,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,"So therefore, we'd prefer to keep such"
cs-410_4_2_251,"00:18:39,200","00:18:43,333",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,"If the collection is not very large,"
cs-410_4_2_252,"00:18:43,333","00:18:47,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,if the collection is very large
cs-410_4_2_253,"00:18:47,810","00:18:52,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1127,"If the vocabulary size is very large,"
cs-410_4_2_254,"00:18:52,100","00:18:55,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1132,"So, in general that's how it goes."
cs-410_4_2_255,"00:18:55,800","00:18:58,578",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1135,So the data structures
cs-410_4_2_256,"00:18:58,578","00:19:01,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1138,"storing dictionary,"
cs-410_4_2_257,"00:19:01,390","00:19:04,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,"There are structures like hash table, or"
cs-410_4_2_258,"00:19:04,375","00:19:09,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1144,b-tree if we can't store
cs-410_4_2_259,"00:19:09,090","00:19:12,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1149,And then try to build a structure that
cs-410_4_2_260,"00:19:14,530","00:19:16,705",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1154,For postings they are huge.
cs-410_4_2_261,"00:19:18,045","00:19:24,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1158,"And in general, we don't have to have"
cs-410_4_2_262,"00:19:24,815","00:19:29,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1164,We generally would just look up
cs-410_4_2_263,"00:19:29,145","00:19:32,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1169,frequencies for all the documents
cs-410_4_2_264,"00:19:33,930","00:19:36,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1173,So would read those entries sequentially.
cs-410_4_2_265,"00:19:37,670","00:19:43,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1177,And therefore because it's large and
cs-410_4_2_266,"00:19:43,704","00:19:49,826",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1183,they have to stay on disc and they would
cs-410_4_2_267,"00:19:49,826","00:19:53,392",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,term frequency or
cs-410_4_2_268,"00:19:53,392","00:19:58,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1193,"Now because they are very large,"
cs-410_4_2_269,"00:19:59,360","00:20:04,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1199,"Now this is not only to save disc space,"
cs-410_4_2_270,"00:20:04,390","00:20:09,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1204,"one benefit of compression, it It's"
cs-410_4_2_271,"00:20:09,080","00:20:11,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,But it's also to help improving speed.
cs-410_4_2_272,"00:20:13,110","00:20:15,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1213,Can you see why?
cs-410_4_2_273,"00:20:15,980","00:20:23,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1215,"Well, we know that input and"
cs-410_4_2_274,"00:20:23,470","00:20:28,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1223,In comparison with the time taken by CPU.
cs-410_4_2_275,"00:20:28,320","00:20:33,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1228,"So, CPU is much faster but"
cs-410_4_2_276,"00:20:33,410","00:20:39,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1233,"so by compressing the inverter index,"
cs-410_4_2_277,"00:20:39,335","00:20:45,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1239,"the entries, that we have the readings,"
cs-410_4_2_278,"00:20:45,115","00:20:50,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1245,"would be smaller, and"
cs-410_4_2_279,"00:20:50,150","00:20:55,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1250,the amount of tracking IO and
cs-410_4_2_280,"00:20:55,080","00:21:00,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1255,"Of course, we have to then do more"
cs-410_4_2_281,"00:21:00,270","00:21:03,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1260,uncompress the data in the memory.
cs-410_4_2_282,"00:21:03,630","00:21:05,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1263,But as I said CPU is fast.
cs-410_4_2_283,"00:21:05,550","00:21:07,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1265,So over all we can still save time.
cs-410_4_2_284,"00:21:08,360","00:21:11,301",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1268,So compression here is both
cs-410_4_2_285,"00:21:11,301","00:21:14,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1271,to speed up the loading of the index.
cs-410_4_2_286,"00:21:14,035","00:21:24,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1274,[MUSIC]
cs-410_5_2_1,"00:00:00,012","00:00:03,586",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_2_2,"00:00:07,325","00:00:10,038",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the inverted index
cs-410_5_2_3,"00:00:10,038","00:00:11,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,construction.
cs-410_5_2_4,"00:00:13,840","00:00:18,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we will continue"
cs-410_5_2_5,"00:00:18,520","00:00:22,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In particular, we're going to discuss"
cs-410_5_2_6,"00:00:25,096","00:00:29,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,The construction of the inverted index
cs-410_5_2_7,"00:00:29,259","00:00:30,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,very small.
cs-410_5_2_8,"00:00:30,450","00:00:35,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,It's very easy to construct a dictionary
cs-410_5_2_9,"00:00:36,600","00:00:42,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,The problem is that when our data
cs-410_5_2_10,"00:00:42,280","00:00:45,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,then we have to use some
cs-410_5_2_11,"00:00:46,500","00:00:51,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,"And unfortunately, in most retrieval"
cs-410_5_2_12,"00:00:51,900","00:00:55,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And they generally cannot be
cs-410_5_2_13,"00:00:56,790","00:01:01,843",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And there are many approaches to
cs-410_5_2_14,"00:01:01,843","00:01:06,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,method is quite common and
cs-410_5_2_15,"00:01:06,710","00:01:11,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"First, you collect the local termID,"
cs-410_5_2_16,"00:01:11,480","00:01:16,946",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,Basically you will locate the terms
cs-410_5_2_17,"00:01:16,946","00:01:24,117",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,And then once you collect those accounts
cs-410_5_2_18,"00:01:24,117","00:01:29,104",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So that you will be able to local
cs-410_5_2_19,"00:01:29,104","00:01:31,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,these are called rounds.
cs-410_5_2_20,"00:01:31,310","00:01:36,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,And then you write them into
cs-410_5_2_21,"00:01:36,930","00:01:38,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,then you merge in step 3.
cs-410_5_2_22,"00:01:38,940","00:01:44,104",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"Do pairwise merging of these runs, until"
cs-410_5_2_23,"00:01:44,104","00:01:46,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,generate a single inverted index.
cs-410_5_2_24,"00:01:47,700","00:01:50,823",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,So this is an illustration of this method.
cs-410_5_2_25,"00:01:50,823","00:01:54,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,On the left you see some documents and
cs-410_5_2_26,"00:01:54,265","00:01:59,942",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,on the right we have a term lexicon and
cs-410_5_2_27,"00:01:59,942","00:02:08,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,These lexicons are to map string-based
cs-410_5_2_28,"00:02:08,070","00:02:12,261",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,terms into integer representations or
cs-410_5_2_29,"00:02:12,261","00:02:18,112",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,map back from integers to
cs-410_5_2_30,"00:02:18,112","00:02:23,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,The reason why we want our interest
cs-410_5_2_31,"00:02:23,010","00:02:26,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,IDs is because integers
cs-410_5_2_32,"00:02:26,930","00:02:29,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"For example,"
cs-410_5_2_33,"00:02:29,770","00:02:33,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"array, and they are also easy to compress."
cs-410_5_2_34,"00:02:34,390","00:02:40,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,So this is one reason why we tend
cs-410_5_2_35,"00:02:42,180","00:02:46,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,so that we don't have to
cs-410_5_2_36,"00:02:46,710","00:02:48,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,So how does this approach work?
cs-410_5_2_37,"00:02:48,070","00:02:49,822",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"Well, it's very simple."
cs-410_5_2_38,"00:02:49,822","00:02:53,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,We're going to scan these
cs-410_5_2_39,"00:02:53,260","00:02:58,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,then parse the documents and
cs-410_5_2_40,"00:02:58,260","00:03:03,306",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,And in this stage we generally sort
cs-410_5_2_41,"00:03:03,306","00:03:06,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,because we process each
cs-410_5_2_42,"00:03:06,961","00:03:11,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,So we'll first encounter all
cs-410_5_2_43,"00:03:11,310","00:03:18,786",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,Therefore the document IDs
cs-410_5_2_44,"00:03:18,786","00:03:25,503",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,And this will be followed by document IDs
cs-410_5_2_45,"00:03:25,503","00:03:31,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,only just because we process
cs-410_5_2_46,"00:03:31,280","00:03:34,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"At some point,"
cs-410_5_2_47,"00:03:34,890","00:03:39,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,that would have to write
cs-410_5_2_48,"00:03:39,080","00:03:45,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,Before we do that we 're going to sort
cs-410_5_2_49,"00:03:45,830","00:03:51,948",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,We can sort them and then this time
cs-410_5_2_50,"00:03:51,948","00:03:59,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"Note that here,"
cs-410_5_2_51,"00:03:59,459","00:04:03,827",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,So all the entries that share the same
cs-410_5_2_52,"00:04:03,827","00:04:08,557",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"In this case,"
cs-410_5_2_53,"00:04:08,557","00:04:14,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,that match term 1 would
cs-410_5_2_54,"00:04:14,090","00:04:18,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,And we're going to write this into
cs-410_5_2_55,"00:04:18,850","00:04:22,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,And would that allows you to
cs-410_5_2_56,"00:04:22,800","00:04:24,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,makes a batch of documents.
cs-410_5_2_57,"00:04:24,030","00:04:26,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,And we're going to do that for
cs-410_5_2_58,"00:04:26,670","00:04:32,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,So we're going to write a lot of
cs-410_5_2_59,"00:04:32,546","00:04:35,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And then the next stage is
cs-410_5_2_60,"00:04:35,400","00:04:38,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,We're going to merge them and
cs-410_5_2_61,"00:04:38,360","00:04:41,729",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"Eventually, we will get"
cs-410_5_2_62,"00:04:41,729","00:04:45,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,where the entries are sorted
cs-410_5_2_63,"00:04:46,960","00:04:50,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"And on the top, we're going to see"
cs-410_5_2_64,"00:04:50,870","00:04:53,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,the documents that match term ID 1.
cs-410_5_2_65,"00:04:53,620","00:05:00,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"So this is basically, how we can do"
cs-410_5_2_66,"00:05:00,300","00:05:06,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,Even though the data cannot be
cs-410_5_2_67,"00:05:06,445","00:05:12,562",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"Now, we mention earlier that"
cs-410_5_2_68,"00:05:12,562","00:05:15,848",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,it's desirable to compress them.
cs-410_5_2_69,"00:05:15,848","00:05:20,481",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,So let's now take a little bit
cs-410_5_2_70,"00:05:20,481","00:05:24,084",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"Well the idea of compression in general,"
cs-410_5_2_71,"00:05:24,084","00:05:28,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,leverage skewed distributions of values.
cs-410_5_2_72,"00:05:28,310","00:05:31,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,And we generally have to use
cs-410_5_2_73,"00:05:31,090","00:05:36,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,instead of the fixed-length
cs-410_5_2_74,"00:05:36,830","00:05:41,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,a program manager like C++.
cs-410_5_2_75,"00:05:41,080","00:05:45,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,And so how can we leverage
cs-410_5_2_76,"00:05:45,840","00:05:48,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,to compress these values?
cs-410_5_2_77,"00:05:48,180","00:05:53,827",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Well in general, we will use few"
cs-410_5_2_78,"00:05:53,827","00:06:00,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,words at the cost of using longer
cs-410_5_2_79,"00:06:00,650","00:06:04,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,"So in our case, let's think about how"
cs-410_5_2_80,"00:06:05,640","00:06:09,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,"Now, if you can picture what"
cs-410_5_2_81,"00:06:09,640","00:06:13,807",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"you will see in post things,"
cs-410_5_2_82,"00:06:13,807","00:06:19,089",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,Those are the frequencies of
cs-410_5_2_83,"00:06:19,089","00:06:25,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"Now, if you think about it, what kind"
cs-410_5_2_84,"00:06:25,650","00:06:29,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,You probably will be able to guess
cs-410_5_2_85,"00:06:29,980","00:06:32,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,far more frequently than large numbers.
cs-410_5_2_86,"00:06:32,540","00:06:33,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,Why?
cs-410_5_2_87,"00:06:33,990","00:06:39,954",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"Well, think about the distribution of"
cs-410_5_2_88,"00:06:39,954","00:06:44,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,and many words occur just rarely so
cs-410_5_2_89,"00:06:44,810","00:06:48,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,"Therefore, we can use fewer bits for"
cs-410_5_2_90,"00:06:48,419","00:06:53,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,highly frequent integers and
cs-410_5_2_91,"00:06:53,855","00:06:57,095",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,that's cost of using more bits for
cs-410_5_2_92,"00:06:58,445","00:07:00,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,This is a trade off of course.
cs-410_5_2_93,"00:07:00,005","00:07:05,712",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"If the values are distributed to uniform,"
cs-410_5_2_94,"00:07:05,712","00:07:10,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,but because we tend to see many small
cs-410_5_2_95,"00:07:10,824","00:07:15,769",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,We can save on average even though
cs-410_5_2_96,"00:07:15,769","00:07:17,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,we have to use a lot of bits.
cs-410_5_2_97,"00:07:19,750","00:07:23,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,What about the document IDs
cs-410_5_2_98,"00:07:23,700","00:07:27,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,Well they are not distributed
cs-410_5_2_99,"00:07:27,240","00:07:31,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,So how can we deal with that?
cs-410_5_2_100,"00:07:31,840","00:07:35,488",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,Well it turns out that we can
cs-410_5_2_101,"00:07:35,488","00:07:38,686",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,that is to store the difference
cs-410_5_2_102,"00:07:38,686","00:07:43,495",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,And we can imagine if a term has
cs-410_5_2_103,"00:07:43,495","00:07:46,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,there will be longest of document IDs.
cs-410_5_2_104,"00:07:46,640","00:07:52,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,"So when we take the gap, and we take the"
cs-410_5_2_105,"00:07:52,030","00:07:54,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,those gaps will be small.
cs-410_5_2_106,"00:07:54,340","00:07:57,594",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"So again, see a lot of small numbers."
cs-410_5_2_107,"00:07:57,594","00:08:00,217",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,Whereas if a term occurred
cs-410_5_2_108,"00:08:00,217","00:08:04,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,"then the gap would be large,"
cs-410_5_2_109,"00:08:04,300","00:08:06,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"So this creates some skewed distribution,"
cs-410_5_2_110,"00:08:06,610","00:08:10,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,that would allow us to
cs-410_5_2_111,"00:08:11,850","00:08:15,621",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,This is also possible because
cs-410_5_2_112,"00:08:15,621","00:08:21,249",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"uncompress these document IDs,"
cs-410_5_2_113,"00:08:21,249","00:08:25,484",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,Because we stored the difference and
cs-410_5_2_114,"00:08:25,484","00:08:29,574",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,document ID we have to first
cs-410_5_2_115,"00:08:29,574","00:08:34,536",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,And then we can add the difference to
cs-410_5_2_116,"00:08:34,536","00:08:36,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,the current document ID.
cs-410_5_2_117,"00:08:36,365","00:08:40,834",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Now this was possible because we only
cs-410_5_2_118,"00:08:40,834","00:08:42,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,those document IDs.
cs-410_5_2_119,"00:08:42,900","00:08:46,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"Once we look up the term, we look up all"
cs-410_5_2_120,"00:08:46,920","00:08:48,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,then we sequentially process them.
cs-410_5_2_121,"00:08:48,670","00:08:52,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"So it's very natural,"
cs-410_5_2_122,"00:08:53,600","00:08:55,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,And there are many different methods for
cs-410_5_2_123,"00:08:55,760","00:09:02,116",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,So binary code is a commonly used
cs-410_5_2_124,"00:09:02,116","00:09:05,994",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,We use basically fixed glance in coding.
cs-410_5_2_125,"00:09:05,994","00:09:09,276",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"Unary code, gamma code, and"
cs-410_5_2_126,"00:09:09,276","00:09:11,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,there are many other possibilities.
cs-410_5_2_127,"00:09:11,240","00:09:14,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,So let's look at some
cs-410_5_2_128,"00:09:14,130","00:09:16,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,Binary coding is really
cs-410_5_2_129,"00:09:16,900","00:09:20,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,that's a property for
cs-410_5_2_130,"00:09:20,930","00:09:24,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,The unary coding is a variable
cs-410_5_2_131,"00:09:24,700","00:09:28,891",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"In this case, integer this 1 will be"
cs-410_5_2_132,"00:09:28,891","00:09:33,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"encoded as x -1, 1 bit followed by 0."
cs-410_5_2_133,"00:09:33,630","00:09:39,329",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,"So for example, 3 will be encoded as 2,"
cs-410_5_2_134,"00:09:39,329","00:09:45,042",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"whereas 5 will be encoded as 4,"
cs-410_5_2_135,"00:09:45,042","00:09:51,599",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,So now you can imagine how many bits do we
cs-410_5_2_136,"00:09:51,599","00:09:57,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,So how many bits do you have to
cs-410_5_2_137,"00:09:57,450","00:10:02,549",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"Well exactly, we have to use 100 bits."
cs-410_5_2_138,"00:10:02,549","00:10:07,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,So it's the same number of bits
cs-410_5_2_139,"00:10:07,150","00:10:12,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,So this is very inefficient if you
cs-410_5_2_140,"00:10:12,360","00:10:17,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,Imagine if you occasionally see a number
cs-410_5_2_141,"00:10:17,620","00:10:22,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,So this only works well if you
cs-410_5_2_142,"00:10:22,894","00:10:28,082",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"no large numbers, mostly very"
cs-410_5_2_143,"00:10:28,082","00:10:30,184",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"Now, how do you decode this code?"
cs-410_5_2_144,"00:10:30,184","00:10:33,662",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,Now since these are variable
cs-410_5_2_145,"00:10:33,662","00:10:37,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,you can't just count how many bits and
cs-410_5_2_146,"00:10:38,500","00:10:44,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"You can't say 8-bits or 32-bits,"
cs-410_5_2_147,"00:10:44,800","00:10:50,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"They are variable length, so"
cs-410_5_2_148,"00:10:50,860","00:10:55,192",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"In this case for unary, you can see"
cs-410_5_2_149,"00:10:55,192","00:10:59,161",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,Now you can easily see 0 would
cs-410_5_2_150,"00:10:59,161","00:11:03,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So you just count up how many 1s you
cs-410_5_2_151,"00:11:03,120","00:11:06,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,"You have finished one number,"
cs-410_5_2_152,"00:11:07,960","00:11:11,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,Now we just saw that unary
cs-410_5_2_153,"00:11:11,266","00:11:13,987",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"In rewarding small numbers, and"
cs-410_5_2_154,"00:11:13,987","00:11:20,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,if you occasionally can see a very
cs-410_5_2_155,"00:11:20,430","00:11:24,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,So what about some other
cs-410_5_2_156,"00:11:24,900","00:11:29,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,Well gamma coding's one of them and
cs-410_5_2_157,"00:11:29,200","00:11:34,072",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,in this method we can use unary coding for
cs-410_5_2_158,"00:11:34,072","00:11:37,239",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=694,a transform form of that.
cs-410_5_2_159,"00:11:37,239","00:11:41,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,So it's 1 plus the floor of log of x.
cs-410_5_2_160,"00:11:41,210","00:11:47,781",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,So the magnitude of this value is
cs-410_5_2_161,"00:11:47,781","00:11:52,703",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,So that's why we can afford
cs-410_5_2_162,"00:11:52,703","00:11:58,728",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,And so first I have the unary code for
cs-410_5_2_163,"00:11:58,728","00:12:02,327",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,And this would be followed by
cs-410_5_2_164,"00:12:02,327","00:12:08,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"And this basically the same uniform code,"
cs-410_5_2_165,"00:12:08,058","00:12:15,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,And we're going to use this coder to code
cs-410_5_2_166,"00:12:15,956","00:12:22,178",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,And this is basically precisely
cs-410_5_2_167,"00:12:25,000","00:12:30,376",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,So the unary code are basically
cs-410_5_2_168,"00:12:30,376","00:12:33,029",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,well add one there and here.
cs-410_5_2_169,"00:12:33,029","00:12:38,428",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,But the remaining part
cs-410_5_2_170,"00:12:38,428","00:12:43,413",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,code through actually code the difference
cs-410_5_2_171,"00:12:43,413","00:12:47,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,between the x and this 2 to the log of x.
cs-410_5_2_172,"00:12:49,530","00:12:53,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,And it's easy to show that for this
cs-410_5_2_173,"00:12:55,790","00:13:00,297",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,difference we only need to use up
cs-410_5_2_174,"00:13:00,297","00:13:05,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,to this many bits and
cs-410_5_2_175,"00:13:06,530","00:13:08,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"And this is easy to understand,"
cs-410_5_2_176,"00:13:08,410","00:13:12,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"if the difference is too large, then we"
cs-410_5_2_177,"00:13:14,330","00:13:19,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,So here are some examples for
cs-410_5_2_178,"00:13:19,000","00:13:22,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,The first two digits are the unary code.
cs-410_5_2_179,"00:13:22,575","00:13:26,706",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,"So this isn't for the value 2,"
cs-410_5_2_180,"00:13:26,706","00:13:30,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,10 encodes 2 in unary coding.
cs-410_5_2_181,"00:13:32,490","00:13:37,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,And so that means the floor of
cs-410_5_2_182,"00:13:37,300","00:13:42,398",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"log of x is 1,"
cs-410_5_2_183,"00:13:42,398","00:13:45,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"In code 1 plus the flow of log of x,"
cs-410_5_2_184,"00:13:45,620","00:13:50,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,since this is two then we know that
cs-410_5_2_185,"00:13:52,000","00:13:55,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,So that 3 is still larger than 2 to the 1.
cs-410_5_2_186,"00:13:55,720","00:14:00,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,"So the difference is 1, and"
cs-410_5_2_187,"00:14:01,460","00:14:04,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,So that's why we have 101 for 3.
cs-410_5_2_188,"00:14:04,690","00:14:11,554",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"Now similarly 5 is encoded as 110,"
cs-410_5_2_189,"00:14:12,970","00:14:17,981",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,And in this case the unary code in code 3.
cs-410_5_2_190,"00:14:17,981","00:14:25,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,And so this is a unary code 110 and
cs-410_5_2_191,"00:14:25,445","00:14:30,362",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,And that means we're going to
cs-410_5_2_192,"00:14:30,362","00:14:32,784",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,the 2 to the 2 and that's 1.
cs-410_5_2_193,"00:14:32,784","00:14:35,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,And so we now have again 1 at the end.
cs-410_5_2_194,"00:14:35,803","00:14:39,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,"But this time we're going to use 2 bits,"
cs-410_5_2_195,"00:14:39,226","00:14:43,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=879,because with this level
cs-410_5_2_196,"00:14:43,570","00:14:51,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,"We could have more numbers a 5, 6, 7 they"
cs-410_5_2_197,"00:14:51,040","00:14:53,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,"So in order to differentiate them,"
cs-410_5_2_198,"00:14:53,210","00:14:57,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,we have to use 2 bits in
cs-410_5_2_199,"00:14:57,690","00:15:03,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,So you can imagine 6 would be 10 here
cs-410_5_2_200,"00:15:04,710","00:15:10,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,It's also true that the form of
cs-410_5_2_201,"00:15:10,615","00:15:15,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,"odd number of bits, and"
cs-410_5_2_202,"00:15:15,155","00:15:17,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=915,That's the end of the unary code.
cs-410_5_2_203,"00:15:18,335","00:15:24,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,And before that or on the left side
cs-410_5_2_204,"00:15:24,385","00:15:30,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=924,"And on the right side of this 0,"
cs-410_5_2_205,"00:15:32,550","00:15:36,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=932,So how can you decode such code?
cs-410_5_2_206,"00:15:36,540","00:15:39,866",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,Well you again first do unary coding.
cs-410_5_2_207,"00:15:39,866","00:15:45,371",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,"Once you hit 0, you have got the unary"
cs-410_5_2_208,"00:15:45,371","00:15:50,408",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,many bits you have to read further
cs-410_5_2_209,"00:15:50,408","00:15:53,693",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,So this is how you can
cs-410_5_2_210,"00:15:53,693","00:15:57,998",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,There is also a delta code that's
cs-410_5_2_211,"00:15:57,998","00:16:01,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,that you replace the unary
cs-410_5_2_212,"00:16:01,340","00:16:04,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=961,So that's even less
cs-410_5_2_213,"00:16:04,980","00:16:08,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=964,in terms of wording the small integers.
cs-410_5_2_214,"00:16:08,910","00:16:12,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,"So that means, it's okay if you"
cs-410_5_2_215,"00:16:14,100","00:16:15,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,It's okay with delta code.
cs-410_5_2_216,"00:16:16,810","00:16:23,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=976,"It's also fine with the gamma code,"
cs-410_5_2_217,"00:16:23,210","00:16:26,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,"And they are all operating of course,"
cs-410_5_2_218,"00:16:26,710","00:16:32,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,at different degrees of favoring short or
cs-410_5_2_219,"00:16:32,360","00:16:38,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,And that also means they would be
cs-410_5_2_220,"00:16:38,560","00:16:41,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,But none of them is perfect for
cs-410_5_2_221,"00:16:41,720","00:16:45,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,And which method works the best would
cs-410_5_2_222,"00:16:45,990","00:16:47,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,in your dataset.
cs-410_5_2_223,"00:16:47,610","00:16:49,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,"For inverted index compression,"
cs-410_5_2_224,"00:16:49,660","00:16:52,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,people have found that gamma
cs-410_5_2_225,"00:16:55,114","00:16:58,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,So how to uncompress inverted index?
cs-410_5_2_226,"00:16:58,340","00:16:59,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,I will just talk about this.
cs-410_5_2_227,"00:16:59,900","00:17:02,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,"Firstly, you decode"
cs-410_5_2_228,"00:17:02,920","00:17:10,877",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,And we just I think discussed the how we
cs-410_5_2_229,"00:17:10,877","00:17:15,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,What about the document IDs that
cs-410_5_2_230,"00:17:15,720","00:17:19,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"Well, we're going to do"
cs-410_5_2_231,"00:17:19,320","00:17:23,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1039,"supposed the encoded I list is x1,"
cs-410_5_2_232,"00:17:23,800","00:17:28,384",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,We first decode x1 to obtain
cs-410_5_2_233,"00:17:28,384","00:17:29,845",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1048,"Then we can decode x2,"
cs-410_5_2_234,"00:17:29,845","00:17:34,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,which is actually the difference between
cs-410_5_2_235,"00:17:34,610","00:17:40,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,So we have to add the decoder
cs-410_5_2_236,"00:17:40,240","00:17:45,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,the value of the ID at
cs-410_5_2_237,"00:17:46,690","00:17:50,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1066,So this is where you can
cs-410_5_2_238,"00:17:50,420","00:17:52,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1070,converting document IDs to integers.
cs-410_5_2_239,"00:17:52,870","00:17:55,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1072,And that allows us to do
cs-410_5_2_240,"00:17:55,730","00:17:59,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1075,And we just repeat until we
cs-410_5_2_241,"00:17:59,590","00:18:04,004",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,Every time we use the document ID in
cs-410_5_2_242,"00:18:04,004","00:18:06,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1084,the document ID in the next position.
cs-410_5_2_243,"00:18:08,871","00:18:18,871",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,[MUSIC]
cs-410_3_2_1,"00:00:00,012","00:00:03,576",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_2_2,"00:00:08,498","00:00:10,214",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,This lecture is about
cs-410_3_2_3,"00:00:10,214","00:00:14,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,Document Length Normalization
cs-410_3_2_4,"00:00:14,988","00:00:19,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we will continue"
cs-410_3_2_5,"00:00:19,740","00:00:23,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In particular, we're going to discuss the"
cs-410_3_2_6,"00:00:25,850","00:00:30,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,So far in the lectures about the vector
cs-410_3_2_7,"00:00:30,330","00:00:37,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,signals from the document to assess
cs-410_3_2_8,"00:00:37,480","00:00:40,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"In particular,"
cs-410_3_2_9,"00:00:40,000","00:00:42,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,The count of a tone in a document.
cs-410_3_2_10,"00:00:42,750","00:00:48,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,We have also considered it's
cs-410_3_2_11,"00:00:48,055","00:00:50,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"IDF, Inverse Document Frequency."
cs-410_3_2_12,"00:00:50,795","00:00:53,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,But we have not considered
cs-410_3_2_13,"00:00:54,855","00:01:00,899",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"So here I show two example documents,"
cs-410_3_2_14,"00:01:01,910","00:01:05,098",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"D6 on the other hand, has a 5000 words."
cs-410_3_2_15,"00:01:05,098","00:01:08,882",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,If you look at the matching
cs-410_3_2_16,"00:01:08,882","00:01:13,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,"we see that in d6, there are more"
cs-410_3_2_17,"00:01:13,878","00:01:18,958",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"But one might reason that,"
cs-410_3_2_18,"00:01:18,958","00:01:23,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,these query words in a scattered manner.
cs-410_3_2_19,"00:01:24,450","00:01:30,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,"So maybe the topic of d6, is not"
cs-410_3_2_20,"00:01:31,350","00:01:34,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"So, the discussion of the campaign"
cs-410_3_2_21,"00:01:34,980","00:01:38,739",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,may have nothing to do with the managing
cs-410_3_2_22,"00:01:40,810","00:01:44,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"In general,"
cs-410_3_2_23,"00:01:44,600","00:01:47,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,they would have a higher chance for
cs-410_3_2_24,"00:01:47,370","00:01:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"In fact, if you generate a long document"
cs-410_3_2_25,"00:01:54,760","00:01:59,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"a distribution of words, then eventually"
cs-410_3_2_26,"00:02:00,760","00:02:05,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"So in this sense, we should penalize on"
cs-410_3_2_27,"00:02:05,800","00:02:10,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"better chance matching to any query, and"
cs-410_3_2_28,"00:02:12,300","00:02:18,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,We also need to be careful in avoiding
cs-410_3_2_29,"00:02:19,770","00:02:22,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"On the one hand,"
cs-410_3_2_30,"00:02:22,790","00:02:27,202",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"But on the other hand,"
cs-410_3_2_31,"00:02:27,202","00:02:30,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"Now, the reasoning is because"
cs-410_3_2_32,"00:02:30,790","00:02:31,309",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,different reasons.
cs-410_3_2_33,"00:02:32,770","00:02:36,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"In one case, the document may be"
cs-410_3_2_34,"00:02:38,270","00:02:44,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"So for example, think about the vortex"
cs-410_3_2_35,"00:02:44,460","00:02:47,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,It would use more words than
cs-410_3_2_36,"00:02:49,560","00:02:53,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"So, this is a case where we probably"
cs-410_3_2_37,"00:02:54,980","00:02:57,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,long documents such as a full paper.
cs-410_3_2_38,"00:02:57,278","00:03:02,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,When we compare the matching
cs-410_3_2_39,"00:03:02,520","00:03:06,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,document with matching of
cs-410_3_2_40,"00:03:07,830","00:03:10,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"Then long papers in general,"
cs-410_3_2_41,"00:03:10,700","00:03:15,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,have a higher chance of matching clearer
cs-410_3_2_42,"00:03:15,380","00:03:18,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"However, there is another case"
cs-410_3_2_43,"00:03:18,550","00:03:21,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,that is when the document
cs-410_3_2_44,"00:03:21,750","00:03:24,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,Now consider another
cs-410_3_2_45,"00:03:24,040","00:03:29,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,where we simply concatenate a lot
cs-410_3_2_46,"00:03:29,450","00:03:34,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"In such a case, obviously, we don't want"
cs-410_3_2_47,"00:03:34,190","00:03:38,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"Indeed, we probably don't want to penalize"
cs-410_3_2_48,"00:03:39,700","00:03:46,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"So that's why, we need to be careful about"
cs-410_3_2_49,"00:03:48,360","00:03:52,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"A method of that has been working well,"
cs-410_3_2_50,"00:03:52,420","00:03:54,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,is called a pivoted length normalization.
cs-410_3_2_51,"00:03:54,890","00:03:55,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"And in this case,"
cs-410_3_2_52,"00:03:55,860","00:04:01,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,the idea is to use the average document
cs-410_3_2_53,"00:04:01,550","00:04:05,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,That means we'll assume that for
cs-410_3_2_54,"00:04:05,820","00:04:10,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,the score is about right so
cs-410_3_2_55,"00:04:10,335","00:04:13,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,But if the document is longer
cs-410_3_2_56,"00:04:14,125","00:04:16,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,then there will be some penalization.
cs-410_3_2_57,"00:04:16,275","00:04:20,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"Whereas if it's a shorter,"
cs-410_3_2_58,"00:04:20,785","00:04:26,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,So this is illustrated at
cs-410_3_2_59,"00:04:26,050","00:04:28,578",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,x-axis you can see the length of document.
cs-410_3_2_60,"00:04:28,578","00:04:33,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,"On the y-axis, we show the normalizer."
cs-410_3_2_61,"00:04:33,390","00:04:39,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"In this case, the Pivoted Length"
cs-410_3_2_62,"00:04:39,080","00:04:45,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,is seeing to be interpolation of 1 and
cs-410_3_2_63,"00:04:45,850","00:04:50,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,the normalize the document in length
cs-410_3_2_64,"00:04:53,110","00:04:58,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"So you can see here,"
cs-410_3_2_65,"00:04:58,640","00:05:03,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"of the document by the average documents,"
cs-410_3_2_66,"00:05:03,470","00:05:07,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,sense about how this document is
cs-410_3_2_67,"00:05:07,890","00:05:16,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,also gives us a benefit of not
cs-410_3_2_68,"00:05:16,120","00:05:18,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,We can measure the length by words or
cs-410_3_2_69,"00:05:20,760","00:05:24,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"Anyway, this normalizer"
cs-410_3_2_70,"00:05:24,260","00:05:29,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"First we see that, if we set the parameter"
cs-410_3_2_71,"00:05:29,660","00:05:33,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,"So, there's no lens normalization at all."
cs-410_3_2_72,"00:05:33,580","00:05:37,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"So, b, in this sense,"
cs-410_3_2_73,"00:05:39,450","00:05:44,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"Whereas, if we set b to a nonzero value,"
cs-410_3_2_74,"00:05:44,980","00:05:49,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,"All right, so"
cs-410_3_2_75,"00:05:49,010","00:05:52,179",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,documents that are longer than
cs-410_3_2_76,"00:05:53,860","00:05:56,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"Whereas, the value of"
cs-410_3_2_77,"00:05:56,580","00:05:59,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,would be smaller for shorter documents.
cs-410_3_2_78,"00:05:59,460","00:06:02,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"So in this sense,"
cs-410_3_2_79,"00:06:02,720","00:06:07,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"long documents, and"
cs-410_3_2_80,"00:06:09,040","00:06:11,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,The degree of penalization
cs-410_3_2_81,"00:06:11,500","00:06:16,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"because if we set b to a larger value,"
cs-410_3_2_82,"00:06:16,750","00:06:20,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,There's even more penalization for
cs-410_3_2_83,"00:06:20,580","00:06:22,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,the short documents.
cs-410_3_2_84,"00:06:22,380","00:06:25,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"By adjusting b, which varies from 0 to 1,"
cs-410_3_2_85,"00:06:25,440","00:06:29,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,we can control the degree
cs-410_3_2_86,"00:06:29,450","00:06:35,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"So, if we plug in this length"
cs-410_3_2_87,"00:06:35,050","00:06:40,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,"the vector space model, ranking functions"
cs-410_3_2_88,"00:06:41,510","00:06:45,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,Then we will end up having
cs-410_3_2_89,"00:06:46,370","00:06:51,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,And these are in fact the state of
cs-410_3_2_90,"00:06:51,569","00:06:55,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Let's take a look at each of them.
cs-410_3_2_91,"00:06:55,290","00:07:00,972",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,The first one is called a pivoted length
cs-410_3_2_92,"00:07:00,972","00:07:04,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,and a reference in [INAUDIBLE]
cs-410_3_2_93,"00:07:04,980","00:07:11,836",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"And here we see that, it's basically"
cs-410_3_2_94,"00:07:11,836","00:07:16,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,the idea of component should
cs-410_3_2_95,"00:07:18,010","00:07:21,608",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,There is also a query term
cs-410_3_2_96,"00:07:24,628","00:07:30,504",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"And then, in the middle, there is"
cs-410_3_2_97,"00:07:30,504","00:07:35,486",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,we see we use the double logarithm
cs-410_3_2_98,"00:07:35,486","00:07:40,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,this is to achieve
cs-410_3_2_99,"00:07:40,460","00:07:45,488",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,But we also put a document
cs-410_3_2_100,"00:07:45,488","00:07:50,596",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"Right, so this would cause"
cs-410_3_2_101,"00:07:50,596","00:07:56,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"because the larger the denominator is,"
cs-410_3_2_102,"00:07:56,698","00:07:59,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,And this is of course controlled
cs-410_3_2_103,"00:08:01,420","00:08:06,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"And you can see again, if b is set to 0"
cs-410_3_2_104,"00:08:08,760","00:08:16,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Okay, so this is one of the two most"
cs-410_3_2_105,"00:08:16,350","00:08:20,652",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"The next one called a BM25 or Okapi,"
cs-410_3_2_106,"00:08:20,652","00:08:26,971",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,is also similar in that it
cs-410_3_2_107,"00:08:26,971","00:08:30,478",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,and query IDF component here.
cs-410_3_2_108,"00:08:32,958","00:08:36,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,"But in the middle,"
cs-410_3_2_109,"00:08:36,150","00:08:41,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"As we explained,"
cs-410_3_2_110,"00:08:41,450","00:08:46,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,and that does sublinear
cs-410_3_2_111,"00:08:48,340","00:08:53,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,In this case we have put the length
cs-410_3_2_112,"00:08:53,610","00:08:58,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,We're adjusting k but
cs-410_3_2_113,"00:08:58,160","00:09:02,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,because we put a normalizer
cs-410_3_2_114,"00:09:02,610","00:09:08,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"Therefore, again, if a document is longer"
cs-410_3_2_115,"00:09:10,110","00:09:16,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,So you can see after we have gone through
cs-410_3_2_116,"00:09:16,070","00:09:24,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,and we have in the end reached
cs-410_3_2_117,"00:09:24,226","00:09:28,726",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"So, So far, we have talked about"
cs-410_3_2_118,"00:09:28,726","00:09:33,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,mainly how to place the document
cs-410_3_2_119,"00:09:35,010","00:09:39,752",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"And, this has played an important role"
cs-410_3_2_120,"00:09:39,752","00:09:41,169",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,the simple function.
cs-410_3_2_121,"00:09:41,169","00:09:45,728",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"But there are also other dimensions,"
cs-410_3_2_122,"00:09:45,728","00:09:50,343",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"For example, can we further"
cs-410_3_2_123,"00:09:50,343","00:09:53,648",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,the dimension of the Vector Space Model?
cs-410_3_2_124,"00:09:53,648","00:09:57,424",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,"Now, we've just assumed that the bag"
cs-410_3_2_125,"00:09:57,424","00:10:01,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"dimension as a word but obviously,"
cs-410_3_2_126,"00:10:01,240","00:10:07,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"For example, a stemmed word, those"
cs-410_3_2_127,"00:10:07,040","00:10:11,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"into the same root form, so"
cs-410_3_2_128,"00:10:11,110","00:10:16,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,that computation and computing were all
cs-410_3_2_129,"00:10:16,510","00:10:18,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,We get those stop word removal.
cs-410_3_2_130,"00:10:18,740","00:10:25,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,This is to remove some very common words
cs-410_3_2_131,"00:10:26,760","00:10:29,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,We get use of phrases
cs-410_3_2_132,"00:10:29,750","00:10:33,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,We can even use later in
cs-410_3_2_133,"00:10:33,630","00:10:38,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,some clusters of words that represent the
cs-410_3_2_134,"00:10:39,700","00:10:44,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,"We can also use smaller unit,"
cs-410_3_2_135,"00:10:44,080","00:10:48,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,are sequences of and
cs-410_3_2_136,"00:10:50,320","00:10:57,087",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"However, in practice, people have found"
cs-410_3_2_137,"00:10:57,087","00:11:02,148",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,phrases is still the most effective
cs-410_3_2_138,"00:11:02,148","00:11:08,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"So, this is still so far the most"
cs-410_3_2_139,"00:11:10,120","00:11:12,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,And it's used in all major search engines.
cs-410_3_2_140,"00:11:13,960","00:11:18,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,"I should also mention, that sometimes"
cs-410_3_2_141,"00:11:18,910","00:11:21,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,domain specific tokenization.
cs-410_3_2_142,"00:11:21,300","00:11:27,991",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"And this is actually very important, as we"
cs-410_3_2_143,"00:11:27,991","00:11:33,545",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,prevent us from matching them with each
cs-410_3_2_144,"00:11:33,545","00:11:39,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,"In some languages like Chinese,"
cs-410_3_2_145,"00:11:40,860","00:11:47,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,text to obtain word band rates because
cs-410_3_2_146,"00:11:47,290","00:11:51,505",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,A word might correspond to one
cs-410_3_2_147,"00:11:51,505","00:11:53,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,even three characters.
cs-410_3_2_148,"00:11:53,248","00:11:58,164",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,"So, it's easier in English when we"
cs-410_3_2_149,"00:11:58,164","00:12:02,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,"In some other languages, we may need"
cs-410_3_2_150,"00:12:02,590","00:12:05,098",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,figure a way out of what
cs-410_3_2_151,"00:12:05,098","00:12:10,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,There is also the possibility to
cs-410_3_2_152,"00:12:10,850","00:12:13,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,And so
cs-410_3_2_153,"00:12:13,510","00:12:16,137",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,one can imagine there are other measures.
cs-410_3_2_154,"00:12:16,137","00:12:20,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"For example, we can measure the cosine"
cs-410_3_2_155,"00:12:20,550","00:12:23,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,Or we can use Euclidean distance measure.
cs-410_3_2_156,"00:12:24,880","00:12:27,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,"And these are all possible, but"
cs-410_3_2_157,"00:12:27,280","00:12:32,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,dot product seems still the best and
cs-410_3_2_158,"00:12:33,780","00:12:38,143",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"In fact that it's sufficiently general,"
cs-410_3_2_159,"00:12:38,143","00:12:43,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,if you consider the possibilities
cs-410_3_2_160,"00:12:44,280","00:12:45,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"So, for example,"
cs-410_3_2_161,"00:12:45,390","00:12:50,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,cosine measure can be thought of as the
cs-410_3_2_162,"00:12:50,440","00:12:54,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,"That means, we first normalize each factor"
cs-410_3_2_163,"00:12:54,720","00:12:57,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,That would be critical
cs-410_3_2_164,"00:12:57,720","00:13:03,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,"I just mentioned that the BM25, seems to"
cs-410_3_2_165,"00:13:04,930","00:13:09,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,But there has been also further
cs-410_3_2_166,"00:13:09,420","00:13:15,478",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"Although, none of these words have"
cs-410_3_2_167,"00:13:15,478","00:13:20,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,"So in one line work,"
cs-410_3_2_168,"00:13:20,090","00:13:26,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,"Here, F stands for field, and this is"
cs-410_3_2_169,"00:13:26,663","00:13:30,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,"So for example, you might consider"
cs-410_3_2_170,"00:13:30,960","00:13:33,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,or body of the research article.
cs-410_3_2_171,"00:13:33,240","00:13:39,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,"Or even anchor text on the web page,"
cs-410_3_2_172,"00:13:39,800","00:13:44,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,links to other pages and
cs-410_3_2_173,"00:13:44,970","00:13:50,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,a proper way of different fields to help
cs-410_3_2_174,"00:13:50,490","00:13:55,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,When we use BM25 for such a document and
cs-410_3_2_175,"00:13:55,430","00:14:00,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,the obvious choice is to apply BM25 for
cs-410_3_2_176,"00:14:00,750","00:14:06,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,"Basically, the idea of BM25F is"
cs-410_3_2_177,"00:14:06,620","00:14:11,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,"counts of terms in all the fields,"
cs-410_3_2_178,"00:14:11,670","00:14:19,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,"Now, this has advantage of avoiding over"
cs-410_3_2_179,"00:14:19,430","00:14:22,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,Remember in the sublinear
cs-410_3_2_180,"00:14:22,000","00:14:27,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,the first occurrence is very important and
cs-410_3_2_181,"00:14:27,800","00:14:29,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,"And if we do that for all the fields,"
cs-410_3_2_182,"00:14:29,660","00:14:35,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,then the same term might have gained
cs-410_3_2_183,"00:14:35,110","00:14:38,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,But when we combine these
cs-410_3_2_184,"00:14:38,820","00:14:42,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,we just do the transformation one time.
cs-410_3_2_185,"00:14:42,110","00:14:42,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,"At that time,"
cs-410_3_2_186,"00:14:42,870","00:14:47,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,then the extra occurrences will not be
cs-410_3_2_187,"00:14:48,790","00:14:54,039",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=888,And this method has been working very well
cs-410_3_2_188,"00:14:55,810","00:14:59,283",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=895,The other line of extension
cs-410_3_2_189,"00:14:59,283","00:15:03,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,"In this line,"
cs-410_3_2_190,"00:15:03,810","00:15:05,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,over penalization of
cs-410_3_2_191,"00:15:08,880","00:15:13,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,"So to address this problem,"
cs-410_3_2_192,"00:15:13,990","00:15:18,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,We can simply add a small constant
cs-410_3_2_193,"00:15:18,180","00:15:23,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,"But what's interesting is that,"
cs-410_3_2_194,"00:15:23,400","00:15:28,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"doing such a small modification,"
cs-410_3_2_195,"00:15:28,340","00:15:33,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,the problem of over penalization of
cs-410_3_2_196,"00:15:33,570","00:15:36,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,"So the new formula called BM25+,"
cs-410_3_2_197,"00:15:36,380","00:15:40,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,is empirically and
cs-410_3_2_198,"00:15:42,590","00:15:48,432",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,So to summarize all what we have
cs-410_3_2_199,"00:15:48,432","00:15:52,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,here are the major take away points.
cs-410_3_2_200,"00:15:52,100","00:15:57,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,"First, in such a model,"
cs-410_3_2_201,"00:15:57,590","00:16:01,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,Assuming that relevance of a document
cs-410_3_2_202,"00:16:02,820","00:16:08,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,basically proportional to the similarity
cs-410_3_2_203,"00:16:08,030","00:16:10,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,"So naturally,"
cs-410_3_2_204,"00:16:10,640","00:16:13,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=970,document must have been
cs-410_3_2_205,"00:16:13,830","00:16:19,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,"And in this case, we will present them as"
cs-410_3_2_206,"00:16:19,050","00:16:24,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,"Where the dimensions are defined by words,"
cs-410_3_2_207,"00:16:25,470","00:16:29,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,"And we generally, need to use a lot of"
cs-410_3_2_208,"00:16:29,850","00:16:34,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"We use some examples, which show"
cs-410_3_2_209,"00:16:34,560","00:16:37,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=994,including Tf weighting and transformation.
cs-410_3_2_210,"00:16:38,740","00:16:41,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,"And IDF weighting, and"
cs-410_3_2_211,"00:16:41,950","00:16:45,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,These major heuristics are the most
cs-410_3_2_212,"00:16:45,890","00:16:51,544",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,to ensure such a general ranking function
cs-410_3_2_213,"00:16:51,544","00:16:55,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,"And finally, BM25 and"
cs-410_3_2_214,"00:16:55,640","00:16:59,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,to be the most effective formulas
cs-410_3_2_215,"00:16:59,890","00:17:05,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,"Now I have to say that, I put BM25 in"
cs-410_3_2_216,"00:17:05,100","00:17:09,759",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,"in fact, the BM25 has been derived"
cs-410_3_2_217,"00:17:11,970","00:17:17,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,So the reason why I've put it in
cs-410_3_2_218,"00:17:17,470","00:17:22,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1037,the ranking function actually has a nice
cs-410_3_2_219,"00:17:22,540","00:17:23,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1042,"We can easily see,"
cs-410_3_2_220,"00:17:23,450","00:17:27,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,it looks very much like a vector space
cs-410_3_2_221,"00:17:28,890","00:17:34,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1048,The second reason is because the original
cs-410_3_2_222,"00:17:36,070","00:17:39,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1056,And that form of IDF after
cs-410_3_2_223,"00:17:39,420","00:17:44,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1059,well as the standard IDF
cs-410_3_2_224,"00:17:44,630","00:17:47,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,"So as effective retrieval function,"
cs-410_3_2_225,"00:17:47,910","00:17:53,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1067,BM25 should probably use a heuristic
cs-410_3_2_226,"00:17:53,360","00:17:55,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1073,To make them even more look
cs-410_3_2_227,"00:17:59,218","00:18:01,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,There are some additional readings.
cs-410_3_2_228,"00:18:01,460","00:18:05,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1081,"The first is, a paper about"
cs-410_3_2_229,"00:18:05,330","00:18:09,224",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,It's an excellent example
cs-410_3_2_230,"00:18:09,224","00:18:13,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1089,analysis to suggest the need for
cs-410_3_2_231,"00:18:13,650","00:18:17,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1093,then further derive the length
cs-410_3_2_232,"00:18:17,590","00:18:22,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1097,"The second, is the original paper"
cs-410_3_2_233,"00:18:24,180","00:18:28,452",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1104,"The third paper,"
cs-410_3_2_234,"00:18:28,452","00:18:31,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1108,"its extensions, particularly BM25 F."
cs-410_3_2_235,"00:18:32,860","00:18:38,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,"And finally, in the last paper"
cs-410_3_2_236,"00:18:38,305","00:18:43,768",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1118,BM25 to correct the over
cs-410_3_2_237,"00:18:43,768","00:18:53,768",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,[MUSIC]
cs-410_1_2_1,"00:00:00,012","00:00:08,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_2_2,"00:00:08,850","00:00:12,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,"In this lecture, we are going to talk about how"
cs-410_1_2_3,"00:00:12,546","00:00:14,288",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,of the vector space model.
cs-410_1_2_4,"00:00:17,448","00:00:22,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,This is a continued discussion
cs-410_1_2_5,"00:00:22,110","00:00:26,859",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,We're going to focus on how to improve
cs-410_1_2_6,"00:00:30,259","00:00:32,327",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"In the previous lecture,"
cs-410_1_2_7,"00:00:32,327","00:00:38,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,you have seen that with simple
cs-410_1_2_8,"00:00:38,155","00:00:43,889",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,we can come up with a simple scoring
cs-410_1_2_9,"00:00:43,889","00:00:49,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,an account of how many unique query
cs-410_1_2_10,"00:00:50,540","00:00:56,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,We also have seen that this function
cs-410_1_2_11,"00:00:56,862","00:01:00,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"In particular,"
cs-410_1_2_12,"00:01:00,226","00:01:05,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,they will all get the same score because
cs-410_1_2_13,"00:01:06,322","00:01:11,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,But intuitively we would like
cs-410_1_2_14,"00:01:11,330","00:01:13,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,d2 is really not relevant.
cs-410_1_2_15,"00:01:14,750","00:01:22,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,So the problem here is that this function
cs-410_1_2_16,"00:01:22,600","00:01:27,504",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"First, we would like to give"
cs-410_1_2_17,"00:01:27,504","00:01:31,297",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,matched presidential more times than d3.
cs-410_1_2_18,"00:01:32,520","00:01:37,657",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"Second, intuitively, matching presidential"
cs-410_1_2_19,"00:01:37,657","00:01:42,808",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"matching about, because about is a very"
cs-410_1_2_20,"00:01:42,808","00:01:44,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,It doesn't really carry that much content.
cs-410_1_2_21,"00:01:47,480","00:01:48,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"So in this lecture,"
cs-410_1_2_22,"00:01:48,945","00:01:53,868",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,let's see how we can improve the model
cs-410_1_2_23,"00:01:53,868","00:01:59,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,It's worth thinking at this point
cs-410_1_2_24,"00:02:01,420","00:02:06,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,If we look back at assumptions we have
cs-410_1_2_25,"00:02:06,600","00:02:11,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"space model,"
cs-410_1_2_26,"00:02:11,645","00:02:15,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,is really coming from
cs-410_1_2_27,"00:02:15,200","00:02:19,391",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,"In particular, it has to do with how we"
cs-410_1_2_28,"00:02:22,380","00:02:25,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"So then naturally,"
cs-410_1_2_29,"00:02:25,390","00:02:27,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,we have to revisit those assumptions.
cs-410_1_2_30,"00:02:27,780","00:02:34,755",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,Perhaps we will have to use different ways
cs-410_1_2_31,"00:02:34,755","00:02:39,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"In particular, we have to place"
cs-410_1_2_32,"00:02:41,690","00:02:45,708",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,So let's see how we can improve this.
cs-410_1_2_33,"00:02:45,708","00:02:50,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,One natural thought is in order to
cs-410_1_2_34,"00:02:50,248","00:02:51,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,"the document,"
cs-410_1_2_35,"00:02:51,266","00:02:57,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,we should consider the term frequency
cs-410_1_2_36,"00:02:57,270","00:03:02,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,In order to consider the difference
cs-410_1_2_37,"00:03:02,900","00:03:07,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,term occurred multiple times and one
cs-410_1_2_38,"00:03:07,620","00:03:12,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"we have to consider the term frequency,"
cs-410_1_2_39,"00:03:13,130","00:03:18,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"In the simplest model, we only modeled"
cs-410_1_2_40,"00:03:18,200","00:03:25,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,We ignored the actual number of times
cs-410_1_2_41,"00:03:25,106","00:03:26,566",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So let's add this back.
cs-410_1_2_42,"00:03:26,566","00:03:30,592",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,So we're going to then
cs-410_1_2_43,"00:03:30,592","00:03:34,214",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,a vector with term frequency as element.
cs-410_1_2_44,"00:03:34,214","00:03:39,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"So that is to say, now the elements"
cs-410_1_2_45,"00:03:39,573","00:03:43,489",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,the document vector will not be 0 or
cs-410_1_2_46,"00:03:43,489","00:03:49,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,instead they will be the counts of
cs-410_1_2_47,"00:03:52,140","00:03:55,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,So this would bring in additional
cs-410_1_2_48,"00:03:55,340","00:04:00,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,this can be seen as more accurate
cs-410_1_2_49,"00:04:00,650","00:04:03,849",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,So now let's see what the formula
cs-410_1_2_50,"00:04:03,849","00:04:05,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,representation.
cs-410_1_2_51,"00:04:05,480","00:04:08,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"So as you'll see on this slide,"
cs-410_1_2_52,"00:04:10,090","00:04:14,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,And so the formula looks
cs-410_1_2_53,"00:04:14,270","00:04:16,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"In fact, it looks identical."
cs-410_1_2_54,"00:04:16,310","00:04:21,178",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"But inside the sum, of course,"
cs-410_1_2_55,"00:04:21,178","00:04:25,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,They are now the counts of word i in
cs-410_1_2_56,"00:04:25,855","00:04:30,208",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,the query and in the document.
cs-410_1_2_57,"00:04:30,208","00:04:35,931",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,Now at this point I also suggest you
cs-410_1_2_58,"00:04:35,931","00:04:41,756",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,just to think about how we can interpret
cs-410_1_2_59,"00:04:41,756","00:04:47,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,It's doing something very similar
cs-410_1_2_60,"00:04:47,710","00:04:50,501",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"But because of the change of the vector,"
cs-410_1_2_61,"00:04:50,501","00:04:54,038",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,now the new score has
cs-410_1_2_62,"00:04:54,038","00:04:56,118",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,Can you see the difference?
cs-410_1_2_63,"00:04:56,118","00:05:00,995",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,And it has to do with the consideration
cs-410_1_2_64,"00:05:00,995","00:05:03,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,the same term in a document.
cs-410_1_2_65,"00:05:03,360","00:05:06,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,"More importantly, we would like to know"
cs-410_1_2_66,"00:05:06,590","00:05:08,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,of the simplest vector space model.
cs-410_1_2_67,"00:05:08,830","00:05:12,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,So let's look at this example again.
cs-410_1_2_68,"00:05:12,320","00:05:16,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So suppose we change the vector
cs-410_1_2_69,"00:05:16,670","00:05:20,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,Now let's look at these
cs-410_1_2_70,"00:05:20,620","00:05:24,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,The query vector is the same
cs-410_1_2_71,"00:05:24,580","00:05:27,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,exactly once in the query.
cs-410_1_2_72,"00:05:27,240","00:05:30,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,So the vector is still a 01 vector.
cs-410_1_2_73,"00:05:30,988","00:05:35,472",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"And in fact, d2 is also essentially"
cs-410_1_2_74,"00:05:35,472","00:05:40,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,because none of these words
cs-410_1_2_75,"00:05:40,120","00:05:43,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"As a result,"
cs-410_1_2_76,"00:05:45,410","00:05:49,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"The same is true for d3,"
cs-410_1_2_77,"00:05:51,510","00:05:57,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"But d4 would be different, because"
cs-410_1_2_78,"00:05:57,400","00:06:02,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,So the ending for presidential in the
cs-410_1_2_79,"00:06:04,240","00:06:08,303",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"As a result, now the score for"
cs-410_1_2_80,"00:06:08,303","00:06:09,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,It's a 4 now.
cs-410_1_2_81,"00:06:10,130","00:06:13,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"So this means by using term frequency,"
cs-410_1_2_82,"00:06:13,380","00:06:17,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,we can now rank d4 above d2 and
cs-410_1_2_83,"00:06:19,250","00:06:23,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,So this solved the problem with d4.
cs-410_1_2_84,"00:06:26,190","00:06:32,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,But we can also see that d2 and
cs-410_1_2_85,"00:06:32,548","00:06:38,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"They still have identical scores,"
cs-410_1_2_86,"00:06:40,420","00:06:42,434",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,So how can we fix this problem?
cs-410_1_2_87,"00:06:42,434","00:06:46,261",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"Intuitively, we would like"
cs-410_1_2_88,"00:06:46,261","00:06:49,736",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,matching presidential than matching about.
cs-410_1_2_89,"00:06:49,736","00:06:53,028",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,But how can we solve
cs-410_1_2_90,"00:06:53,028","00:06:57,651",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,Is there any way to determine
cs-410_1_2_91,"00:06:57,651","00:07:02,478",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,more importantly and
cs-410_1_2_92,"00:07:02,478","00:07:09,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,About is such a word which does not
cs-410_1_2_93,"00:07:09,670","00:07:11,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,We can essentially ignore that.
cs-410_1_2_94,"00:07:11,760","00:07:15,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,We sometimes call such
cs-410_1_2_95,"00:07:15,110","00:07:18,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,Those are generally very frequent and
cs-410_1_2_96,"00:07:18,710","00:07:21,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,Matching it doesn't really mean anything.
cs-410_1_2_97,"00:07:21,570","00:07:23,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,But computationally how
cs-410_1_2_98,"00:07:24,960","00:07:27,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"So again, I encourage you to"
cs-410_1_2_99,"00:07:29,460","00:07:33,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,Can you came up with any statistical
cs-410_1_2_100,"00:07:33,000","00:07:34,358",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,presidential from about?
cs-410_1_2_101,"00:07:37,109","00:07:39,691",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"Now if you think about it for a moment,"
cs-410_1_2_102,"00:07:39,691","00:07:46,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,you'll realize that one difference is
cs-410_1_2_103,"00:07:46,170","00:07:50,764",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,So if you count the occurrence of
cs-410_1_2_104,"00:07:50,764","00:07:55,852",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,then we will see that about has much
cs-410_1_2_105,"00:07:55,852","00:07:58,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,which tends to occur
cs-410_1_2_106,"00:08:01,000","00:08:05,887",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So this idea suggests
cs-410_1_2_107,"00:08:05,887","00:08:09,396",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,the global statistics of terms or
cs-410_1_2_108,"00:08:09,396","00:08:14,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,some other information
cs-410_1_2_109,"00:08:14,660","00:08:20,568",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,the element of about in
cs-410_1_2_110,"00:08:20,568","00:08:24,754",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"At the same time,"
cs-410_1_2_111,"00:08:24,754","00:08:29,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,the weight of presidential
cs-410_1_2_112,"00:08:29,278","00:08:34,284",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"If we can do that, then we can"
cs-410_1_2_113,"00:08:34,284","00:08:39,036",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,score to be less than 3 while
cs-410_1_2_114,"00:08:39,036","00:08:42,996",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,Then we would be able to
cs-410_1_2_115,"00:08:45,138","00:08:47,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,So how can we do this systematically?
cs-410_1_2_116,"00:08:48,730","00:08:52,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"Again, we can rely on"
cs-410_1_2_117,"00:08:52,030","00:08:57,218",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"And in this case, the particular idea"
cs-410_1_2_118,"00:08:57,218","00:09:01,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,Now we have seen document
cs-410_1_2_119,"00:09:01,425","00:09:04,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,the modern retrieval functions.
cs-410_1_2_120,"00:09:05,800","00:09:08,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,We discussed this in a previous lecture.
cs-410_1_2_121,"00:09:08,500","00:09:10,859",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,So here is the specific way of using it.
cs-410_1_2_122,"00:09:10,859","00:09:15,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,Document frequency is the count of
cs-410_1_2_123,"00:09:15,910","00:09:21,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Here we say inverse document frequency
cs-410_1_2_124,"00:09:21,000","00:09:22,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,that doesn't occur in many documents.
cs-410_1_2_125,"00:09:24,890","00:09:30,544",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,And so the way to incorporate this
cs-410_1_2_126,"00:09:30,544","00:09:35,477",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,is then to modify the frequency
cs-410_1_2_127,"00:09:35,477","00:09:39,918",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"the IDF of the corresponding word,"
cs-410_1_2_128,"00:09:39,918","00:09:46,044",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"If we can do that,"
cs-410_1_2_129,"00:09:46,044","00:09:50,401",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"which generally have a lower IDF, and"
cs-410_1_2_130,"00:09:50,401","00:09:56,138",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"reward rare words,"
cs-410_1_2_131,"00:09:56,138","00:09:58,078",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"So more specifically,"
cs-410_1_2_132,"00:09:58,078","00:10:03,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,the IDF can be defined as
cs-410_1_2_133,"00:10:03,025","00:10:08,845",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,where M is the total number of documents
cs-410_1_2_134,"00:10:08,845","00:10:15,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"document frequency, the total number"
cs-410_1_2_135,"00:10:15,058","00:10:18,596",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Now if you plot this
cs-410_1_2_136,"00:10:18,596","00:10:23,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,then you would see the curve
cs-410_1_2_137,"00:10:23,430","00:10:28,273",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"In general, you can see it"
cs-410_1_2_138,"00:10:28,273","00:10:30,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"a low DF word, a rare word."
cs-410_1_2_139,"00:10:34,220","00:10:38,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,You can also see the maximum value
cs-410_1_2_140,"00:10:40,952","00:10:45,158",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,It would be interesting for you to think
cs-410_1_2_141,"00:10:45,158","00:10:46,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,this function.
cs-410_1_2_142,"00:10:46,900","00:10:48,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,This could be an interesting exercise.
cs-410_1_2_143,"00:10:50,918","00:10:55,238",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,Now the specific function
cs-410_1_2_144,"00:10:55,238","00:10:59,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,the heuristic to simply
cs-410_1_2_145,"00:11:01,528","00:11:05,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,But it turns out that this particular
cs-410_1_2_146,"00:11:07,340","00:11:12,221",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,Now whether there's a better
cs-410_1_2_147,"00:11:12,221","00:11:14,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,the open research question.
cs-410_1_2_148,"00:11:14,939","00:11:19,665",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,But it's also clear that if
cs-410_1_2_149,"00:11:19,665","00:11:22,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"like what's shown here with this line,"
cs-410_1_2_150,"00:11:22,945","00:11:27,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,then it may not be as
cs-410_1_2_151,"00:11:29,110","00:11:34,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,"In particular, you can see"
cs-410_1_2_152,"00:11:35,940","00:11:39,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,and we somehow have
cs-410_1_2_153,"00:11:41,110","00:11:45,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,"After this point, we're going to say these"
cs-410_1_2_154,"00:11:45,770","00:11:48,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,They can be essentially ignored.
cs-410_1_2_155,"00:11:48,180","00:11:52,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,And this makes sense when
cs-410_1_2_156,"00:11:52,110","00:11:57,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,let's say a term occurs in more
cs-410_1_2_157,"00:11:57,310","00:12:01,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,then the term is unlikely very important
cs-410_1_2_158,"00:12:03,120","00:12:05,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,It's not very important
cs-410_1_2_159,"00:12:05,150","00:12:10,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,So with the standard IDF you can
cs-410_1_2_160,"00:12:10,145","00:12:12,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,they all have low weights.
cs-410_1_2_161,"00:12:12,285","00:12:14,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,There's no difference.
cs-410_1_2_162,"00:12:14,020","00:12:16,413",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,But if you look at
cs-410_1_2_163,"00:12:16,413","00:12:19,206",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,at this point that there
cs-410_1_2_164,"00:12:19,206","00:12:26,123",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,So intuitively we'd want to
cs-410_1_2_165,"00:12:26,123","00:12:31,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,of low DF words rather
cs-410_1_2_166,"00:12:32,990","00:12:37,972",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,"Well, of course,"
cs-410_1_2_167,"00:12:37,972","00:12:43,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,validated by using the empirically
cs-410_1_2_168,"00:12:43,168","00:12:46,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,And we have to use users to
cs-410_1_2_169,"00:12:48,580","00:12:52,948",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,So now let's see how
cs-410_1_2_170,"00:12:52,948","00:12:55,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,So now let's look at
cs-410_1_2_171,"00:12:56,100","00:13:00,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,"Now without the IDF weighting before,"
cs-410_1_2_172,"00:13:00,530","00:13:05,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,But with IDF weighting we
cs-410_1_2_173,"00:13:05,810","00:13:09,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,by multiplying with the IDF value.
cs-410_1_2_174,"00:13:09,520","00:13:14,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"For example,"
cs-410_1_2_175,"00:13:14,150","00:13:19,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,in particular for about there's adjustment
cs-410_1_2_176,"00:13:19,680","00:13:23,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,which is smaller than the IDF
cs-410_1_2_177,"00:13:23,980","00:13:28,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,"So if you look at these,"
cs-410_1_2_178,"00:13:28,930","00:13:34,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=808,"As a result, adjustment here would be"
cs-410_1_2_179,"00:13:37,190","00:13:44,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"So if we score with these new vectors,"
cs-410_1_2_180,"00:13:44,035","00:13:48,752",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,"of course,"
cs-410_1_2_181,"00:13:48,752","00:13:54,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,"campaign, but the matching of"
cs-410_1_2_182,"00:13:54,830","00:14:01,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,"So now as a result of IDF weighting,"
cs-410_1_2_183,"00:14:01,250","00:14:06,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,"because it matched a rare word,"
cs-410_1_2_184,"00:14:06,460","00:14:10,156",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,So this shows that the IDF
cs-410_1_2_185,"00:14:12,798","00:14:19,434",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,So how effective is this model in
cs-410_1_2_186,"00:14:19,434","00:14:23,438",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"Well, let's look at all these"
cs-410_1_2_187,"00:14:23,438","00:14:28,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,These are the new scores
cs-410_1_2_188,"00:14:28,100","00:14:32,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=868,But how effective is this new weighting
cs-410_1_2_189,"00:14:33,770","00:14:38,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,So now let's see overall how effective
cs-410_1_2_190,"00:14:38,520","00:14:39,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,with TF-IDF weighting.
cs-410_1_2_191,"00:14:40,630","00:14:44,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,Here we show all the five documents
cs-410_1_2_192,"00:14:44,330","00:14:45,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,these are their scores.
cs-410_1_2_193,"00:14:47,000","00:14:49,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,Now we can see the scores for
cs-410_1_2_194,"00:14:49,760","00:14:56,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,the first four documents here
cs-410_1_2_195,"00:14:56,410","00:14:57,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=896,They are as we expected.
cs-410_1_2_196,"00:14:58,740","00:15:05,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,"However, we also see a new"
cs-410_1_2_197,"00:15:05,710","00:15:10,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=905,which did not have a very high score
cs-410_1_2_198,"00:15:10,490","00:15:13,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,now actually has a very high score.
cs-410_1_2_199,"00:15:13,270","00:15:15,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,"In fact, it has the highest score here."
cs-410_1_2_200,"00:15:16,850","00:15:19,002",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,So this creates a new problem.
cs-410_1_2_201,"00:15:19,002","00:15:23,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,This is actually a common phenomenon
cs-410_1_2_202,"00:15:23,080","00:15:25,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"Basically, when you try"
cs-410_1_2_203,"00:15:25,570","00:15:27,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,you tend to introduce other problems.
cs-410_1_2_204,"00:15:27,960","00:15:32,674",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And that's why it's very tricky how
cs-410_1_2_205,"00:15:32,674","00:15:39,658",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=932,And what's the best ranking function
cs-410_1_2_206,"00:15:39,658","00:15:41,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,Researchers are still working on that.
cs-410_1_2_207,"00:15:42,360","00:15:47,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,But in the next few lectures we're going
cs-410_1_2_208,"00:15:47,530","00:15:53,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=947,ideas to further improve this model and
cs-410_1_2_209,"00:15:55,920","00:16:00,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=955,"So to summarize this lecture, we've talked"
cs-410_1_2_210,"00:16:00,740","00:16:04,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,"model, and"
cs-410_1_2_211,"00:16:04,340","00:16:08,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=964,the vector space model
cs-410_1_2_212,"00:16:08,470","00:16:13,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,So the improvement is mostly on
cs-410_1_2_213,"00:16:13,573","00:16:18,673",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,give high weight to a term that
cs-410_1_2_214,"00:16:18,673","00:16:21,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,infrequently in the whole collection.
cs-410_1_2_215,"00:16:23,630","00:16:26,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,And we have seen that this
cs-410_1_2_216,"00:16:26,210","00:16:29,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,looks better than the simplest
cs-410_1_2_217,"00:16:29,440","00:16:33,268",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,But it also still has some problems.
cs-410_1_2_218,"00:16:33,268","00:16:40,448",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,In the next lecture we're going to look at
cs-410_1_2_219,"00:16:40,448","00:16:50,448",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,[MUSIC]
cs-410_2_2_1,"00:00:00,000","00:00:05,293",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_2_2_2,"00:00:10,067","00:00:15,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,"In this lecture, we continue"
cs-410_2_2_3,"00:00:15,310","00:00:18,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"In particular, we're going to"
cs-410_2_2_4,"00:00:18,810","00:00:20,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In the previous lecture,"
cs-410_2_2_5,"00:00:20,100","00:00:25,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,we have derived a TF idea of weighting
cs-410_2_2_6,"00:00:27,100","00:00:31,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,And we have assumed that this model
cs-410_2_2_7,"00:00:31,760","00:00:37,302",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"these examples as shown on this slide,"
cs-410_2_2_8,"00:00:37,302","00:00:41,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"d5, which has received a very high score."
cs-410_2_2_9,"00:00:41,340","00:00:46,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"Indeed, it has received the highest"
cs-410_2_2_10,"00:00:46,510","00:00:51,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,But this document is intuitive and
cs-410_2_2_11,"00:00:53,240","00:00:55,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"In this lecture,"
cs-410_2_2_12,"00:00:55,390","00:00:58,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,how we're going to use TF
cs-410_2_2_13,"00:01:00,410","00:01:04,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"Before we discuss the details,"
cs-410_2_2_14,"00:01:04,870","00:01:08,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,this simple TF-IDF
cs-410_2_2_15,"00:01:08,820","00:01:13,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,And see why this document has
cs-410_2_2_16,"00:01:13,520","00:01:17,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"So this is the formula, and"
cs-410_2_2_17,"00:01:17,510","00:01:21,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,then you will see it involves a sum
cs-410_2_2_18,"00:01:23,810","00:01:28,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"And inside the sum, each matched"
cs-410_2_2_19,"00:01:28,140","00:01:30,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,And this weight is TF-IDF weighting.
cs-410_2_2_20,"00:01:31,580","00:01:36,853",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"So it has an idea of component,"
cs-410_2_2_21,"00:01:36,853","00:01:42,586",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,One is the total number of documents
cs-410_2_2_22,"00:01:42,586","00:01:45,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,The other is the document of frequency.
cs-410_2_2_23,"00:01:45,890","00:01:48,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,This is the number of
cs-410_2_2_24,"00:01:48,220","00:01:49,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,This word w.
cs-410_2_2_25,"00:01:49,070","00:01:53,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,The other variables
cs-410_2_2_26,"00:01:53,810","00:01:58,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,involved in the formula include
cs-410_2_2_27,"00:02:01,440","00:02:06,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"W in the query, and"
cs-410_2_2_28,"00:02:07,650","00:02:12,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"If you look at this document again,"
cs-410_2_2_29,"00:02:12,150","00:02:16,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,the reason why it hasn't
cs-410_2_2_30,"00:02:16,710","00:02:21,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,it has a very high count of campaign.
cs-410_2_2_31,"00:02:21,035","00:02:27,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,So the count of campaign in this document
cs-410_2_2_32,"00:02:27,170","00:02:31,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"the other documents, and has contributed"
cs-410_2_2_33,"00:02:31,580","00:02:35,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,So in treating the amount
cs-410_2_2_34,"00:02:35,485","00:02:40,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"this document, we need to somehow"
cs-410_2_2_35,"00:02:40,695","00:02:44,514",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,of the matching of this
cs-410_2_2_36,"00:02:44,514","00:02:49,934",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,And if you think about the matching
cs-410_2_2_37,"00:02:49,934","00:02:52,193",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"you actually would realize,"
cs-410_2_2_38,"00:02:52,193","00:02:57,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,we probably shouldn't reward
cs-410_2_2_39,"00:02:57,540","00:03:02,406",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"And by that I mean,"
cs-410_2_2_40,"00:03:02,406","00:03:06,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,says a lot about
cs-410_2_2_41,"00:03:06,680","00:03:11,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,because it goes from zero
cs-410_2_2_42,"00:03:11,570","00:03:15,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,And that increase means a lot.
cs-410_2_2_43,"00:03:17,160","00:03:19,277",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"Once we see a word in the document,"
cs-410_2_2_44,"00:03:19,277","00:03:23,219",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,it's very likely that the document
cs-410_2_2_45,"00:03:23,219","00:03:27,934",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,If we see a extra occurrence on
cs-410_2_2_46,"00:03:27,934","00:03:33,493",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"that is to go from one to two,"
cs-410_2_2_47,"00:03:33,493","00:03:39,844",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,occurrence kind of confirmed that it's
cs-410_2_2_48,"00:03:39,844","00:03:44,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,Now we are more sure that this
cs-410_2_2_49,"00:03:44,220","00:03:50,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"But imagine we have seen, let's say,"
cs-410_2_2_50,"00:03:50,430","00:03:56,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"Now, adding one extra occurrence is not"
cs-410_2_2_51,"00:03:56,140","00:03:59,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,because we're already sure that
cs-410_2_2_52,"00:04:01,160","00:04:06,656",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"So if you're thinking this way, it seems"
cs-410_2_2_53,"00:04:06,656","00:04:12,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"of a high count of a term, and"
cs-410_2_2_54,"00:04:12,785","00:04:17,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,So this transformation function is
cs-410_2_2_55,"00:04:17,965","00:04:22,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,word into a term frequency weight for
cs-410_2_2_56,"00:04:22,990","00:04:28,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"So here I show in x axis that we'll count,"
cs-410_2_2_57,"00:04:28,420","00:04:31,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,y axis I show the term frequency weight.
cs-410_2_2_58,"00:04:33,360","00:04:36,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"So in the previous breaking functions,"
cs-410_2_2_59,"00:04:36,370","00:04:41,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,we actually have imprison rate
cs-410_2_2_60,"00:04:41,140","00:04:43,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"So for example,"
cs-410_2_2_61,"00:04:44,960","00:04:49,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,we actually use such a transformation
cs-410_2_2_62,"00:04:49,070","00:04:53,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"Basically if the count is 0,"
cs-410_2_2_63,"00:04:53,420","00:04:56,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,otherwise it would have a weight of 1.
cs-410_2_2_64,"00:04:56,790","00:04:57,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,It's flat.
cs-410_2_2_65,"00:04:59,550","00:05:04,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"Now, what about using"
cs-410_2_2_66,"00:05:04,870","00:05:10,515",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"Well, that's a linear function, so it has"
cs-410_2_2_67,"00:05:11,575","00:05:16,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,Now we have just seen that
cs-410_2_2_68,"00:05:18,395","00:05:20,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,So what we want is something like this.
cs-410_2_2_69,"00:05:20,695","00:05:23,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"So for example,"
cs-410_2_2_70,"00:05:23,160","00:05:26,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,we can't have a sublinear
cs-410_2_2_71,"00:05:26,620","00:05:29,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,And this will control the influence
cs-410_2_2_72,"00:05:29,850","00:05:32,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,because it's going to lower its inference.
cs-410_2_2_73,"00:05:32,270","00:05:35,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"Yet, it will retain"
cs-410_2_2_74,"00:05:36,110","00:05:41,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,Or we might want to even bend the curve
cs-410_2_2_75,"00:05:42,730","00:05:45,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Now people have tried all these methods.
cs-410_2_2_76,"00:05:45,320","00:05:48,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,And they are indeed working better than
cs-410_2_2_77,"00:05:50,230","00:05:54,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"But so far, what works the best seems"
cs-410_2_2_78,"00:05:54,820","00:05:56,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,called a BM25 transformation.
cs-410_2_2_79,"00:05:58,070","00:05:59,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,BM stands for best matching.
cs-410_2_2_80,"00:06:01,210","00:06:04,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"Now in this transformation,"
cs-410_2_2_81,"00:06:06,460","00:06:10,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,And this k controls the upper
cs-410_2_2_82,"00:06:10,910","00:06:15,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,It's easy to see this
cs-410_2_2_83,"00:06:15,165","00:06:21,748",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,because if you look at the x divided by
cs-410_2_2_84,"00:06:21,748","00:06:28,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,then the numerator will never be able
cs-410_2_2_85,"00:06:28,060","00:06:29,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,So it's upper bounded by k+1.
cs-410_2_2_86,"00:06:29,820","00:06:34,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"Now, this is also difference between"
cs-410_2_2_87,"00:06:34,540","00:06:35,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,a logarithm transformation.
cs-410_2_2_88,"00:06:37,010","00:06:38,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,Which it doesn't have upper bound.
cs-410_2_2_89,"00:06:39,830","00:06:44,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"Furthermore, one interesting property"
cs-410_2_2_90,"00:06:45,610","00:06:50,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,we can actually simulate different
cs-410_2_2_91,"00:06:50,310","00:06:52,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,Including the two extremes
cs-410_2_2_92,"00:06:52,900","00:06:57,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"That is, the 0/1 bit transformation and"
cs-410_2_2_93,"00:06:57,480","00:07:01,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,"So for example, if we set k to 0,"
cs-410_2_2_94,"00:07:03,630","00:07:06,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,the function value will be 1.
cs-410_2_2_95,"00:07:06,710","00:07:13,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,So we precisely recover
cs-410_2_2_96,"00:07:15,630","00:07:20,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,If you set k to very large
cs-410_2_2_97,"00:07:20,040","00:07:22,919",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,it's going to look more like
cs-410_2_2_98,"00:07:24,980","00:07:29,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"So in this sense,"
cs-410_2_2_99,"00:07:29,400","00:07:34,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,It allows us to control
cs-410_2_2_100,"00:07:34,600","00:07:36,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,It also has a nice property
cs-410_2_2_101,"00:07:38,020","00:07:42,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,And this upper bound is useful to control
cs-410_2_2_102,"00:07:43,860","00:07:49,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,And so that we can prevent a spammer
cs-410_2_2_103,"00:07:49,718","00:07:54,947",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,of one term to spam all queries
cs-410_2_2_104,"00:07:57,258","00:08:00,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"In other words, this upper bound"
cs-410_2_2_105,"00:08:00,824","00:08:05,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,terms would be counted when we aggregate
cs-410_2_2_106,"00:08:06,680","00:08:10,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"As I said, this transformation"
cs-410_2_2_107,"00:08:12,300","00:08:16,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"So to summarize this lecture,"
cs-410_2_2_108,"00:08:16,890","00:08:21,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"Sublinear TF Transformation,"
cs-410_2_2_109,"00:08:21,930","00:08:25,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,capture the intuition of diminishing
cs-410_2_2_110,"00:08:26,620","00:08:30,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,It's also to avoid the dominance by
cs-410_2_2_111,"00:08:30,980","00:08:37,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,This BM25 transformation that we
cs-410_2_2_112,"00:08:37,050","00:08:43,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,It's so far one of the best-performing
cs-410_2_2_113,"00:08:43,130","00:08:46,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,"It has upper bound, and so"
cs-410_2_2_114,"00:08:47,830","00:08:54,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,Now if we're plugging this function into
cs-410_2_2_115,"00:08:54,080","00:08:57,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,Then we'd end up having
cs-410_2_2_116,"00:08:57,730","00:09:00,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,which has a BM25 TF component.
cs-410_2_2_117,"00:09:01,870","00:09:06,833",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"Now, this is already"
cs-410_2_2_118,"00:09:06,833","00:09:11,537",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,the odd ranking function called BM25.
cs-410_2_2_119,"00:09:11,537","00:09:17,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And we'll discuss how we can further
cs-410_2_2_120,"00:09:17,890","00:09:27,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,[MUSIC]
cs-410_3_1_1,"00:00:00,168","00:00:07,728",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_3_1_2,"00:00:07,728","00:00:10,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_3_1_3,"00:00:12,820","00:00:15,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,This picture shows our overall plan for
cs-410_3_1_4,"00:00:16,780","00:00:21,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In the last lecture, we talked about"
cs-410_3_1_5,"00:00:21,780","00:00:24,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,We talked about push versus pull.
cs-410_3_1_6,"00:00:25,350","00:00:30,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,Such engines are the main tools for
cs-410_3_1_7,"00:00:30,720","00:00:32,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"Starting from this lecture,"
cs-410_3_1_8,"00:00:32,690","00:00:36,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,we're going to talk about the how
cs-410_3_1_9,"00:00:38,110","00:00:40,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,So first it's about
cs-410_3_1_10,"00:00:42,660","00:00:46,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,We're going to talk about
cs-410_3_1_11,"00:00:46,120","00:00:49,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,"First, we define Text Retrieval."
cs-410_3_1_12,"00:00:49,650","00:00:54,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,Second we're going to make a comparison
cs-410_3_1_13,"00:00:54,200","00:00:56,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,the related task Database Retrieval.
cs-410_3_1_14,"00:00:58,240","00:01:02,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"Finally, we're going to talk about"
cs-410_3_1_15,"00:01:02,190","00:01:06,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,Document Ranking as two strategies for
cs-410_3_1_16,"00:01:09,728","00:01:11,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,So what is Text Retrieval?
cs-410_3_1_17,"00:01:12,850","00:01:14,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,It should be a task that's familiar for
cs-410_3_1_18,"00:01:14,840","00:01:18,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,the most of us because we're using
cs-410_3_1_19,"00:01:19,920","00:01:24,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,So text retrieval is basically a task
cs-410_3_1_20,"00:01:24,190","00:01:29,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,where the system would respond to
cs-410_3_1_21,"00:01:29,900","00:01:31,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"Basically, it's for supporting a query"
cs-410_3_1_22,"00:01:32,730","00:01:37,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,as one way to implement the poll
cs-410_3_1_23,"00:01:39,250","00:01:40,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,So the situation is the following.
cs-410_3_1_24,"00:01:40,940","00:01:43,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,You have a collection of
cs-410_3_1_25,"00:01:43,590","00:01:47,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,These documents could be all
cs-410_3_1_26,"00:01:47,364","00:01:50,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,all the literature articles
cs-410_3_1_27,"00:01:50,988","00:01:56,528",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,Or maybe all the text
cs-410_3_1_28,"00:01:58,528","00:02:04,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,A user will typically give a query to
cs-410_3_1_29,"00:02:04,340","00:02:09,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"And then, the system would return"
cs-410_3_1_30,"00:02:09,480","00:02:14,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,Relevant documents refer to those
cs-410_3_1_31,"00:02:14,040","00:02:15,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,the user who typed in the query.
cs-410_3_1_32,"00:02:16,910","00:02:19,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,All this task is a phone call
cs-410_3_1_33,"00:02:21,170","00:02:25,585",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,But literally information retrieval would
cs-410_3_1_34,"00:02:25,585","00:02:30,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"non-textual information as well,"
cs-410_3_1_35,"00:02:30,660","00:02:35,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,It's worth noting that
cs-410_3_1_36,"00:02:35,960","00:02:41,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,of information retrieval in
cs-410_3_1_37,"00:02:41,610","00:02:47,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,video can be retrieved by
cs-410_3_1_38,"00:02:47,010","00:02:52,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"So for example,"
cs-410_3_1_39,"00:02:52,270","00:02:57,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,match a user's query was
cs-410_3_1_40,"00:02:59,850","00:03:03,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,This problem is also
cs-410_3_1_41,"00:03:05,550","00:03:08,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And the technology is often called
cs-410_3_1_42,"00:03:11,190","00:03:14,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,If you ever take a course in databases it
cs-410_3_1_43,"00:03:14,540","00:03:18,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,will be useful to pause
cs-410_3_1_44,"00:03:18,400","00:03:25,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,think about the differences between
cs-410_3_1_45,"00:03:25,200","00:03:28,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Now these two tasks
cs-410_3_1_46,"00:03:29,530","00:03:31,928",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"But, there are some important differences."
cs-410_3_1_47,"00:03:33,708","00:03:38,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"So, spend a moment to think about"
cs-410_3_1_48,"00:03:38,140","00:03:43,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Think about the data, and the information"
cs-410_3_1_49,"00:03:43,300","00:03:46,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,those that are managed
cs-410_3_1_50,"00:03:47,350","00:03:51,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,Think about the different between
cs-410_3_1_51,"00:03:51,570","00:03:57,389",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,database system versus queries that
cs-410_3_1_52,"00:03:59,180","00:04:00,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,And then finally think about the answers.
cs-410_3_1_53,"00:04:02,870","00:04:06,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,What's the difference between the two?
cs-410_3_1_54,"00:04:06,980","00:04:11,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"Okay, so if we think about the information"
cs-410_3_1_55,"00:04:11,760","00:04:14,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,we will see that in text retrieval.
cs-410_3_1_56,"00:04:14,890","00:04:18,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"The data is unstructured, it's free text."
cs-410_3_1_57,"00:04:18,100","00:04:24,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,"But in databases, they are structured data"
cs-410_3_1_58,"00:04:24,020","00:04:30,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,to tell you this column is the names
cs-410_3_1_59,"00:04:31,880","00:04:35,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,The unstructured text is not obvious
cs-410_3_1_60,"00:04:35,020","00:04:38,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,what are the names of people
cs-410_3_1_61,"00:04:40,440","00:04:45,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"Because of this difference, we also see"
cs-410_3_1_62,"00:04:45,930","00:04:52,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,ambiguous and we talk about that in the
cs-410_3_1_63,"00:04:52,900","00:04:55,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,But they don't tend to have
cs-410_3_1_64,"00:04:58,230","00:05:01,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,The results important
cs-410_3_1_65,"00:05:01,990","00:05:05,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,this is partly due to the difference
cs-410_3_1_66,"00:05:07,610","00:05:10,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,So test queries tend to be ambiguous.
cs-410_3_1_67,"00:05:10,960","00:05:16,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"Whereas in their research,"
cs-410_3_1_68,"00:05:16,290","00:05:22,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,Think about a SQL query that would clearly
cs-410_3_1_69,"00:05:22,330","00:05:24,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,So it has very well-defined semantics.
cs-410_3_1_70,"00:05:27,230","00:05:32,252",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,Keyword queries or electronic queries tend
cs-410_3_1_71,"00:05:32,252","00:05:37,952",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"to be incomplete,"
cs-410_3_1_72,"00:05:37,952","00:05:43,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,specify what documents
cs-410_3_1_73,"00:05:43,390","00:05:46,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,Whereas complete specification for
cs-410_3_1_74,"00:05:47,390","00:05:50,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"And because of these differences,"
cs-410_3_1_75,"00:05:50,900","00:05:56,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"Being the case of text retrieval, we're"
cs-410_3_1_76,"00:05:58,110","00:06:02,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"In the database search,"
cs-410_3_1_77,"00:06:02,740","00:06:07,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,match records with the sequel
cs-410_3_1_78,"00:06:09,110","00:06:14,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"Now in the case of text retrieval,"
cs-410_3_1_79,"00:06:14,550","00:06:19,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"to the query is not very well specified,"
cs-410_3_1_80,"00:06:21,140","00:06:25,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,So it's unclear what should be
cs-410_3_1_81,"00:06:25,830","00:06:30,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"And this has very important consequences,"
cs-410_3_1_82,"00:06:30,510","00:06:35,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,textual retrieval is
cs-410_3_1_83,"00:06:38,578","00:06:44,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,So this is a problem because
cs-410_3_1_84,"00:06:44,100","00:06:51,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,then we can not mathematically prove one
cs-410_3_1_85,"00:06:52,620","00:06:56,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,That also means we must rely
cs-410_3_1_86,"00:06:56,650","00:07:01,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,involving users to know
cs-410_3_1_87,"00:07:02,460","00:07:05,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,And that's why we have.
cs-410_3_1_88,"00:07:05,080","00:07:09,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,You need more than one lectures
cs-410_3_1_89,"00:07:09,420","00:07:12,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,Because this is very important topic for
cs-410_3_1_90,"00:07:13,890","00:07:18,902",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,Without knowing how to evaluate heroism
cs-410_3_1_91,"00:07:18,902","00:07:24,563",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,whether we have got the better or
cs-410_3_1_92,"00:07:28,393","00:07:31,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So now let's look at
cs-410_3_1_93,"00:07:32,240","00:07:36,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"So, this slide shows a formal formulation"
cs-410_3_1_94,"00:07:37,460","00:07:43,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"First, we have our vocabulary set, which"
cs-410_3_1_95,"00:07:44,920","00:07:49,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"Now here,"
cs-410_3_1_96,"00:07:49,140","00:07:53,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"in reality, on the web,"
cs-410_3_1_97,"00:07:53,360","00:07:56,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,We have texts that are in
cs-410_3_1_98,"00:07:57,530","00:08:01,478",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"But here for simplicity, we just"
cs-410_3_1_99,"00:08:01,478","00:08:07,088",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,As the techniques used for retrieving
cs-410_3_1_100,"00:08:07,088","00:08:12,783",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,less similar to the techniques used for
cs-410_3_1_101,"00:08:12,783","00:08:18,819",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"although there is important difference,"
cs-410_3_1_102,"00:08:21,759","00:08:24,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,"Next, we have the query,"
cs-410_3_1_103,"00:08:26,015","00:08:28,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"And so here, you can see"
cs-410_3_1_104,"00:08:31,175","00:08:36,482",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,the query is defined as
cs-410_3_1_105,"00:08:36,482","00:08:41,252",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Each q sub i is a word in the vocabulary.
cs-410_3_1_106,"00:08:42,302","00:08:47,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"A document is defined in the same way,"
cs-410_3_1_107,"00:08:47,000","00:08:51,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"And here,"
cs-410_3_1_108,"00:08:52,920","00:08:55,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"Now typically, the documents"
cs-410_3_1_109,"00:08:57,100","00:09:01,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,But there are also cases where
cs-410_3_1_110,"00:09:04,370","00:09:08,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,So you can think about what
cs-410_3_1_111,"00:09:09,670","00:09:13,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,I hope you can think of Twitter search.
cs-410_3_1_112,"00:09:13,570","00:09:14,992",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,Tweets are very short.
cs-410_3_1_113,"00:09:16,557","00:09:20,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,"But in general,"
cs-410_3_1_114,"00:09:22,934","00:09:27,389",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,"Now, then we have"
cs-410_3_1_115,"00:09:27,389","00:09:31,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,and this collection can be very large.
cs-410_3_1_116,"00:09:31,240","00:09:32,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,So think about the web.
cs-410_3_1_117,"00:09:32,370","00:09:33,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,It could be very large.
cs-410_3_1_118,"00:09:36,140","00:09:40,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,And then the goal of text retrieval
cs-410_3_1_119,"00:09:40,300","00:09:46,358",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,"the documents, which we denote by R'(q),"
cs-410_3_1_120,"00:09:46,358","00:09:50,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"And this in general, a subset of all"
cs-410_3_1_121,"00:09:52,410","00:09:57,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,"Unfortunately, this set of relevant"
cs-410_3_1_122,"00:09:57,862","00:10:03,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"and user-dependent in the sense that,"
cs-410_3_1_123,"00:10:03,000","00:10:08,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,"in by different users, they expect"
cs-410_3_1_124,"00:10:09,330","00:10:13,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,The query given to us by
cs-410_3_1_125,"00:10:13,600","00:10:15,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,on which document should be in this set.
cs-410_3_1_126,"00:10:17,840","00:10:24,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,"And indeed, the user is generally"
cs-410_3_1_127,"00:10:24,940","00:10:28,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,"be in this set, especially in the case"
cs-410_3_1_128,"00:10:28,940","00:10:32,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"large, the user doesn't have complete"
cs-410_3_1_129,"00:10:34,000","00:10:39,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,So the best search system
cs-410_3_1_130,"00:10:39,550","00:10:45,856",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,an approximation of this
cs-410_3_1_131,"00:10:45,856","00:10:50,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,So we denote it by R'(q).
cs-410_3_1_132,"00:10:50,168","00:10:54,835",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"So formerly,"
cs-410_3_1_133,"00:10:54,835","00:10:59,849",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,R'(q) approximation of
cs-410_3_1_134,"00:10:59,849","00:11:01,902",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So how can we do that?
cs-410_3_1_135,"00:11:01,902","00:11:07,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,Now imagine if you are now asked
cs-410_3_1_136,"00:11:08,980","00:11:10,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,What would you do?
cs-410_3_1_137,"00:11:10,480","00:11:12,526",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,Now think for a moment.
cs-410_3_1_138,"00:11:12,526","00:11:14,433",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"Right, so these are your input."
cs-410_3_1_139,"00:11:14,433","00:11:18,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,"The query, the documents."
cs-410_3_1_140,"00:11:20,399","00:11:24,021",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,And then you are to compute
cs-410_3_1_141,"00:11:24,021","00:11:28,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,which is a set of documents that
cs-410_3_1_142,"00:11:29,770","00:11:31,926",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,"So, how would you solve the problem?"
cs-410_3_1_143,"00:11:31,926","00:11:37,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"Now in general,"
cs-410_3_1_144,"00:11:39,720","00:11:42,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,The first strategy is we do a document
cs-410_3_1_145,"00:11:42,970","00:11:47,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,going to have a binary classification
cs-410_3_1_146,"00:11:49,350","00:11:52,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,That's a function that
cs-410_3_1_147,"00:11:52,110","00:11:55,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"query as input, and then give a zero or"
cs-410_3_1_148,"00:11:55,740","00:12:01,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,one as output to indicate whether this
cs-410_3_1_149,"00:12:02,330","00:12:05,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"So in this case, you can see the document."
cs-410_3_1_150,"00:12:08,700","00:12:15,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,"The relevant document is set,"
cs-410_3_1_151,"00:12:15,130","00:12:22,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,"It basically, all the documents that"
cs-410_3_1_152,"00:12:25,410","00:12:26,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,"So in this case,"
cs-410_3_1_153,"00:12:26,040","00:12:29,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,you can see the system must have decide
cs-410_3_1_154,"00:12:29,930","00:12:33,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"Basically, it has to say"
cs-410_3_1_155,"00:12:33,680","00:12:36,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,And this is called absolute relevance.
cs-410_3_1_156,"00:12:36,330","00:12:38,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,"Basically, it needs to know"
cs-410_3_1_157,"00:12:38,940","00:12:39,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,useful to the user.
cs-410_3_1_158,"00:12:41,940","00:12:44,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"Alternatively, there's another"
cs-410_3_1_159,"00:12:46,160","00:12:47,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,"Now in this case,"
cs-410_3_1_160,"00:12:47,150","00:12:52,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,the system is not going to make a call
cs-410_3_1_161,"00:12:52,290","00:12:57,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,But rather the system is going to
cs-410_3_1_162,"00:12:58,440","00:13:01,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,That would simply give us a value
cs-410_3_1_163,"00:13:01,540","00:13:04,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,that would indicate which
cs-410_3_1_164,"00:13:05,740","00:13:08,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,So it's not going to make a call whether
cs-410_3_1_165,"00:13:08,590","00:13:12,696",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,But rather it would say which
cs-410_3_1_166,"00:13:12,696","00:13:17,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,So this function then can be
cs-410_3_1_167,"00:13:17,669","00:13:22,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,then we're going to let
cs-410_3_1_168,"00:13:22,135","00:13:25,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,when the user looks at the document.
cs-410_3_1_169,"00:13:25,296","00:13:31,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,So we have a threshold theta
cs-410_3_1_170,"00:13:31,410","00:13:37,398",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=811,documents should be in
cs-410_3_1_171,"00:13:37,398","00:13:40,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And we're going to assume
cs-410_3_1_172,"00:13:40,802","00:13:45,312",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,are ranked above the threshold
cs-410_3_1_173,"00:13:45,312","00:13:49,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,these are the documents that
cs-410_3_1_174,"00:13:49,780","00:13:54,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,And theta is a cutoff
cs-410_3_1_175,"00:13:56,980","00:14:00,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,So here we've got some collaboration
cs-410_3_1_176,"00:14:00,980","00:14:03,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,because we don't really make a cutoff.
cs-410_3_1_177,"00:14:03,330","00:14:07,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=843,And the user kind of helped
cs-410_3_1_178,"00:14:08,120","00:14:10,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,"So in this case,"
cs-410_3_1_179,"00:14:10,950","00:14:14,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,if one document is more
cs-410_3_1_180,"00:14:14,440","00:14:17,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,"And that is, it only needs to"
cs-410_3_1_181,"00:14:19,050","00:14:20,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,as opposed to absolute relevance.
cs-410_3_1_182,"00:14:22,230","00:14:26,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,Now you can probably already sense that
cs-410_3_1_183,"00:14:26,290","00:14:31,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,relative relevance would be easier to
cs-410_3_1_184,"00:14:31,560","00:14:32,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,"Because in the first case,"
cs-410_3_1_185,"00:14:32,800","00:14:36,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,we have to say exactly whether
cs-410_3_1_186,"00:14:37,990","00:14:45,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,And it turns out that ranking is indeed
cs-410_3_1_187,"00:14:46,710","00:14:50,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,So let's look at these two
cs-410_3_1_188,"00:14:50,240","00:14:53,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,So this picture shows how it works.
cs-410_3_1_189,"00:14:53,960","00:14:58,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,"So on the left side,"
cs-410_3_1_190,"00:14:58,780","00:15:02,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,we use the pluses to indicate
cs-410_3_1_191,"00:15:02,710","00:15:09,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,So we can see the true relevant
cs-410_3_1_192,"00:15:09,990","00:15:15,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,"of true relevant documents, consists"
cs-410_3_1_193,"00:15:17,450","00:15:20,972",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,"And with the document selection function,"
cs-410_3_1_194,"00:15:20,972","00:15:25,636",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,we're going to basically
cs-410_3_1_195,"00:15:25,636","00:15:30,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,"relevant documents, and non-relevant ones."
cs-410_3_1_196,"00:15:30,050","00:15:34,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=930,"Of course, the classified will not"
cs-410_3_1_197,"00:15:34,700","00:15:39,522",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,"So here we can see, in the approximation"
cs-410_3_1_198,"00:15:39,522","00:15:41,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,we have got some number in the documents.
cs-410_3_1_199,"00:15:43,090","00:15:44,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,"And similarly,"
cs-410_3_1_200,"00:15:44,168","00:15:48,868",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,there is a relevant document that's
cs-410_3_1_201,"00:15:48,868","00:15:53,972",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"In the case of document ranking,"
cs-410_3_1_202,"00:15:53,972","00:15:59,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,simply ranks all the documents in
cs-410_3_1_203,"00:15:59,368","00:16:04,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"And then, we're going to let the user"
cs-410_3_1_204,"00:16:04,580","00:16:07,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=964,If the user wants to
cs-410_3_1_205,"00:16:07,510","00:16:11,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,then the user will scroll down some
cs-410_3_1_206,"00:16:11,630","00:16:17,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,But if the user only wants to
cs-410_3_1_207,"00:16:17,010","00:16:20,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=977,the user might stop at the top position.
cs-410_3_1_208,"00:16:20,750","00:16:24,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"So in this case, the user stops at d4."
cs-410_3_1_209,"00:16:24,200","00:16:30,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=984,"So in fact, we have delivered"
cs-410_3_1_210,"00:16:33,940","00:16:40,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,So as I said ranking is generally
cs-410_3_1_211,"00:16:40,300","00:16:46,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,because the classifier in the case of
cs-410_3_1_212,"00:16:46,410","00:16:47,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,Why?
cs-410_3_1_213,"00:16:47,790","00:16:51,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,Because the only clue
cs-410_3_1_214,"00:16:51,100","00:16:56,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,But the query may not be accurate in the
cs-410_3_1_215,"00:16:57,660","00:17:02,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,"For example, you might expect relevant"
cs-410_3_1_216,"00:17:04,460","00:17:08,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1024,topics by using specific vocabulary.
cs-410_3_1_217,"00:17:08,050","00:17:13,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,"And as a result,"
cs-410_3_1_218,"00:17:13,550","00:17:15,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,"Because in the collection,"
cs-410_3_1_219,"00:17:15,690","00:17:19,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,no others have discussed the topic
cs-410_3_1_220,"00:17:19,990","00:17:24,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1039,"So in this case,"
cs-410_3_1_221,"00:17:25,970","00:17:31,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,no relevant documents to return in
cs-410_3_1_222,"00:17:33,230","00:17:37,892",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1053,"On the other hand,"
cs-410_3_1_223,"00:17:37,892","00:17:40,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1057,"for example, if the query"
cs-410_3_1_224,"00:17:40,430","00:17:44,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,does not have sufficient descriptive
cs-410_3_1_225,"00:17:44,610","00:17:51,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,You may actually end up having of
cs-410_3_1_226,"00:17:51,100","00:17:55,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,thought these words my be sufficient
cs-410_3_1_227,"00:17:55,840","00:17:58,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1075,"But, it turns out they"
cs-410_3_1_228,"00:17:58,590","00:18:04,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1078,"there are many distractions,"
cs-410_3_1_229,"00:18:04,280","00:18:07,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1084,"And so, this is a case of over delivery."
cs-410_3_1_230,"00:18:08,570","00:18:13,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,"Unfortunately, it's very hard to find the"
cs-410_3_1_231,"00:18:15,390","00:18:15,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,Why?
cs-410_3_1_232,"00:18:15,900","00:18:19,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,Because whether users looking for
cs-410_3_1_233,"00:18:19,940","00:18:24,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,not have a good knowledge about
cs-410_3_1_234,"00:18:24,520","00:18:28,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1104,"And in that case, the user does not"
cs-410_3_1_235,"00:18:30,240","00:18:33,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1110,vocabularies will be used in
cs-410_3_1_236,"00:18:33,770","00:18:36,064",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1113,So it's very hard for
cs-410_3_1_237,"00:18:36,064","00:18:42,061",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,a user to pre-specify the right
cs-410_3_1_238,"00:18:44,569","00:18:49,502",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1124,"Even if the classifier is accurate,"
cs-410_3_1_239,"00:18:49,502","00:18:54,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,"relevant documents, because they"
cs-410_3_1_240,"00:18:56,130","00:18:58,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1136,Relevance is often a matter of degree.
cs-410_3_1_241,"00:18:59,560","00:19:05,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,So we must prioritize these documents for
cs-410_3_1_242,"00:19:06,320","00:19:10,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1146,And note that this
cs-410_3_1_243,"00:19:12,300","00:19:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1152,because a user cannot
cs-410_3_1_244,"00:19:15,840","00:19:20,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1155,the user generally would have to
cs-410_3_1_245,"00:19:21,750","00:19:29,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1161,"And therefore, it would make sense to"
cs-410_3_1_246,"00:19:29,220","00:19:32,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1169,And that's what ranking is doing.
cs-410_3_1_247,"00:19:32,100","00:19:35,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,"So for these reasons,"
cs-410_3_1_248,"00:19:36,330","00:19:39,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1176,Now this preference also has
cs-410_3_1_249,"00:19:39,610","00:19:42,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1179,this is given by the probability
cs-410_3_1_250,"00:19:44,210","00:19:47,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,"In the end of this lecture,"
cs-410_3_1_251,"00:19:49,320","00:19:54,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,"This principle says, returning a ranked"
cs-410_3_1_252,"00:19:54,260","00:19:57,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1194,of probability that a document
cs-410_3_1_253,"00:19:57,590","00:20:01,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1197,is the optimal strategy under
cs-410_3_1_254,"00:20:02,620","00:20:05,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1202,"First, the utility of"
cs-410_3_1_255,"00:20:05,690","00:20:09,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,Is independent of the utility
cs-410_3_1_256,"00:20:10,980","00:20:15,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1210,"Second, a user would be assumed to"
cs-410_3_1_257,"00:20:15,300","00:20:21,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1215,Now it's easy to understand why these
cs-410_3_1_258,"00:20:21,775","00:20:27,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1221,Site for the ranking strategy.
cs-410_3_1_259,"00:20:27,130","00:20:30,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1227,"Because if the documents are independent,"
cs-410_3_1_260,"00:20:30,930","00:20:34,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,then we can evaluate the utility
cs-410_3_1_261,"00:20:36,350","00:20:40,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1236,And this would allow the computer
cs-410_3_1_262,"00:20:40,270","00:20:43,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1240,"And then, we are going to rank these"
cs-410_3_1_263,"00:20:45,710","00:20:51,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1245,The second assumption is to say that the
cs-410_3_1_264,"00:20:51,300","00:20:55,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1251,If the user is not going to follow
cs-410_3_1_265,"00:20:55,050","00:20:59,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1255,"the documents sequentially, then obviously"
cs-410_3_1_266,"00:21:00,560","00:21:08,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1260,"So under these two assumptions, we can"
cs-410_3_1_267,"00:21:08,270","00:21:13,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1268,"is, in fact, the best that you could do."
cs-410_3_1_268,"00:21:13,170","00:21:14,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1273,"Now, I've put one question here."
cs-410_3_1_269,"00:21:14,700","00:21:16,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1274,Do these two assumptions hold?
cs-410_3_1_270,"00:21:18,240","00:21:23,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1278,"I suggest you to pause the lecture,"
cs-410_3_1_271,"00:21:27,950","00:21:33,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1287,"Now, can you think of"
cs-410_3_1_272,"00:21:33,065","00:21:39,238",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1293,suggest these assumptions
cs-410_3_1_273,"00:21:44,462","00:21:46,953",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1304,"Now, if you think for a moment,"
cs-410_3_1_274,"00:21:46,953","00:21:51,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1306,you may realize none of
cs-410_3_1_275,"00:21:53,230","00:21:57,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1313,"For example, in the case of"
cs-410_3_1_276,"00:21:57,690","00:22:02,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1317,have documents that have similar or
cs-410_3_1_277,"00:22:02,590","00:22:06,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,"If we look at each of them alone,"
cs-410_3_1_278,"00:22:07,790","00:22:12,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1327,But if the user has already seen
cs-410_3_1_279,"00:22:12,510","00:22:17,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1332,generally not very useful for the user to
cs-410_3_1_280,"00:22:19,030","00:22:22,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1339,So clearly the utility
cs-410_3_1_281,"00:22:22,040","00:22:25,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1342,is dependent on other documents
cs-410_3_1_282,"00:22:27,350","00:22:32,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1347,In some other cases you might see
cs-410_3_1_283,"00:22:32,510","00:22:38,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1352,"be useful to the user, but when three"
cs-410_3_1_284,"00:22:38,490","00:22:41,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1358,They provide answers to
cs-410_3_1_285,"00:22:42,140","00:22:46,883",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1362,So this is a collective relevance and
cs-410_3_1_286,"00:22:46,883","00:22:51,542",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1366,the value of the document might
cs-410_3_1_287,"00:22:53,329","00:22:58,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1373,Sequential browsing generally would make
cs-410_3_1_288,"00:22:59,220","00:23:04,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1379,"But even if you have a rank list,"
cs-410_3_1_289,"00:23:04,650","00:23:10,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1384,users don't always just go strictly
cs-410_3_1_290,"00:23:10,140","00:23:14,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1390,They sometimes will look at the bottom for
cs-410_3_1_291,"00:23:14,910","00:23:17,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1394,And if you think about the more
cs-410_3_1_292,"00:23:17,810","00:23:22,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1397,we could possibly use like
cs-410_3_1_293,"00:23:22,100","00:23:26,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1402,Where you can put that additional
cs-410_3_1_294,"00:23:26,820","00:23:29,379",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1406,sequential browsing is a very
cs-410_3_1_295,"00:23:32,010","00:23:34,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1412,So the point here is that
cs-410_3_1_296,"00:23:35,740","00:23:39,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1415,none of these assumptions is
cs-410_3_1_297,"00:23:41,100","00:23:45,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1421,But probability ranking principle
cs-410_3_1_298,"00:23:46,870","00:23:51,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1426,ranking as a primary pattern for
cs-410_3_1_299,"00:23:51,020","00:23:53,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1431,And this has actually been the basis for
cs-410_3_1_300,"00:23:53,180","00:23:57,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1433,a lot of research work in
cs-410_3_1_301,"00:23:57,090","00:24:00,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1437,And many hours have been designed
cs-410_3_1_302,"00:24:01,590","00:24:06,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1441,despite that the assumptions
cs-410_3_1_303,"00:24:06,410","00:24:11,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1446,And we can address this problem
cs-410_3_1_304,"00:24:11,570","00:24:15,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1451,"Of a ranked list, for example,"
cs-410_3_1_305,"00:24:20,260","00:24:22,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1460,"So to summarize this lecture,"
cs-410_3_1_306,"00:24:22,500","00:24:28,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1462,the main points that you can
cs-410_3_1_307,"00:24:28,000","00:24:31,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1468,"First, text retrieval is"
cs-410_3_1_308,"00:24:31,760","00:24:37,951",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1471,And that means which algorithm is
cs-410_3_1_309,"00:24:37,951","00:24:42,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1477,"Second, document ranking"
cs-410_3_1_310,"00:24:42,500","00:24:46,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1482,And this will help users prioritize
cs-410_3_1_311,"00:24:47,410","00:24:52,693",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1487,And this is also to bypass the difficulty
cs-410_3_1_312,"00:24:52,693","00:24:58,221",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1492,Because we can get some help from users
cs-410_3_1_313,"00:24:58,221","00:24:59,809",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1498,it's more flexible.
cs-410_3_1_314,"00:25:01,967","00:25:06,904",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1501,"So, this further suggests that the main"
cs-410_3_1_315,"00:25:06,904","00:25:09,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1506,engine is the design
cs-410_3_1_316,"00:25:10,970","00:25:16,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1510,"In other words, we need to define"
cs-410_3_1_317,"00:25:16,150","00:25:19,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1516,on the query and document pair.
cs-410_3_1_318,"00:25:21,360","00:25:26,151",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1521,How we design such a function is the main
cs-410_3_1_319,"00:25:29,123","00:25:32,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1529,There are two suggested
cs-410_3_1_320,"00:25:32,060","00:25:36,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1532,The first is the classical paper on
cs-410_3_1_321,"00:25:37,470","00:25:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1537,The second one is a must-read for anyone
cs-410_3_1_322,"00:25:42,380","00:25:49,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1542,"It's a classic IR book, which has"
cs-410_3_1_323,"00:25:49,220","00:25:55,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1549,and results in early days up to
cs-410_3_1_324,"00:25:55,540","00:25:59,762",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1555,Chapter six of this book has
cs-410_3_1_325,"00:25:59,762","00:26:06,211",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1559,the Probability Ranking Principle and
cs-410_3_1_326,"00:26:06,211","00:26:16,211",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1566,[MUSIC]
cs-410_1_1_1,"00:00:00,008","00:00:04,018",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_1_2,"00:00:09,625","00:00:12,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,>> This lecture is about Natural Language
cs-410_1_1_3,"00:00:12,226","00:00:13,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,of Content Analysis.
cs-410_1_1_4,"00:00:13,732","00:00:15,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"As you see from this picture,"
cs-410_1_1_5,"00:00:15,569","00:00:19,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,this is really the first step
cs-410_1_1_6,"00:00:19,540","00:00:22,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,Text data are in natural languages.
cs-410_1_1_7,"00:00:22,060","00:00:26,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So computers have to understand
cs-410_1_1_8,"00:00:26,820","00:00:29,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,in order to make use of the data.
cs-410_1_1_9,"00:00:29,380","00:00:32,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,So that's the topic of this lecture.
cs-410_1_1_10,"00:00:32,000","00:00:33,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,We're going to cover three things.
cs-410_1_1_11,"00:00:33,910","00:00:36,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"First, what is natural"
cs-410_1_1_12,"00:00:36,430","00:00:41,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,which is the main technique for processing
cs-410_1_1_13,"00:00:43,150","00:00:46,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,The second is the state of
cs-410_1_1_14,"00:00:46,420","00:00:48,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,natural language processing.
cs-410_1_1_15,"00:00:49,540","00:00:53,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,Finally we're going to cover the relation
cs-410_1_1_16,"00:00:53,430","00:00:54,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,text retrieval.
cs-410_1_1_17,"00:00:54,900","00:00:57,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"First, what is NLP?"
cs-410_1_1_18,"00:00:57,280","00:01:02,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,Well the best way to explain it
cs-410_1_1_19,"00:01:02,240","00:01:05,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,a text in a foreign language
cs-410_1_1_20,"00:01:06,980","00:01:10,907",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,Now what do you have to do in
cs-410_1_1_21,"00:01:10,907","00:01:13,172",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,This is basically what
cs-410_1_1_22,"00:01:13,172","00:01:17,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,So looking at the simple sentence like
cs-410_1_1_23,"00:01:18,730","00:01:22,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,We don't have any problems
cs-410_1_1_24,"00:01:22,250","00:01:25,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,But imagine what the computer would
cs-410_1_1_25,"00:01:25,930","00:01:27,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"Well in general,"
cs-410_1_1_26,"00:01:27,830","00:01:34,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"First, it would have to know dog"
cs-410_1_1_27,"00:01:34,310","00:01:38,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"So this is called lexical analysis,"
cs-410_1_1_28,"00:01:38,410","00:01:42,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,we need to figure out the syntactic
cs-410_1_1_29,"00:01:42,230","00:01:43,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,So that's the first step.
cs-410_1_1_30,"00:01:43,930","00:01:48,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"After that, we're going to figure"
cs-410_1_1_31,"00:01:48,060","00:01:50,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"So for example, here it shows that A and"
cs-410_1_1_32,"00:01:50,370","00:01:54,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,the dog would go together
cs-410_1_1_33,"00:01:55,730","00:01:59,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,And we won't have dog and is to go first.
cs-410_1_1_34,"00:01:59,500","00:02:02,969",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,And there are some structures
cs-410_1_1_35,"00:02:04,470","00:02:09,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,But this structure shows what we might
cs-410_1_1_36,"00:02:09,650","00:02:11,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,try to interpret the sentence.
cs-410_1_1_37,"00:02:11,850","00:02:13,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"Some words would go together first, and"
cs-410_1_1_38,"00:02:13,960","00:02:15,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,then they will go together
cs-410_1_1_39,"00:02:16,640","00:02:20,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,So here we show we have noun phrases
cs-410_1_1_40,"00:02:20,200","00:02:21,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,then verbal phrases.
cs-410_1_1_41,"00:02:21,500","00:02:23,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Finally we have a sentence.
cs-410_1_1_42,"00:02:23,670","00:02:25,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And you get this structure.
cs-410_1_1_43,"00:02:25,430","00:02:29,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,We need to do something called
cs-410_1_1_44,"00:02:29,400","00:02:31,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And we may have a parser
cs-410_1_1_45,"00:02:31,610","00:02:34,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,that would automatically
cs-410_1_1_46,"00:02:34,880","00:02:38,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,At this point you would know
cs-410_1_1_47,"00:02:38,220","00:02:40,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,still you don't know
cs-410_1_1_48,"00:02:40,440","00:02:44,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,So we have to go further
cs-410_1_1_49,"00:02:44,060","00:02:47,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,In our mind we usually can map
cs-410_1_1_50,"00:02:47,120","00:02:51,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,such a sentence to what we already
cs-410_1_1_51,"00:02:51,330","00:02:53,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"For example, you might imagine"
cs-410_1_1_52,"00:02:53,970","00:02:56,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,There's a boy and
cs-410_1_1_53,"00:02:56,800","00:02:59,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,But for a computer would have
cs-410_1_1_54,"00:03:00,890","00:03:05,232",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,We'd use a symbol (d1) to denote a dog.
cs-410_1_1_55,"00:03:05,232","00:03:10,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And (b)1 can denote a boy and
cs-410_1_1_56,"00:03:12,650","00:03:15,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,Now there is also a chasing
cs-410_1_1_57,"00:03:15,440","00:03:19,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,we have a relationship chasing
cs-410_1_1_58,"00:03:19,130","00:03:23,909",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,So this is how a computer would obtain
cs-410_1_1_59,"00:03:25,920","00:03:31,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Now from this representation we could
cs-410_1_1_60,"00:03:31,590","00:03:35,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,and we might indeed naturally think of
cs-410_1_1_61,"00:03:35,960","00:03:37,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,this is called inference.
cs-410_1_1_62,"00:03:37,470","00:03:42,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"So for example, if you believe"
cs-410_1_1_63,"00:03:42,490","00:03:46,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"this person might be scared,"
cs-410_1_1_64,"00:03:46,180","00:03:50,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,you can see computers could also
cs-410_1_1_65,"00:03:50,880","00:03:54,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,So this is some extra knowledge
cs-410_1_1_66,"00:03:54,080","00:03:56,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,some understanding of the text.
cs-410_1_1_67,"00:03:56,430","00:04:02,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,You can even go further to understand
cs-410_1_1_68,"00:04:02,280","00:04:05,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,So this has to do as a use of language.
cs-410_1_1_69,"00:04:05,000","00:04:08,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,This is called pragmatic analysis.
cs-410_1_1_70,"00:04:08,740","00:04:13,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,In order to understand the speak
cs-410_1_1_71,"00:04:13,910","00:04:18,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,We say something to
cs-410_1_1_72,"00:04:18,370","00:04:19,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,There's some purpose there.
cs-410_1_1_73,"00:04:19,440","00:04:22,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,And this has to do with
cs-410_1_1_74,"00:04:22,100","00:04:24,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,In this case the person who said
cs-410_1_1_75,"00:04:24,750","00:04:29,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,this sentence might be reminding
cs-410_1_1_76,"00:04:29,200","00:04:31,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,That could be one possible intent.
cs-410_1_1_77,"00:04:33,020","00:04:36,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,To reach this level of
cs-410_1_1_78,"00:04:36,500","00:04:41,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,all of these steps and
cs-410_1_1_79,"00:04:41,390","00:04:46,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,these steps in order to completely
cs-410_1_1_80,"00:04:46,940","00:04:49,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,Yet we humans have no trouble
cs-410_1_1_81,"00:04:49,560","00:04:51,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,we instantly would get everything.
cs-410_1_1_82,"00:04:52,790","00:04:53,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,There is a reason for that.
cs-410_1_1_83,"00:04:53,760","00:04:57,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,That's because we have a large
cs-410_1_1_84,"00:04:57,430","00:05:01,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we can use common sense knowledge
cs-410_1_1_85,"00:05:01,890","00:05:06,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,Computers unfortunately are hard
cs-410_1_1_86,"00:05:06,330","00:05:08,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,They don't have such a knowledge base.
cs-410_1_1_87,"00:05:08,430","00:05:12,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,They are still incapable of doing
cs-410_1_1_88,"00:05:14,290","00:05:18,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,so that makes natural language
cs-410_1_1_89,"00:05:18,430","00:05:21,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,But the fundamental reason why natural
cs-410_1_1_90,"00:05:21,540","00:05:25,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,computers is simply because natural
cs-410_1_1_91,"00:05:25,430","00:05:26,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,computers.
cs-410_1_1_92,"00:05:26,430","00:05:30,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,Natural languages are designed for
cs-410_1_1_93,"00:05:30,960","00:05:33,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,There are other languages designed for
cs-410_1_1_94,"00:05:33,480","00:05:36,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"For example, programming languages."
cs-410_1_1_95,"00:05:36,220","00:05:38,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"Those are harder for us, right?"
cs-410_1_1_96,"00:05:38,780","00:05:43,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,So natural languages is designed to
cs-410_1_1_97,"00:05:43,690","00:05:46,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"As a result,"
cs-410_1_1_98,"00:05:46,770","00:05:49,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,because we assume everyone
cs-410_1_1_99,"00:05:49,540","00:05:56,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,We also keep a lot of ambiguities because
cs-410_1_1_100,"00:05:56,250","00:06:02,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,know how to decipher an ambiguous word
cs-410_1_1_101,"00:06:02,020","00:06:05,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,There's no need to demand different
cs-410_1_1_102,"00:06:05,320","00:06:08,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,We could overload the same word with
cs-410_1_1_103,"00:06:10,460","00:06:14,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,Because of these reasons this makes every
cs-410_1_1_104,"00:06:14,350","00:06:17,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"difficult for computers,"
cs-410_1_1_105,"00:06:18,780","00:06:22,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,And common sense and reasoning is
cs-410_1_1_106,"00:06:23,800","00:06:26,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,So let me give you some
cs-410_1_1_107,"00:06:27,505","00:06:29,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Consider the word level ambiguity.
cs-410_1_1_108,"00:06:30,730","00:06:34,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,The same word can have
cs-410_1_1_109,"00:06:34,510","00:06:36,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,For example design can be a noun or
cs-410_1_1_110,"00:06:39,270","00:06:42,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,The word of root may
cs-410_1_1_111,"00:06:42,160","00:06:45,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So square root in math sense or
cs-410_1_1_112,"00:06:46,450","00:06:49,464",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,You might be able to think
cs-410_1_1_113,"00:06:49,464","00:06:52,609",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,There are also syntactical ambiguities.
cs-410_1_1_114,"00:06:52,609","00:06:56,932",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"For example, the main topic of this"
cs-410_1_1_115,"00:06:56,932","00:07:01,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,can actually be interpreted in two
cs-410_1_1_116,"00:07:01,480","00:07:03,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Think for a moment and
cs-410_1_1_117,"00:07:03,900","00:07:09,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We usually think of this as
cs-410_1_1_118,"00:07:09,560","00:07:13,991",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,but you could also think of this as do
cs-410_1_1_119,"00:07:16,130","00:07:20,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,So this is an example
cs-410_1_1_120,"00:07:20,440","00:07:23,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,What we have different is
cs-410_1_1_121,"00:07:24,510","00:07:27,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,applied to the same sequence of words.
cs-410_1_1_122,"00:07:27,480","00:07:31,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Another common example of an ambiguous
cs-410_1_1_123,"00:07:31,730","00:07:34,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,A man saw a boy with a telescope.
cs-410_1_1_124,"00:07:34,480","00:07:37,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"Now in this case the question is,"
cs-410_1_1_125,"00:07:38,820","00:07:42,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,This is called a prepositional
cs-410_1_1_126,"00:07:42,700","00:07:45,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,PP attachment ambiguity.
cs-410_1_1_127,"00:07:45,030","00:07:50,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now we generally don't have a problem with
cs-410_1_1_128,"00:07:50,000","00:07:54,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,background knowledge to help
cs-410_1_1_129,"00:07:55,380","00:07:57,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Another example of difficulty
cs-410_1_1_130,"00:07:57,961","00:08:03,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So think about the sentence John
cs-410_1_1_131,"00:08:03,290","00:08:07,632",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,The question here is does
cs-410_1_1_132,"00:08:07,632","00:08:10,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So again this is something that
cs-410_1_1_133,"00:08:10,803","00:08:12,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,the context to figure out.
cs-410_1_1_134,"00:08:12,540","00:08:15,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"Finally, presupposition"
cs-410_1_1_135,"00:08:15,470","00:08:18,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"Consider the sentence,"
cs-410_1_1_136,"00:08:18,110","00:08:20,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Now this obviously implies
cs-410_1_1_137,"00:08:22,430","00:08:27,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So imagine a computer wants to understand
cs-410_1_1_138,"00:08:27,000","00:08:30,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,It would have to use a lot of
cs-410_1_1_139,"00:08:30,750","00:08:35,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,It also would have to maintain a large
cs-410_1_1_140,"00:08:35,890","00:08:41,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,words and how they are connected to our
cs-410_1_1_141,"00:08:41,940","00:08:44,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this is why it's very difficult.
cs-410_1_1_142,"00:08:45,530","00:08:49,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So as a result, we are steep not perfect,"
cs-410_1_1_143,"00:08:49,110","00:08:54,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,in fact far from perfect in understanding
cs-410_1_1_144,"00:08:54,240","00:09:00,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,So this slide sort of gains a simplified
cs-410_1_1_145,"00:09:01,580","00:09:06,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,We can do part of speech
cs-410_1_1_146,"00:09:06,640","00:09:09,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,I showed 97% accuracy here.
cs-410_1_1_147,"00:09:09,610","00:09:13,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,Now this number is obviously
cs-410_1_1_148,"00:09:13,830","00:09:15,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,don't take this literally.
cs-410_1_1_149,"00:09:15,680","00:09:18,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,This just shows that we
cs-410_1_1_150,"00:09:18,210","00:09:20,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,But it's still not perfect.
cs-410_1_1_151,"00:09:20,320","00:09:23,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"In terms of parsing,"
cs-410_1_1_152,"00:09:23,620","00:09:27,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,That means we can get noun phrase
cs-410_1_1_153,"00:09:27,800","00:09:31,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"or some segment of the sentence, and"
cs-410_1_1_154,"00:09:31,106","00:09:33,439",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,this dude correct them in
cs-410_1_1_155,"00:09:34,470","00:09:39,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"And in some evaluation results,"
cs-410_1_1_156,"00:09:39,310","00:09:43,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,accuracy in terms of partial
cs-410_1_1_157,"00:09:43,140","00:09:46,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"Again, I have to say these numbers"
cs-410_1_1_158,"00:09:46,910","00:09:50,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"In some other datasets,"
cs-410_1_1_159,"00:09:50,300","00:09:54,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Most of the existing work has been
cs-410_1_1_160,"00:09:54,230","00:09:59,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,And so a lot of these numbers are more or
cs-410_1_1_161,"00:09:59,800","00:10:02,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"Think about social media data,"
cs-410_1_1_162,"00:10:05,460","00:10:07,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,"In terms of a semantical analysis,"
cs-410_1_1_163,"00:10:07,860","00:10:13,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,we are far from being able to do
cs-410_1_1_164,"00:10:13,730","00:10:16,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,But we have some techniques
cs-410_1_1_165,"00:10:16,430","00:10:18,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,do partial understanding of the sentence.
cs-410_1_1_166,"00:10:18,880","00:10:22,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So I could mention some of them.
cs-410_1_1_167,"00:10:22,360","00:10:27,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"For example, we have techniques that can"
cs-410_1_1_168,"00:10:27,190","00:10:30,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,relations mentioned in text articles.
cs-410_1_1_169,"00:10:30,310","00:10:34,766",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"For example,"
cs-410_1_1_170,"00:10:34,766","00:10:38,606",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"locations, organizations, etc in text."
cs-410_1_1_171,"00:10:38,606","00:10:40,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,So this is called entity extraction.
cs-410_1_1_172,"00:10:40,930","00:10:42,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,We may be able to recognize the relations.
cs-410_1_1_173,"00:10:42,950","00:10:46,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"For example,"
cs-410_1_1_174,"00:10:46,140","00:10:51,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,this person met that person or
cs-410_1_1_175,"00:10:51,340","00:10:54,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,Such relations can be extracted by using
cs-410_1_1_176,"00:10:54,350","00:10:57,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,the computer current
cs-410_1_1_177,"00:10:57,230","00:11:00,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,They're not perfect but
cs-410_1_1_178,"00:11:00,170","00:11:02,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,Some entities are harder than others.
cs-410_1_1_179,"00:11:03,040","00:11:05,907",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,We can also do word sense
cs-410_1_1_180,"00:11:05,907","00:11:10,446",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,We have to figure out whether this word in
cs-410_1_1_181,"00:11:10,446","00:11:15,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,in another context the computer could
cs-410_1_1_182,"00:11:15,250","00:11:18,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,"Again, it's not perfect, but"
cs-410_1_1_183,"00:11:19,530","00:11:21,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"We can also do sentiment analysis,"
cs-410_1_1_184,"00:11:21,240","00:11:25,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"meaning, to figure out whether"
cs-410_1_1_185,"00:11:25,830","00:11:28,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,This is especially useful for
cs-410_1_1_186,"00:11:30,410","00:11:33,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,So these are examples
cs-410_1_1_187,"00:11:33,150","00:11:37,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,And they help us to obtain partial
cs-410_1_1_188,"00:11:38,850","00:11:43,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,It's not giving us a complete
cs-410_1_1_189,"00:11:43,410","00:11:44,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,this sentence.
cs-410_1_1_190,"00:11:44,380","00:11:48,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,But it would still help us gain
cs-410_1_1_191,"00:11:48,150","00:11:49,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,And these can be useful.
cs-410_1_1_192,"00:11:51,620","00:11:54,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,"In terms of inference,"
cs-410_1_1_193,"00:11:54,730","00:12:00,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,probably because of the general difficulty
cs-410_1_1_194,"00:12:00,050","00:12:03,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,This is a general challenge
cs-410_1_1_195,"00:12:03,390","00:12:07,468",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,Now that's probably also because
cs-410_1_1_196,"00:12:07,468","00:12:10,172",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,representation for
cs-410_1_1_197,"00:12:10,172","00:12:11,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,So this is hard.
cs-410_1_1_198,"00:12:11,320","00:12:16,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"Yet in some domains perhaps,"
cs-410_1_1_199,"00:12:16,540","00:12:23,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"restrictions on the word uses, you may be"
cs-410_1_1_200,"00:12:23,340","00:12:28,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,But in general we can not
cs-410_1_1_201,"00:12:28,050","00:12:31,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,Speech act analysis is also
cs-410_1_1_202,"00:12:31,650","00:12:36,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,we can only do that analysis for
cs-410_1_1_203,"00:12:36,600","00:12:41,193",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,So this roughly gives you some
cs-410_1_1_204,"00:12:41,193","00:12:46,356",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,And then we also talk a little
cs-410_1_1_205,"00:12:46,356","00:12:51,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,and so we can't even do 100%
cs-410_1_1_206,"00:12:51,780","00:12:54,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,"Now this looks like a simple task, but"
cs-410_1_1_207,"00:12:54,700","00:12:59,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,"think about the example here,"
cs-410_1_1_208,"00:12:59,800","00:13:04,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,have different syntactic categories if you
cs-410_1_1_209,"00:13:04,840","00:13:07,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,It's not that easy to figure
cs-410_1_1_210,"00:13:10,000","00:13:12,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,It's also hard to do
cs-410_1_1_211,"00:13:12,900","00:13:16,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,"And again, the same sentence"
cs-410_1_1_212,"00:13:18,010","00:13:23,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,This ambiguity can be very hard to
cs-410_1_1_213,"00:13:23,330","00:13:27,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,where you have to use a lot of knowledge
cs-410_1_1_214,"00:13:27,940","00:13:33,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"from the background, in order to figure"
cs-410_1_1_215,"00:13:33,310","00:13:37,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So although the sentence looks very
cs-410_1_1_216,"00:13:37,730","00:13:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And in cases when the sentence is
cs-410_1_1_217,"00:13:42,380","00:13:46,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"five prepositional phrases, and there"
cs-410_1_1_218,"00:13:48,580","00:13:51,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,It's also harder to do precise
cs-410_1_1_219,"00:13:51,650","00:13:53,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,So here's an example.
cs-410_1_1_220,"00:13:53,410","00:14:00,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"In the sentence ""John owns a restaurant."""
cs-410_1_1_221,"00:14:00,108","00:14:05,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,"The word own,"
cs-410_1_1_222,"00:14:05,340","00:14:10,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,it's very hard to precisely describe
cs-410_1_1_223,"00:14:11,430","00:14:16,467",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,So as a result we have a robust and
cs-410_1_1_224,"00:14:16,467","00:14:20,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,Natural Language Processing techniques
cs-410_1_1_225,"00:14:22,490","00:14:25,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,"In a shallow way,"
cs-410_1_1_226,"00:14:25,640","00:14:33,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"For example, parts of speech tagging or a"
cs-410_1_1_227,"00:14:33,600","00:14:35,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,"And those are not deep understanding,"
cs-410_1_1_228,"00:14:35,520","00:14:39,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,because we're not really understanding
cs-410_1_1_229,"00:14:41,270","00:14:45,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,On the other hand of the deep
cs-410_1_1_230,"00:14:45,170","00:14:50,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,"up well, meaning that they would"
cs-410_1_1_231,"00:14:50,840","00:14:54,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,And if you don't restrict
cs-410_1_1_232,"00:14:54,850","00:14:59,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"the use of words, then these"
cs-410_1_1_233,"00:14:59,750","00:15:04,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,They may work well based on machine
cs-410_1_1_234,"00:15:04,310","00:15:08,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,that are similar to the training data
cs-410_1_1_235,"00:15:08,520","00:15:13,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,But they generally wouldn't work well on
cs-410_1_1_236,"00:15:13,090","00:15:14,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,the training data.
cs-410_1_1_237,"00:15:14,290","00:15:19,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,So this pretty much summarizes the state
cs-410_1_1_238,"00:15:19,150","00:15:23,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,"Of course, within such a short amount"
cs-410_1_1_239,"00:15:23,590","00:15:27,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"a complete view of NLP,"
cs-410_1_1_240,"00:15:27,120","00:15:35,896",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And I'd expect to see multiple courses on
cs-410_1_1_241,"00:15:35,896","00:15:40,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,But because of its relevance to the topic
cs-410_1_1_242,"00:15:40,960","00:15:45,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,you to know the background in case
cs-410_1_1_243,"00:15:45,410","00:15:47,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,So what does that mean for Text Retrieval?
cs-410_1_1_244,"00:15:48,980","00:15:53,254",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"Well, in Text Retrieval we"
cs-410_1_1_245,"00:15:53,254","00:15:56,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,It's very hard to restrict
cs-410_1_1_246,"00:15:56,470","00:16:00,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=956,And we also are often dealing
cs-410_1_1_247,"00:16:00,092","00:16:06,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,So that means The NLP techniques must
cs-410_1_1_248,"00:16:06,730","00:16:12,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,And that just implies today we can only
cs-410_1_1_249,"00:16:12,060","00:16:13,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,text retrieval.
cs-410_1_1_250,"00:16:13,550","00:16:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,"In fact,"
cs-410_1_1_251,"00:16:14,780","00:16:19,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,most search engines today use something
cs-410_1_1_252,"00:16:20,740","00:16:25,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"Now, this is probably the simplest"
cs-410_1_1_253,"00:16:25,450","00:16:29,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,That is to turn text data
cs-410_1_1_254,"00:16:29,250","00:16:33,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"Meaning we'll keep individual words, but"
cs-410_1_1_255,"00:16:33,930","00:16:37,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,And we'll keep duplicated
cs-410_1_1_256,"00:16:37,660","00:16:39,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,So this is called a bag
cs-410_1_1_257,"00:16:39,950","00:16:45,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=999,"When you represent text in this way,"
cs-410_1_1_258,"00:16:45,990","00:16:51,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,That just makes it harder to understand
cs-410_1_1_259,"00:16:51,020","00:16:52,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,because we've lost the order.
cs-410_1_1_260,"00:16:53,870","00:16:57,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,But yet this representation tends
cs-410_1_1_261,"00:16:57,320","00:16:59,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,most search tasks.
cs-410_1_1_262,"00:16:59,150","00:17:03,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,And this was partly because the search
cs-410_1_1_263,"00:17:03,450","00:17:08,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,If you see matching of some of
cs-410_1_1_264,"00:17:08,230","00:17:12,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,chances are that that document is about
cs-410_1_1_265,"00:17:13,670","00:17:15,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,"So in comparison of some other tasks, for"
cs-410_1_1_266,"00:17:15,775","00:17:20,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"example, machine translation would require"
cs-410_1_1_267,"00:17:20,490","00:17:22,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,Otherwise the translation would be wrong.
cs-410_1_1_268,"00:17:22,680","00:17:25,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1042,So in comparison such tasks
cs-410_1_1_269,"00:17:25,780","00:17:30,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,Such a representation is often sufficient
cs-410_1_1_270,"00:17:30,670","00:17:34,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,"the major search engines today,"
cs-410_1_1_271,"00:17:35,770","00:17:40,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"Of course, I put in parentheses but"
cs-410_1_1_272,"00:17:40,240","00:17:42,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,that are not answered well by
cs-410_1_1_273,"00:17:42,750","00:17:48,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1062,they do require the replantation that
cs-410_1_1_274,"00:17:48,320","00:17:51,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1068,That would require more natural
cs-410_1_1_275,"00:17:52,950","00:17:56,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1072,There was another reason why we
cs-410_1_1_276,"00:17:56,600","00:17:59,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1076,NLP techniques in modern search engines.
cs-410_1_1_277,"00:17:59,100","00:18:02,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,And that's because some
cs-410_1_1_278,"00:18:02,460","00:18:05,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1082,naturally solved the problem of NLP.
cs-410_1_1_279,"00:18:05,400","00:18:09,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,So one example is word
cs-410_1_1_280,"00:18:09,240","00:18:11,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1089,Think about a word like Java.
cs-410_1_1_281,"00:18:11,060","00:18:13,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,It could mean coffee or
cs-410_1_1_282,"00:18:15,090","00:18:18,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,"If you look at the word anome,"
cs-410_1_1_283,"00:18:18,230","00:18:23,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1098,"when the user uses the word in the query,"
cs-410_1_1_284,"00:18:23,050","00:18:26,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1103,"For example, I'm looking for"
cs-410_1_1_285,"00:18:26,240","00:18:31,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,"When I have applet there,"
cs-410_1_1_286,"00:18:31,990","00:18:36,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1111,And that contest can help us
cs-410_1_1_287,"00:18:36,360","00:18:39,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,which Java is referring
cs-410_1_1_288,"00:18:39,690","00:18:43,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,Because those documents would
cs-410_1_1_289,"00:18:43,710","00:18:48,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,If Java occurs in that
cs-410_1_1_290,"00:18:48,560","00:18:52,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,then you would never match applet or
cs-410_1_1_291,"00:18:52,960","00:18:56,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1132,So this is the case when
cs-410_1_1_292,"00:18:56,250","00:18:58,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1136,naturally achieve the goal of word.
cs-410_1_1_293,"00:19:01,530","00:19:05,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,Another example is some technique called
cs-410_1_1_294,"00:19:05,920","00:19:11,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1145,feedback which we will talk about
cs-410_1_1_295,"00:19:11,360","00:19:16,938",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,This technique would allow us to add
cs-410_1_1_296,"00:19:16,938","00:19:21,859",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,those additional words could
cs-410_1_1_297,"00:19:21,859","00:19:26,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1161,And these words can help matching
cs-410_1_1_298,"00:19:26,155","00:19:27,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1166,have not occurred.
cs-410_1_1_299,"00:19:27,680","00:19:32,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1167,"So this achieves, to some extent,"
cs-410_1_1_300,"00:19:32,500","00:19:35,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,So those techniques also helped us
cs-410_1_1_301,"00:19:35,350","00:19:38,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1175,bypass some of the difficulties
cs-410_1_1_302,"00:19:40,530","00:19:43,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1180,"However, in the long run we still need"
cs-410_1_1_303,"00:19:43,920","00:19:47,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1183,techniques in order to improve the
cs-410_1_1_304,"00:19:47,280","00:19:50,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1187,And it's particularly needed for
cs-410_1_1_305,"00:19:52,160","00:19:53,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1192,Or for question and answering.
cs-410_1_1_306,"00:19:55,310","00:20:00,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,Google has recently launched a knowledge
cs-410_1_1_307,"00:20:00,540","00:20:05,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1200,"that goal, because knowledge graph would"
cs-410_1_1_308,"00:20:05,220","00:20:09,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,And this goes beyond the simple
cs-410_1_1_309,"00:20:09,170","00:20:12,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,And such technique should help us
cs-410_1_1_310,"00:20:14,180","00:20:19,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1214,"significantly, although this is the open"
cs-410_1_1_311,"00:20:19,220","00:20:24,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1219,"In sum, in this lecture we"
cs-410_1_1_312,"00:20:24,990","00:20:27,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1224,we've talked about the state
cs-410_1_1_313,"00:20:27,820","00:20:30,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1227,"What we can do, what we cannot do."
cs-410_1_1_314,"00:20:30,550","00:20:34,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,"And finally, we also explain why"
cs-410_1_1_315,"00:20:34,510","00:20:38,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1234,remains the dominant replantation
cs-410_1_1_316,"00:20:38,290","00:20:43,258",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1238,even though deeper NLP would be needed for
cs-410_1_1_317,"00:20:43,258","00:20:46,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1243,"If you want to know more, you can take"
cs-410_1_1_318,"00:20:46,470","00:20:49,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1246,I only cited one here and
cs-410_1_1_319,"00:20:49,070","00:20:52,976",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1249,Thanks.
cs-410_1_1_320,"00:20:52,976","00:21:02,976",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1252,[MUSIC]
cs-410_4_1_1,"00:00:00,000","00:00:07,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_1_2,"00:00:07,569","00:00:10,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is a overview of
cs-410_4_1_3,"00:00:13,630","00:00:17,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In the previous lecture, we introduced"
cs-410_4_1_4,"00:00:17,820","00:00:20,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,We explained that the main problem
cs-410_4_1_5,"00:00:20,330","00:00:24,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,is the design of ranking function
cs-410_4_1_6,"00:00:24,780","00:00:25,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"In this lecture,"
cs-410_4_1_7,"00:00:25,510","00:00:31,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,we will give an overview of different
cs-410_4_1_8,"00:00:33,840","00:00:35,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So the problem is the following.
cs-410_4_1_9,"00:00:35,750","00:00:39,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,We have a query that has
cs-410_4_1_10,"00:00:39,310","00:00:42,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,the document that's also
cs-410_4_1_11,"00:00:42,710","00:00:44,509",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,And we hope to define a function f
cs-410_4_1_12,"00:00:45,770","00:00:49,596",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,that can compute a score based
cs-410_4_1_13,"00:00:49,596","00:00:54,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So the main challenge you hear is with
cs-410_4_1_14,"00:00:54,870","00:01:00,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,can rank all the relevant documents
cs-410_4_1_15,"00:01:00,275","00:01:05,544",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"Clearly, this means our function"
cs-410_4_1_16,"00:01:05,544","00:01:10,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,the likelihood that a document
cs-410_4_1_17,"00:01:10,824","00:01:16,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,That also means we have to have
cs-410_4_1_18,"00:01:16,490","00:01:19,621",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"In particular, in order to"
cs-410_4_1_19,"00:01:19,621","00:01:23,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,we have to have a computational
cs-410_4_1_20,"00:01:23,380","00:01:27,232",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And we achieve this goal by
cs-410_4_1_21,"00:01:27,232","00:01:30,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,which gives us
cs-410_4_1_22,"00:01:32,650","00:01:34,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"Now, over many decades,"
cs-410_4_1_23,"00:01:34,110","00:01:38,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,researchers have designed many
cs-410_4_1_24,"00:01:38,420","00:01:40,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,And they fall into different categories.
cs-410_4_1_25,"00:01:42,290","00:01:48,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"First, one family of the models"
cs-410_4_1_26,"00:01:50,090","00:01:54,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"Basically, we assume that if"
cs-410_4_1_27,"00:01:54,170","00:01:57,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"the query than another document is,"
cs-410_4_1_28,"00:01:57,970","00:02:02,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,then we will say the first document
cs-410_4_1_29,"00:02:02,310","00:02:05,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"So in this case,"
cs-410_4_1_30,"00:02:05,330","00:02:08,572",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,the similarity between the query and
cs-410_4_1_31,"00:02:08,572","00:02:13,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,One well known example in this
cs-410_4_1_32,"00:02:13,760","00:02:17,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,which we will cover more in
cs-410_4_1_33,"00:02:20,370","00:02:24,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,A second kind of models
cs-410_4_1_34,"00:02:24,010","00:02:30,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"In this family of models, we follow a very"
cs-410_4_1_35,"00:02:30,600","00:02:35,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,queries and documents are all
cs-410_4_1_36,"00:02:36,420","00:02:41,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,And we assume there is a binary
cs-410_4_1_37,"00:02:42,370","00:02:45,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,to indicate whether a document
cs-410_4_1_38,"00:02:46,530","00:02:53,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,We then define the score of document with
cs-410_4_1_39,"00:02:53,090","00:02:59,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"this random variable R is equal to 1,"
cs-410_4_1_40,"00:02:59,780","00:03:04,363",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,There are different cases
cs-410_4_1_41,"00:03:04,363","00:03:08,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"One is classic probabilistic model,"
cs-410_4_1_42,"00:03:08,003","00:03:10,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,yet another is divergence
cs-410_4_1_43,"00:03:12,580","00:03:17,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"In a later lecture, we will talk more"
cs-410_4_1_44,"00:03:17,865","00:03:21,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,A third kind of model are based
cs-410_4_1_45,"00:03:21,740","00:03:27,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,So here the idea is to associate
cs-410_4_1_46,"00:03:27,440","00:03:31,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,and we can then quantify
cs-410_4_1_47,"00:03:31,230","00:03:34,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,show that the query
cs-410_4_1_48,"00:03:37,100","00:03:41,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"Finally, there is also a family of models"
cs-410_4_1_49,"00:03:41,940","00:03:46,237",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,that are using axiomatic thinking.
cs-410_4_1_50,"00:03:46,237","00:03:50,849",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,"Here, an idea is to define"
cs-410_4_1_51,"00:03:50,849","00:03:54,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,hope a good retrieval function to satisfy.
cs-410_4_1_52,"00:03:55,760","00:04:00,572",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"So in this case, the problem is"
cs-410_4_1_53,"00:04:00,572","00:04:04,288",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,that can satisfy all
cs-410_4_1_54,"00:04:05,867","00:04:12,326",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"Interestingly, although these different"
cs-410_4_1_55,"00:04:12,326","00:04:17,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"in the end, the retrieval function"
cs-410_4_1_56,"00:04:17,930","00:04:22,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And these functions tend to
cs-410_4_1_57,"00:04:22,020","00:04:28,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,So now let's take a look at the common
cs-410_4_1_58,"00:04:28,010","00:04:32,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,and to examine some of the common
cs-410_4_1_59,"00:04:33,900","00:04:38,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"First, these models are all"
cs-410_4_1_60,"00:04:38,810","00:04:43,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"of using bag of words to represent text,"
cs-410_4_1_61,"00:04:43,060","00:04:47,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,we explained this in the natural
cs-410_4_1_62,"00:04:47,500","00:04:51,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,Bag of words representation remains
cs-410_4_1_63,"00:04:51,450","00:04:52,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,the search engines.
cs-410_4_1_64,"00:04:53,620","00:04:57,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"So with this assumption,"
cs-410_4_1_65,"00:04:57,690","00:05:03,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,like a presidential campaign news
cs-410_4_1_66,"00:05:03,300","00:05:08,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,would be based on scores computed
cs-410_4_1_67,"00:05:09,560","00:05:15,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,And that means the score would
cs-410_4_1_68,"00:05:15,710","00:05:19,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"such as presidential, campaign, and news."
cs-410_4_1_69,"00:05:19,510","00:05:23,749",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"Here, we can see there"
cs-410_4_1_70,"00:05:23,749","00:05:29,501",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,each corresponding to how well the
cs-410_4_1_71,"00:05:31,475","00:05:36,729",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"Inside of these functions,"
cs-410_4_1_72,"00:05:38,760","00:05:43,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"So for example, one factor that"
cs-410_4_1_73,"00:05:43,770","00:05:48,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,is how many times does the word
cs-410_4_1_74,"00:05:48,570","00:05:50,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"This is called a term frequency, or TF."
cs-410_4_1_75,"00:05:51,710","00:05:56,823",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,We might also denote as
cs-410_4_1_76,"00:05:56,823","00:06:03,533",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"In general, if the word occurs"
cs-410_4_1_77,"00:06:03,533","00:06:08,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,then the value of this
cs-410_4_1_78,"00:06:08,550","00:06:13,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"Another factor is,"
cs-410_4_1_79,"00:06:13,610","00:06:18,141",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,And this is to use the document length for
cs-410_4_1_80,"00:06:18,141","00:06:22,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"In general, if a term occurs in a long"
cs-410_4_1_81,"00:06:22,910","00:06:28,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"document many times,"
cs-410_4_1_82,"00:06:28,430","00:06:32,678",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,if it occurred the same number
cs-410_4_1_83,"00:06:32,678","00:06:37,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"Because in a long document, any term"
cs-410_4_1_84,"00:06:38,980","00:06:42,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"Finally, there is this factor"
cs-410_4_1_85,"00:06:42,840","00:06:48,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"That is, we also want to look at how"
cs-410_4_1_86,"00:06:48,020","00:06:55,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"collection, and we call this document"
cs-410_4_1_87,"00:06:55,240","00:07:01,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"And in some other models,"
cs-410_4_1_88,"00:07:01,200","00:07:04,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,to characterize this information.
cs-410_4_1_89,"00:07:05,860","00:07:09,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"So here, I show the probability of"
cs-410_4_1_90,"00:07:10,830","00:07:14,564",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,So all these are trying to characterize
cs-410_4_1_91,"00:07:14,564","00:07:15,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,the collection.
cs-410_4_1_92,"00:07:15,555","00:07:20,418",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"In general, matching a rare term in"
cs-410_4_1_93,"00:07:20,418","00:07:23,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,to the overall score than
cs-410_4_1_94,"00:07:25,720","00:07:30,564",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,So this captures some of the main ideas
cs-410_4_1_95,"00:07:30,564","00:07:32,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,the art original models.
cs-410_4_1_96,"00:07:34,000","00:07:38,422",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"So now, a natural question is,"
cs-410_4_1_97,"00:07:39,834","00:07:45,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Now it turns out that many
cs-410_4_1_98,"00:07:45,080","00:07:47,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,So here are a list of
cs-410_4_1_99,"00:07:47,700","00:07:52,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,that are generally regarded as
cs-410_4_1_100,"00:07:52,463","00:07:57,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"pivoted length normalization,"
cs-410_4_1_101,"00:07:57,920","00:08:02,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"When optimized,"
cs-410_4_1_102,"00:08:02,110","00:08:08,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,And this was discussed in detail in this
cs-410_4_1_103,"00:08:08,508","00:08:13,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Among all these,"
cs-410_4_1_104,"00:08:13,130","00:08:17,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,It's most likely that this has been used
cs-410_4_1_105,"00:08:17,750","00:08:21,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,and you will also often see this
cs-410_4_1_106,"00:08:22,800","00:08:27,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And we'll talk more about this
cs-410_4_1_107,"00:08:30,430","00:08:36,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"So, to summarize, the main points made"
cs-410_4_1_108,"00:08:36,770","00:08:41,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,of a good ranking function pre-requires a
cs-410_4_1_109,"00:08:41,540","00:08:45,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,we achieve this goal by designing
cs-410_4_1_110,"00:08:47,170","00:08:52,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Second, many models are equally effective,"
cs-410_4_1_111,"00:08:52,260","00:08:55,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,Researchers are still active and
cs-410_4_1_112,"00:08:55,760","00:08:58,636",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,trying to find a truly
cs-410_4_1_113,"00:09:00,865","00:09:03,926",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"Finally, the state of the art"
cs-410_4_1_114,"00:09:03,926","00:09:05,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,to rely on the following ideas.
cs-410_4_1_115,"00:09:05,920","00:09:08,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"First, bag of words representation."
cs-410_4_1_116,"00:09:08,970","00:09:14,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"Second, TF and"
cs-410_4_1_117,"00:09:14,740","00:09:19,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,Such information is used in
cs-410_4_1_118,"00:09:19,787","00:09:25,028",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,the overall contribution of matching
cs-410_4_1_119,"00:09:25,028","00:09:29,692",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,These are often combined in interesting
cs-410_4_1_120,"00:09:29,692","00:09:34,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,exactly they are combined to rank
cs-410_4_1_121,"00:09:36,390","00:09:40,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,There are two suggested additional
cs-410_4_1_122,"00:09:41,760","00:09:45,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,The first is a paper where you can
cs-410_4_1_123,"00:09:45,150","00:09:48,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,comparison of multiple
cs-410_4_1_124,"00:09:49,840","00:09:54,674",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,The second is a book with
cs-410_4_1_125,"00:09:54,674","00:09:58,507",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,review of different retrieval models.
cs-410_4_1_126,"00:09:58,507","00:10:08,507",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,[MUSIC]
cs-410_2_1_1,"00:00:00,012","00:00:09,434",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_1_2,"00:00:09,434","00:00:12,223",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,"In this lecture,"
cs-410_2_1_3,"00:00:14,279","00:00:18,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In the previous lecture, we talked about"
cs-410_2_1_4,"00:00:19,360","00:00:23,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We explained that the state of the are
cs-410_2_1_5,"00:00:23,970","00:00:28,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,are still not good enough to process
cs-410_2_1_6,"00:00:28,970","00:00:30,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,in a robust manner.
cs-410_2_1_7,"00:00:30,550","00:00:31,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"As a result,"
cs-410_2_1_8,"00:00:31,360","00:00:37,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,bag of words remains very popular in
cs-410_2_1_9,"00:00:39,140","00:00:44,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"In this lecture, we're going to talk"
cs-410_2_1_10,"00:00:44,100","00:00:48,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,help users get access to the text data.
cs-410_2_1_11,"00:00:48,120","00:00:55,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,This is also important step to convert
cs-410_2_1_12,"00:00:55,000","00:00:57,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,That are actually needed
cs-410_2_1_13,"00:00:57,610","00:01:02,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,"So the main question we'll address here,"
cs-410_2_1_14,"00:01:02,510","00:01:07,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"can a text information system, help users"
cs-410_2_1_15,"00:01:07,450","00:01:11,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,We're going to cover two complimentary
cs-410_2_1_16,"00:01:12,610","00:01:17,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,And then we're going to talk about
cs-410_2_1_17,"00:01:17,700","00:01:19,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,querying versus browsing.
cs-410_2_1_18,"00:01:20,770","00:01:22,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,So first push versus pull.
cs-410_2_1_19,"00:01:24,500","00:01:29,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,These are two different ways connect
cs-410_2_1_20,"00:01:29,250","00:01:29,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,at the right time.
cs-410_2_1_21,"00:01:31,190","00:01:35,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,The difference is which
cs-410_2_1_22,"00:01:37,230","00:01:38,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,which party takes the initiative.
cs-410_2_1_23,"00:01:40,290","00:01:41,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"In the pull mode,"
cs-410_2_1_24,"00:01:41,380","00:01:46,439",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,the users take the initiative to
cs-410_2_1_25,"00:01:47,700","00:01:53,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And in this case, a user typically would"
cs-410_2_1_26,"00:01:53,420","00:01:56,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,"For example,"
cs-410_2_1_27,"00:01:56,100","00:02:01,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,then browse the results to
cs-410_2_1_28,"00:02:02,790","00:02:06,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,So this is usually appropriate for
cs-410_2_1_29,"00:02:06,280","00:02:09,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,satisfying a user's ad
cs-410_2_1_30,"00:02:10,580","00:02:14,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,An ad hoc information need is
cs-410_2_1_31,"00:02:14,280","00:02:17,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"For example, you want to buy a product so"
cs-410_2_1_32,"00:02:17,870","00:02:22,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,you suddenly have a need to read
cs-410_2_1_33,"00:02:22,550","00:02:26,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"But after you have cracked information,"
cs-410_2_1_34,"00:02:26,620","00:02:28,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,You generally no longer
cs-410_2_1_35,"00:02:28,880","00:02:30,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,it's a temporary information need.
cs-410_2_1_36,"00:02:31,360","00:02:35,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"In such a case, it's very hard for"
cs-410_2_1_37,"00:02:35,230","00:02:39,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,it's more proper for
cs-410_2_1_38,"00:02:39,480","00:02:42,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,that's why search engines are very useful.
cs-410_2_1_39,"00:02:42,260","00:02:48,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,Today because many people have many
cs-410_2_1_40,"00:02:48,370","00:02:52,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,So as we're speaking Google is probably
cs-410_2_1_41,"00:02:52,620","00:02:55,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"And those are all, or mostly adequate."
cs-410_2_1_42,"00:02:55,720","00:02:56,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,Information needs.
cs-410_2_1_43,"00:02:57,950","00:02:59,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,So this is a pull mode.
cs-410_2_1_44,"00:02:59,680","00:03:03,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,In contrast in the push mode in
cs-410_2_1_45,"00:03:03,570","00:03:07,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,to push the information to the user or
cs-410_2_1_46,"00:03:07,510","00:03:11,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,So in this case this is usually
cs-410_2_1_47,"00:03:13,100","00:03:15,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,Now this would be appropriate if.
cs-410_2_1_48,"00:03:15,190","00:03:16,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,The user has a stable information.
cs-410_2_1_49,"00:03:17,900","00:03:22,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,For example you may have a research
cs-410_2_1_50,"00:03:22,040","00:03:24,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,that interest tends to stay for a while.
cs-410_2_1_51,"00:03:24,980","00:03:26,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"So, it's rather stable."
cs-410_2_1_52,"00:03:26,930","00:03:29,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,Your hobby is another example of.
cs-410_2_1_53,"00:03:29,240","00:03:34,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,A stable information need is such a case
cs-410_2_1_54,"00:03:34,100","00:03:38,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"can learn your interest, and"
cs-410_2_1_55,"00:03:38,860","00:03:43,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,If the system hasn't seen any
cs-410_2_1_56,"00:03:43,710","00:03:47,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,the system could then take the initiative
cs-410_2_1_57,"00:03:47,900","00:03:49,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"So, for example, a news filter or"
cs-410_2_1_58,"00:03:49,940","00:03:53,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,news recommended system could
cs-410_2_1_59,"00:03:53,020","00:03:56,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,identify interesting news to you and
cs-410_2_1_60,"00:03:59,130","00:04:03,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,This mode of information access may be
cs-410_2_1_61,"00:04:03,960","00:04:08,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,has good knowledge about the users need
cs-410_2_1_62,"00:04:08,790","00:04:11,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"So for example, when you search for"
cs-410_2_1_63,"00:04:11,850","00:04:16,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,a search engine might infer you might be
cs-410_2_1_64,"00:04:16,130","00:04:17,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,Formation.
cs-410_2_1_65,"00:04:17,530","00:04:20,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And they would recommend the information
cs-410_2_1_66,"00:04:20,950","00:04:24,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"example, of an advertisement"
cs-410_2_1_67,"00:04:27,790","00:04:34,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So this is about the two high level
cs-410_2_1_68,"00:04:35,720","00:04:38,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,Now let's look at the pull
cs-410_2_1_69,"00:04:39,900","00:04:43,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"In the pull mode, we can further"
cs-410_2_1_70,"00:04:43,740","00:04:46,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,Querying versus browsing.
cs-410_2_1_71,"00:04:46,010","00:04:48,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"In querying,"
cs-410_2_1_72,"00:04:48,790","00:04:50,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"Typical the keyword query, and"
cs-410_2_1_73,"00:04:50,560","00:04:53,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,the search engine system would
cs-410_2_1_74,"00:04:54,500","00:05:00,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,And this works well when the user knows
cs-410_2_1_75,"00:05:00,730","00:05:02,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,So if you know exactly
cs-410_2_1_76,"00:05:02,450","00:05:04,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,you tend to know the right keywords.
cs-410_2_1_77,"00:05:04,540","00:05:07,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"And then query works very well,"
cs-410_2_1_78,"00:05:09,290","00:05:12,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,But we also know that sometimes
cs-410_2_1_79,"00:05:12,740","00:05:16,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,When you don't know the right
cs-410_2_1_80,"00:05:16,970","00:05:21,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,you want to browse information
cs-410_2_1_81,"00:05:21,760","00:05:24,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,You use because browsing
cs-410_2_1_82,"00:05:24,780","00:05:29,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"So in this case, in the case of browsing,"
cs-410_2_1_83,"00:05:29,890","00:05:33,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,into the relevant information
cs-410_2_1_84,"00:05:34,740","00:05:39,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,supported by the structures of documents.
cs-410_2_1_85,"00:05:39,850","00:05:42,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,So the system would maintain
cs-410_2_1_86,"00:05:42,690","00:05:45,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,then the user could follow
cs-410_2_1_87,"00:05:47,370","00:05:53,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,So this really works well when the user
cs-410_2_1_88,"00:05:53,850","00:05:59,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,or the user doesn't know what
cs-410_2_1_89,"00:05:59,750","00:06:05,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,Or simply because the user finds it
cs-410_2_1_90,"00:06:05,070","00:06:10,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,So even if a user knows what query to
cs-410_2_1_91,"00:06:10,450","00:06:12,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,to search for information.
cs-410_2_1_92,"00:06:12,370","00:06:14,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,It's still harder to enter the query.
cs-410_2_1_93,"00:06:14,760","00:06:18,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"In such a case, again,"
cs-410_2_1_94,"00:06:18,840","00:06:23,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,The relationship between browsing and
cs-410_2_1_95,"00:06:23,060","00:06:24,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,imagine you're site seeing.
cs-410_2_1_96,"00:06:25,230","00:06:27,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,Imagine if you're touring a city.
cs-410_2_1_97,"00:06:27,080","00:06:29,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Now if you know the exact
cs-410_2_1_98,"00:06:31,670","00:06:34,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,Taking a taxi there is
cs-410_2_1_99,"00:06:34,900","00:06:36,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,You can go directly to the site.
cs-410_2_1_100,"00:06:36,860","00:06:40,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"But if you don't know the exact address,"
cs-410_2_1_101,"00:06:40,440","00:06:43,579",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,Or you can take a taxi to a nearby
cs-410_2_1_102,"00:06:44,670","00:06:48,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,It turns out that we do exactly
cs-410_2_1_103,"00:06:48,160","00:06:51,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,If you know exactly what you
cs-410_2_1_104,"00:06:51,480","00:06:55,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,use the right keywords in your query
cs-410_2_1_105,"00:06:55,360","00:06:58,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"That's usually the fastest way to do,"
cs-410_2_1_106,"00:06:59,480","00:07:02,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,But what if you don't know
cs-410_2_1_107,"00:07:02,180","00:07:04,369",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"Well, you clearly probably won't so well."
cs-410_2_1_108,"00:07:04,369","00:07:06,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,You will not related pages.
cs-410_2_1_109,"00:07:06,150","00:07:10,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"And then, you need to also walk"
cs-410_2_1_110,"00:07:10,160","00:07:14,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,meaning by following the links or
cs-410_2_1_111,"00:07:14,110","00:07:16,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,You can then finally get
cs-410_2_1_112,"00:07:17,580","00:07:20,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,If you want to learn about again.
cs-410_2_1_113,"00:07:20,720","00:07:24,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,You will likely do a lot of browsing so
cs-410_2_1_114,"00:07:24,610","00:07:29,914",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,just like you are looking around in
cs-410_2_1_115,"00:07:29,914","00:07:36,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,interesting attractions
cs-410_2_1_116,"00:07:36,405","00:07:39,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,[INAUDIBLE].
cs-410_2_1_117,"00:07:39,200","00:07:45,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,So this analogy also tells us that
cs-410_2_1_118,"00:07:45,330","00:07:50,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"query, but we don't really have"
cs-410_2_1_119,"00:07:50,600","00:07:54,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,And this is because in order
cs-410_2_1_120,"00:07:54,470","00:07:57,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"we need a map to guide us,"
cs-410_2_1_121,"00:07:57,840","00:07:58,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"Of Chicago,"
cs-410_2_1_122,"00:07:58,410","00:08:04,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"through the city of Chicago, you need a"
cs-410_2_1_123,"00:08:04,060","00:08:08,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So how to construct such a topical
cs-410_2_1_124,"00:08:08,190","00:08:12,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,research question that might bring us
cs-410_2_1_125,"00:08:12,730","00:08:16,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,more interesting browsing experience
cs-410_2_1_126,"00:08:19,170","00:08:21,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"So, to summarize this lecture,"
cs-410_2_1_127,"00:08:21,280","00:08:26,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,we've talked about the two high level
cs-410_2_1_128,"00:08:26,550","00:08:29,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,Push tends to be supported by
cs-410_2_1_129,"00:08:29,130","00:08:31,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,Pull tends to be supported
cs-410_2_1_130,"00:08:31,770","00:08:35,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"Of course, in the sophisticated"
cs-410_2_1_131,"00:08:35,710","00:08:36,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,we should combine the two.
cs-410_2_1_132,"00:08:38,590","00:08:41,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,"In the pull mode, we can further this"
cs-410_2_1_133,"00:08:41,830","00:08:47,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,Again we generally want to combine
cs-410_2_1_134,"00:08:47,140","00:08:50,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,so that you can support
cs-410_2_1_135,"00:08:51,220","00:08:55,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,If you want to know more about
cs-410_2_1_136,"00:08:55,420","00:08:58,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"push, you can read this article."
cs-410_2_1_137,"00:08:58,600","00:09:03,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,This give excellent discussion of the
cs-410_2_1_138,"00:09:03,560","00:09:05,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,information retrieval.
cs-410_2_1_139,"00:09:05,330","00:09:10,271",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,Here informational filtering is similar
cs-410_2_1_140,"00:09:10,271","00:09:12,749",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,the push mode of information access.
cs-410_2_1_141,"00:09:12,749","00:09:22,749",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,[MUSIC]
cs-410_6_1_1,"00:00:00,008","00:00:05,424",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,In this lecture we're going to talk
cs-410_6_1_2,"00:00:05,424","00:00:12,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,about how to instantiate
cs-410_6_1_3,"00:00:12,465","00:00:19,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,that we can get very
cs-410_6_1_4,"00:00:22,974","00:00:27,888",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So this is to continue the discussion
cs-410_6_1_5,"00:00:27,888","00:00:32,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,which is one particular approach
cs-410_6_1_6,"00:00:34,420","00:00:38,551",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,And we're going to talk about how
cs-410_6_1_7,"00:00:38,551","00:00:42,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,the the vector space
cs-410_6_1_8,"00:00:42,810","00:00:48,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,instantiate the framework to derive
cs-410_6_1_9,"00:00:48,270","00:00:53,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And we're going to cover the symbolist
cs-410_6_1_10,"00:00:55,360","00:00:58,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,So as we discussed in
cs-410_6_1_11,"00:00:58,390","00:01:00,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,the vector space model
cs-410_6_1_12,"00:01:00,600","00:01:02,619",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,And this didn't say.
cs-410_6_1_13,"00:01:05,266","00:01:11,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"As we discussed in the previous lecture,"
cs-410_6_1_14,"00:01:11,040","00:01:13,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,It does not say many things.
cs-410_6_1_15,"00:01:14,710","00:01:15,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"So, for example,"
cs-410_6_1_16,"00:01:15,520","00:01:19,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,here it shows that it did not say
cs-410_6_1_17,"00:01:20,770","00:01:25,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,It also did not say how we place
cs-410_6_1_18,"00:01:27,130","00:01:31,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,It did not say how we place a query
cs-410_6_1_19,"00:01:32,500","00:01:37,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"And, finally, it did not say how we"
cs-410_6_1_20,"00:01:37,250","00:01:39,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,the query vector and the document vector.
cs-410_6_1_21,"00:01:40,570","00:01:44,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"So you can imagine,"
cs-410_6_1_22,"00:01:46,040","00:01:52,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,we have to say specifically
cs-410_6_1_23,"00:01:52,940","00:01:54,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,What is exactly xi?
cs-410_6_1_24,"00:01:54,830","00:01:56,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,And what is exactly yi?
cs-410_6_1_25,"00:01:58,460","00:02:02,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,This will determine where
cs-410_6_1_26,"00:02:02,260","00:02:04,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,where we place a query vector.
cs-410_6_1_27,"00:02:04,560","00:02:05,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"And, of course,"
cs-410_6_1_28,"00:02:05,230","00:02:08,869",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,we also need to say exactly what
cs-410_6_1_29,"00:02:11,120","00:02:16,653",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So if we can provide a definition
cs-410_6_1_30,"00:02:16,653","00:02:22,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,the dimensions and these xi's or
cs-410_6_1_31,"00:02:22,590","00:02:28,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"queries and document, then we will be"
cs-410_6_1_32,"00:02:28,725","00:02:33,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,query vectors in this well defined space.
cs-410_6_1_33,"00:02:33,080","00:02:36,414",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,"And then,"
cs-410_6_1_34,"00:02:36,414","00:02:39,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,then we'll have a well
cs-410_6_1_35,"00:02:41,427","00:02:47,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,So let's see how we can do that and
cs-410_6_1_36,"00:02:47,630","00:02:52,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"Actually, I would suggest you to"
cs-410_6_1_37,"00:02:52,460","00:02:54,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,spend a couple minutes to think about.
cs-410_6_1_38,"00:02:54,980","00:02:58,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,Suppose you are asked
cs-410_6_1_39,"00:02:59,590","00:03:05,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,You have come up with the idea of vector
cs-410_6_1_40,"00:03:05,810","00:03:10,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"out how to compute these vectors exactly,"
cs-410_6_1_41,"00:03:10,310","00:03:10,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,What would you do?
cs-410_6_1_42,"00:03:12,540","00:03:15,857",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"So, think for a couple of minutes,"
cs-410_6_1_43,"00:03:20,581","00:03:26,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"So, let's think about some simplest ways"
cs-410_6_1_44,"00:03:26,460","00:03:28,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"First, how do we define the dimension?"
cs-410_6_1_45,"00:03:28,810","00:03:31,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"Well, the obvious choice is to use"
cs-410_6_1_46,"00:03:31,430","00:03:34,636",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,each word in our vocabulary
cs-410_6_1_47,"00:03:34,636","00:03:38,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,And show that there are N
cs-410_6_1_48,"00:03:38,775","00:03:41,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Therefore, there are N dimensions."
cs-410_6_1_49,"00:03:41,160","00:03:42,818",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,Each word defines one dimension.
cs-410_6_1_50,"00:03:42,818","00:03:46,273",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,And this is basically
cs-410_6_1_51,"00:03:48,965","00:03:52,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,Now let's look at how we
cs-410_6_1_52,"00:03:54,395","00:03:57,175",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"Again here, the simplest strategy is to"
cs-410_6_1_53,"00:03:58,700","00:04:03,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,use a Bit Vector to represent
cs-410_6_1_54,"00:04:04,720","00:04:07,937",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"And that means each element, xi and"
cs-410_6_1_55,"00:04:07,937","00:04:12,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,yi will be taking a value
cs-410_6_1_56,"00:04:13,270","00:04:14,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"When it's 1,"
cs-410_6_1_57,"00:04:14,300","00:04:20,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,it means the corresponding word is
cs-410_6_1_58,"00:04:20,750","00:04:25,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"When it's 0,"
cs-410_6_1_59,"00:04:27,070","00:04:31,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So you can imagine if the user
cs-410_6_1_60,"00:04:31,210","00:04:35,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,then the query vector will only
cs-410_6_1_61,"00:04:37,630","00:04:41,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"The document vector,"
cs-410_6_1_62,"00:04:41,450","00:04:46,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,But it will also have many zeros since
cs-410_6_1_63,"00:04:46,700","00:04:50,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,Many words don't really
cs-410_6_1_64,"00:04:52,110","00:04:56,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,Many words will only occasionally
cs-410_6_1_65,"00:04:58,680","00:05:01,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,A lot of words will be absent
cs-410_6_1_66,"00:05:04,390","00:05:09,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,So now we have placed the documents and
cs-410_6_1_67,"00:05:11,450","00:05:14,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,Let's look at how we
cs-410_6_1_68,"00:05:15,770","00:05:19,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"So, a commonly used similarity"
cs-410_6_1_69,"00:05:20,900","00:05:25,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,The Dot Product of two
cs-410_6_1_70,"00:05:25,590","00:05:30,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,the sum of the products of the
cs-410_6_1_71,"00:05:30,590","00:05:38,596",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"So, here we see that it's"
cs-410_6_1_72,"00:05:38,596","00:05:40,228",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"So, here."
cs-410_6_1_73,"00:05:40,228","00:05:43,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"And then, x2 multiplied by y2."
cs-410_6_1_74,"00:05:43,420","00:05:47,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"And then, finally, xn multiplied by yn."
cs-410_6_1_75,"00:05:47,100","00:05:48,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"And then, we take a sum here."
cs-410_6_1_76,"00:05:50,630","00:05:52,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,So that's a Dot Product.
cs-410_6_1_77,"00:05:52,610","00:05:57,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"Now, we can represent this in a more"
cs-410_6_1_78,"00:05:58,740","00:06:04,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,So this is only one of the many different
cs-410_6_1_79,"00:06:04,120","00:06:10,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"So, now we see that we have"
cs-410_6_1_80,"00:06:10,640","00:06:16,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"we have defined the vectors, and we have"
cs-410_6_1_81,"00:06:16,050","00:06:21,495",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,So now we finally have the simplest
cs-410_6_1_82,"00:06:21,495","00:06:26,882",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,on the bit vector [INAUDIBLE] dot product
cs-410_6_1_83,"00:06:26,882","00:06:30,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,And the formula looks like this.
cs-410_6_1_84,"00:06:30,195","00:06:32,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,So this is our formula.
cs-410_6_1_85,"00:06:32,415","00:06:37,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,And that's actually a particular retrieval
cs-410_6_1_86,"00:06:37,670","00:06:42,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,Now we can finally implement this
cs-410_6_1_87,"00:06:42,573","00:06:45,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,and then rank the documents for query.
cs-410_6_1_88,"00:06:45,350","00:06:50,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"Now, at this point you should"
cs-410_6_1_89,"00:06:50,110","00:06:53,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,to think about how we can
cs-410_6_1_90,"00:06:53,400","00:06:56,972",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,"So, we have gone through the process"
cs-410_6_1_91,"00:06:56,972","00:07:00,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,using a vector space model.
cs-410_6_1_92,"00:07:00,620","00:07:05,185",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"And then,"
cs-410_6_1_93,"00:07:05,185","00:07:09,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"vectors in the vector space, and"
cs-410_6_1_94,"00:07:09,780","00:07:14,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,"So in the end, we've got a specific"
cs-410_6_1_95,"00:07:15,370","00:07:18,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"Now, the next step is to think about"
cs-410_6_1_96,"00:07:18,370","00:07:21,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"actually makes sense, right?"
cs-410_6_1_97,"00:07:21,160","00:07:24,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,Can we expect this function
cs-410_6_1_98,"00:07:24,140","00:07:27,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,when we used it to rank documents for
cs-410_6_1_99,"00:07:28,790","00:07:35,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So it's worth thinking about what is
cs-410_6_1_100,"00:07:35,870","00:07:38,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"So, in the end, we'll get a number."
cs-410_6_1_101,"00:07:38,220","00:07:40,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,But what does this number mean?
cs-410_6_1_102,"00:07:40,240","00:07:40,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,Is it meaningful?
cs-410_6_1_103,"00:07:42,200","00:07:44,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,"So, spend a couple minutes"
cs-410_6_1_104,"00:07:45,880","00:07:46,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"And, of course,"
cs-410_6_1_105,"00:07:46,540","00:07:52,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,the general question here is do you
cs-410_6_1_106,"00:07:52,600","00:07:54,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,Would it actually work well?
cs-410_6_1_107,"00:07:54,680","00:07:58,329",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"So, again,"
cs-410_6_1_108,"00:07:58,329","00:08:00,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,Is it actually meaningful?
cs-410_6_1_109,"00:08:01,280","00:08:03,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,Does it mean something?
cs-410_6_1_110,"00:08:03,190","00:08:06,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,This is related to how well
cs-410_6_1_111,"00:08:08,260","00:08:11,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"So, in order to assess"
cs-410_6_1_112,"00:08:11,530","00:08:15,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"vector space model actually works well,"
cs-410_6_1_113,"00:08:17,170","00:08:22,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,"So, here I show some sample documents and"
cs-410_6_1_114,"00:08:22,570","00:08:26,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,The query is news about
cs-410_6_1_115,"00:08:26,390","00:08:28,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,And we have five documents here.
cs-410_6_1_116,"00:08:28,580","00:08:32,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,They cover different terms in the query.
cs-410_6_1_117,"00:08:34,710","00:08:39,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,And if you look at these documents for
cs-410_6_1_118,"00:08:41,880","00:08:47,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,"some documents are probably relevant, and"
cs-410_6_1_119,"00:08:48,300","00:08:54,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"Now, if I asked you to rank these"
cs-410_6_1_120,"00:08:54,690","00:08:57,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,This is basically our ideal ranking.
cs-410_6_1_121,"00:08:57,270","00:09:01,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,"When humans can examine the documents,"
cs-410_6_1_122,"00:09:03,430","00:09:06,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"Now, so think for a moment,"
cs-410_6_1_123,"00:09:06,900","00:09:10,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,And perhaps by pausing the lecture.
cs-410_6_1_124,"00:09:12,510","00:09:18,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,So I think most of you would
cs-410_6_1_125,"00:09:18,750","00:09:23,353",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,better than others because they
cs-410_6_1_126,"00:09:23,353","00:09:26,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"They match news,"
cs-410_6_1_127,"00:09:27,900","00:09:33,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"So, it looks like these documents"
cs-410_6_1_128,"00:09:33,160","00:09:37,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,They should be ranked on top.
cs-410_6_1_129,"00:09:37,230","00:09:41,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"And the other three d2, d1, and"
cs-410_6_1_130,"00:09:41,810","00:09:45,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,So we can also say d4 and
cs-410_6_1_131,"00:09:45,990","00:09:50,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"d1, d2 and d5 are non-relevant."
cs-410_6_1_132,"00:09:50,150","00:09:55,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,So now let's see if our simplest
cs-410_6_1_133,"00:09:55,290","00:09:57,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,or could do something closer.
cs-410_6_1_134,"00:09:57,400","00:10:01,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"So, let's first think about"
cs-410_6_1_135,"00:10:01,250","00:10:02,272",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,to score documents.
cs-410_6_1_136,"00:10:02,272","00:10:04,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,All right.
cs-410_6_1_137,"00:10:04,000","00:10:07,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"Here I show two documents, d1 and d3."
cs-410_6_1_138,"00:10:07,420","00:10:10,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,And we have the query also here.
cs-410_6_1_139,"00:10:10,390","00:10:15,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,"In the vector space model, of course we"
cs-410_6_1_140,"00:10:15,130","00:10:16,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,these documents and the query.
cs-410_6_1_141,"00:10:16,830","00:10:18,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"Now, I showed the vocabulary here as well."
cs-410_6_1_142,"00:10:18,860","00:10:22,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So these are the end dimensions
cs-410_6_1_143,"00:10:22,850","00:10:26,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,So what do you think is the vector for
cs-410_6_1_144,"00:10:27,700","00:10:32,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,Note that we're assuming
cs-410_6_1_145,"00:10:32,870","00:10:39,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,to indicate whether a term is absent or
cs-410_6_1_146,"00:10:39,230","00:10:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,"So these are zero,1 bit vectors."
cs-410_6_1_147,"00:10:43,880","00:10:45,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,So what do you think is the query vector?
cs-410_6_1_148,"00:10:47,820","00:10:51,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,"Well, the query has four words here."
cs-410_6_1_149,"00:10:51,200","00:10:54,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,"So for these four words,"
cs-410_6_1_150,"00:10:54,380","00:10:55,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"And for the rest, there will be zeros."
cs-410_6_1_151,"00:10:57,680","00:10:59,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,"Now, what about the documents?"
cs-410_6_1_152,"00:10:59,290","00:11:00,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,It's the same.
cs-410_6_1_153,"00:11:00,610","00:11:03,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"So d1 has two rows, news and about."
cs-410_6_1_154,"00:11:03,430","00:11:07,367",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,"So, there are two 1's here,"
cs-410_6_1_155,"00:11:07,367","00:11:12,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,"Similarly, so now that we"
cs-410_6_1_156,"00:11:12,220","00:11:16,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"have the two vectors,"
cs-410_6_1_157,"00:11:17,470","00:11:19,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,And we're going to use Do Product.
cs-410_6_1_158,"00:11:19,550","00:11:21,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"So you can see when we use Dot Product,"
cs-410_6_1_159,"00:11:21,610","00:11:26,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,we just multiply the corresponding
cs-410_6_1_160,"00:11:26,030","00:11:30,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,"So these two will be formal product,"
cs-410_6_1_161,"00:11:30,894","00:11:33,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,and these two will
cs-410_6_1_162,"00:11:33,920","00:11:38,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,and these two will generate yet
cs-410_6_1_163,"00:11:40,020","00:11:46,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,"Now you can easily see if we do that,"
cs-410_6_1_164,"00:11:48,180","00:11:54,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,these zeroes because whenever we have
cs-410_6_1_165,"00:11:54,170","00:11:57,538",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,So when we take a sum
cs-410_6_1_166,"00:11:57,538","00:12:02,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,then the zero entries will be gone.
cs-410_6_1_167,"00:12:04,400","00:12:08,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"As long as you have one zero,"
cs-410_6_1_168,"00:12:08,010","00:12:14,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,"So, in the fact, we're just"
cs-410_6_1_169,"00:12:14,710","00:12:18,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,"In this case, we have seen two,"
cs-410_6_1_170,"00:12:18,220","00:12:20,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,So what does that mean?
cs-410_6_1_171,"00:12:20,240","00:12:25,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,"Well, that means this number, or"
cs-410_6_1_172,"00:12:25,190","00:12:33,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,is simply the count of how many unique
cs-410_6_1_173,"00:12:33,130","00:12:39,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Because if a term is matched in the
cs-410_6_1_174,"00:12:41,390","00:12:44,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"If it's not, then there will"
cs-410_6_1_175,"00:12:46,310","00:12:50,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,"Similarly, if the document has a term but"
cs-410_6_1_176,"00:12:50,410","00:12:53,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,there will be a zero in the query vector.
cs-410_6_1_177,"00:12:53,220","00:12:55,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,So those don't count.
cs-410_6_1_178,"00:12:55,020","00:12:58,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So, as a result,"
cs-410_6_1_179,"00:12:58,760","00:13:03,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,measures how many unique query
cs-410_6_1_180,"00:13:03,820","00:13:05,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,This is how we interpret this score.
cs-410_6_1_181,"00:13:07,150","00:13:10,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=787,"Now, we can also take a look at d3."
cs-410_6_1_182,"00:13:10,520","00:13:18,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,"In this case, you can see the result"
cs-410_6_1_183,"00:13:18,003","00:13:23,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"distinctive query words news, presidential"
cs-410_6_1_184,"00:13:23,140","00:13:28,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,"Now in this case, this seems"
cs-410_6_1_185,"00:13:29,260","00:13:33,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,And this simplest vector
cs-410_6_1_186,"00:13:33,440","00:13:35,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So that looks pretty good.
cs-410_6_1_187,"00:13:35,050","00:13:40,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,"However, if we examine this model in"
cs-410_6_1_188,"00:13:40,030","00:13:44,891",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,"So, here I'm going to show all"
cs-410_6_1_189,"00:13:44,891","00:13:49,977",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,And you can easily verify they're
cs-410_6_1_190,"00:13:49,977","00:13:55,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,counting the number of unique query
cs-410_6_1_191,"00:13:56,470","00:13:59,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,Now note that this measure
cs-410_6_1_192,"00:13:59,270","00:14:03,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,It basically means if a document
cs-410_6_1_193,"00:14:03,740","00:14:07,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=843,then the document will be
cs-410_6_1_194,"00:14:07,210","00:14:09,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,And that seems to make sense.
cs-410_6_1_195,"00:14:09,190","00:14:16,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,The only problem is here we can note that
cs-410_6_1_196,"00:14:16,870","00:14:22,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,And they tied with a 3 as a score.
cs-410_6_1_197,"00:14:25,050","00:14:31,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"So, that's a problem because if you look"
cs-410_6_1_198,"00:14:31,000","00:14:36,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,should be ranked above d3 because
cs-410_6_1_199,"00:14:36,920","00:14:42,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=876,"d3 only mentions the presidential once,"
cs-410_6_1_200,"00:14:42,100","00:14:47,634",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,"In the case of d3,"
cs-410_6_1_201,"00:14:47,634","00:14:51,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,But d4 is clearly above
cs-410_6_1_202,"00:14:51,360","00:14:58,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,Another problem is that d2 and
cs-410_6_1_203,"00:14:58,200","00:15:01,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,But if you look at the three words
cs-410_6_1_204,"00:15:01,880","00:15:07,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=901,"it matched the news, about and campaign."
cs-410_6_1_205,"00:15:07,020","00:15:11,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,"But in the case of d3, it matched news,"
cs-410_6_1_206,"00:15:12,530","00:15:17,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=912,So intuitively this reads better
cs-410_6_1_207,"00:15:17,960","00:15:21,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,"is more important than matching about,"
cs-410_6_1_208,"00:15:21,920","00:15:24,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,even though about and
cs-410_6_1_209,"00:15:26,170","00:15:30,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,"So intuitively,"
cs-410_6_1_210,"00:15:30,730","00:15:32,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=930,But this model doesn't do that.
cs-410_6_1_211,"00:15:33,860","00:15:37,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,So that means this model
cs-410_6_1_212,"00:15:37,150","00:15:39,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=937,We have to solve these problems.
cs-410_6_1_213,"00:15:41,188","00:15:41,991",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,"To summarize,"
cs-410_6_1_214,"00:15:41,991","00:15:45,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,in this lecture we talked about how
cs-410_6_1_215,"00:15:47,610","00:15:49,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=947,We mainly need to do three things.
cs-410_6_1_216,"00:15:49,540","00:15:51,796",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=949,One is to define the dimension.
cs-410_6_1_217,"00:15:51,796","00:15:59,896",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=951,The second is to decide how to place
cs-410_6_1_218,"00:15:59,896","00:16:05,761",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,and to also place a query in
cs-410_6_1_219,"00:16:07,862","00:16:11,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,And third is to define
cs-410_6_1_220,"00:16:11,900","00:16:15,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,particularly the query vector and
cs-410_6_1_221,"00:16:17,080","00:16:22,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=977,We also talked about various simple way
cs-410_6_1_222,"00:16:22,430","00:16:27,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,"Indeed, that's probably the simplest"
cs-410_6_1_223,"00:16:27,910","00:16:31,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=987,"In this case,"
cs-410_6_1_224,"00:16:31,480","00:16:37,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,"We use a zero, 1 bit vector to"
cs-410_6_1_225,"00:16:37,430","00:16:42,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,"In this case, we basically only care"
cs-410_6_1_226,"00:16:42,690","00:16:43,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1002,We ignore the frequency.
cs-410_6_1_227,"00:16:45,560","00:16:49,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,And we use the Dot Product
cs-410_6_1_228,"00:16:50,360","00:16:53,304",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"And with such a instantiation,"
cs-410_6_1_229,"00:16:53,304","00:16:58,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,we showed that the scoring
cs-410_6_1_230,"00:16:58,870","00:17:03,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,a document based on the number of distinct
cs-410_6_1_231,"00:17:04,650","00:17:09,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1024,We also showed that such a simple vector
cs-410_6_1_232,"00:17:09,800","00:17:10,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1029,we need to improve it.
cs-410_6_1_233,"00:17:12,540","00:17:18,797",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1032,And this is a topic that we're
cs-410_6_1_234,"00:17:18,797","00:17:28,797",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,[MUSIC]
cs-410_5_1_1,"00:00:00,008","00:00:07,957",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_1_2,"00:00:07,957","00:00:11,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the
cs-410_5_1_3,"00:00:11,940","00:00:14,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,We're going to give
cs-410_5_1_4,"00:00:18,800","00:00:23,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In the last lecture, we talked about"
cs-410_5_1_5,"00:00:23,730","00:00:29,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"a retrieval model, which would give"
cs-410_5_1_6,"00:00:30,270","00:00:33,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"In this lecture, we're going to"
cs-410_5_1_7,"00:00:33,600","00:00:36,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,designing a ramping function called
cs-410_5_1_8,"00:00:37,760","00:00:41,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,And we're going to give a brief
cs-410_5_1_9,"00:00:44,330","00:00:47,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,Vector space model is a special case of
cs-410_5_1_10,"00:00:47,320","00:00:50,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,similarity based models
cs-410_5_1_11,"00:00:50,800","00:00:56,049",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,Which means we assume relevance
cs-410_5_1_12,"00:00:56,049","00:00:59,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,between the document and the query.
cs-410_5_1_13,"00:01:02,140","00:01:06,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,Now whether is this assumption
cs-410_5_1_14,"00:01:06,280","00:01:09,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"But in order to solve the search problem,"
cs-410_5_1_15,"00:01:09,965","00:01:15,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,we have to convert the vague notion
cs-410_5_1_16,"00:01:15,860","00:01:21,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,definition that can be implemented
cs-410_5_1_17,"00:01:21,459","00:01:26,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"So in this process,"
cs-410_5_1_18,"00:01:26,430","00:01:31,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,This is the first assumption
cs-410_5_1_19,"00:01:31,510","00:01:36,091",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"Basically, we assume that if a document"
cs-410_5_1_20,"00:01:36,091","00:01:37,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,another document.
cs-410_5_1_21,"00:01:37,419","00:01:42,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,Then the first document will be assumed it
cs-410_5_1_22,"00:01:42,070","00:01:45,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,And this is the basis for
cs-410_5_1_23,"00:01:46,800","00:01:51,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Again, it's questionable whether this is"
cs-410_5_1_24,"00:01:51,970","00:01:55,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,As we will see later there
cs-410_5_1_25,"00:01:58,300","00:01:59,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,The basic idea of vectors for
cs-410_5_1_26,"00:01:59,790","00:02:03,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,base retrieval model is actually
cs-410_5_1_27,"00:02:03,070","00:02:10,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,Imagine a high dimensional space where
cs-410_5_1_28,"00:02:11,660","00:02:17,088",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So here I issue a three dimensional
cs-410_5_1_29,"00:02:17,088","00:02:21,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"programming, library and presidential."
cs-410_5_1_30,"00:02:21,120","00:02:23,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,So each term here defines one dimension.
cs-410_5_1_31,"00:02:24,370","00:02:28,966",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"Now we can consider vectors in this,"
cs-410_5_1_32,"00:02:28,966","00:02:32,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,And we're going to assume
cs-410_5_1_33,"00:02:32,275","00:02:36,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,the query will be placed
cs-410_5_1_34,"00:02:36,340","00:02:43,526",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"So for example, on document might"
cs-410_5_1_35,"00:02:43,526","00:02:48,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,Now this means this document
cs-410_5_1_36,"00:02:48,710","00:02:54,657",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"presidential, but"
cs-410_5_1_37,"00:02:54,657","00:03:00,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,What does this mean in terms
cs-410_5_1_38,"00:03:00,270","00:03:04,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,That just means we're going to look at
cs-410_5_1_39,"00:03:04,370","00:03:05,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,this vector.
cs-410_5_1_40,"00:03:05,710","00:03:07,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,We're going to ignore everything else.
cs-410_5_1_41,"00:03:07,910","00:03:12,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"Basically, what we see here is only"
cs-410_5_1_42,"00:03:14,470","00:03:16,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,"Of course,"
cs-410_5_1_43,"00:03:16,380","00:03:20,223",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"For example, the orders of"
cs-410_5_1_44,"00:03:20,223","00:03:25,038",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,that's because we assume that
cs-410_5_1_45,"00:03:25,038","00:03:29,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So with this presentation
cs-410_5_1_46,"00:03:29,310","00:03:33,472",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,d1 simply suggests a [INAUDIBLE] library.
cs-410_5_1_47,"00:03:33,472","00:03:37,949",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,Now this is different from another
cs-410_5_1_48,"00:03:37,949","00:03:39,906",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"a different vector, d2 here."
cs-410_5_1_49,"00:03:39,906","00:03:44,319",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"Now in this case, the document that"
cs-410_5_1_50,"00:03:44,319","00:03:46,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,it doesn't talk about presidential.
cs-410_5_1_51,"00:03:46,679","00:03:48,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,So what does this remind you?
cs-410_5_1_52,"00:03:48,830","00:03:54,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,Well you can probably guess the topic
cs-410_5_1_53,"00:03:54,540","00:03:56,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,the library is software lab library.
cs-410_5_1_54,"00:03:58,366","00:04:04,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,So this shows that by using
cs-410_5_1_55,"00:04:04,110","00:04:08,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,we can actually capture the differences
cs-410_5_1_56,"00:04:09,610","00:04:12,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,Now you can also imagine
cs-410_5_1_57,"00:04:12,190","00:04:15,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"For example,"
cs-410_5_1_58,"00:04:15,296","00:04:17,632",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,that might be a presidential program.
cs-410_5_1_59,"00:04:17,632","00:04:22,649",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And in fact we can place all
cs-410_5_1_60,"00:04:22,649","00:04:26,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,And they will be pointing
cs-410_5_1_61,"00:04:26,700","00:04:27,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"And similarly,"
cs-410_5_1_62,"00:04:27,340","00:04:31,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,we're going to place our query also
cs-410_5_1_63,"00:04:32,630","00:04:37,226",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And then we're going to measure the
cs-410_5_1_64,"00:04:37,226","00:04:39,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,every document vector.
cs-410_5_1_65,"00:04:39,510","00:04:40,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"So in this case for example,"
cs-410_5_1_66,"00:04:40,740","00:04:47,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,we can easily see d2 seems to be
cs-410_5_1_67,"00:04:47,200","00:04:50,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"And therefore,"
cs-410_5_1_68,"00:04:51,900","00:04:56,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,So this is basically the main
cs-410_5_1_69,"00:04:58,320","00:05:02,455",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"So to be more precise,"
cs-410_5_1_70,"00:05:02,455","00:05:09,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,vector space model is a framework.
cs-410_5_1_71,"00:05:09,000","00:05:12,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"In this framework,"
cs-410_5_1_72,"00:05:12,510","00:05:17,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"First, we represent a document and"
cs-410_5_1_73,"00:05:18,680","00:05:21,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,So here a term can be any basic concept.
cs-410_5_1_74,"00:05:21,670","00:05:28,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,"For example, a word or a phrase or"
cs-410_5_1_75,"00:05:28,920","00:05:32,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,Those are just sequence of
cs-410_5_1_76,"00:05:34,460","00:05:37,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,Each term is assumed that will
cs-410_5_1_77,"00:05:37,400","00:05:42,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Therefore n terms in our vocabulary,"
cs-410_5_1_78,"00:05:44,060","00:05:48,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,A query vector would consist
cs-410_5_1_79,"00:05:49,610","00:05:53,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,corresponding to the weights
cs-410_5_1_80,"00:05:56,250","00:05:59,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,Each document vector is also similar.
cs-410_5_1_81,"00:05:59,540","00:06:04,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,It has a number of elements and
cs-410_5_1_82,"00:06:04,518","00:06:08,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,indicating the weight of
cs-410_5_1_83,"00:06:08,900","00:06:12,397",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"Here, you can see,"
cs-410_5_1_84,"00:06:12,397","00:06:14,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,"Therefore, they are N elements"
cs-410_5_1_85,"00:06:15,525","00:06:18,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,each corresponding to the weight
cs-410_5_1_86,"00:06:21,385","00:06:23,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,So the relevance in this case
cs-410_5_1_87,"00:06:23,860","00:06:28,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,will be assumed to be the similarity
cs-410_5_1_88,"00:06:29,420","00:06:33,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"Therefore, our ranking function"
cs-410_5_1_89,"00:06:33,500","00:06:35,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,between the query vector and
cs-410_5_1_90,"00:06:37,570","00:06:41,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,Now if I ask you to write a program
cs-410_5_1_91,"00:06:41,780","00:06:42,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,in a search engine.
cs-410_5_1_92,"00:06:44,042","00:06:48,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,You would realize that
cs-410_5_1_93,"00:06:48,248","00:06:50,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"We haven't said a lot of things in detail,"
cs-410_5_1_94,"00:06:50,750","00:06:56,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,therefore it's impossible to actually
cs-410_5_1_95,"00:06:56,080","00:06:58,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"That's why I said, this is a framework."
cs-410_5_1_96,"00:06:59,370","00:07:03,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,And this has to be refined
cs-410_5_1_97,"00:07:04,350","00:07:08,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,suggest a particular ranking function
cs-410_5_1_98,"00:07:10,890","00:07:13,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,So what does this framework not say?
cs-410_5_1_99,"00:07:13,810","00:07:17,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"Well, it actually hasn't said many things"
cs-410_5_1_100,"00:07:17,800","00:07:22,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,that would be required in order
cs-410_5_1_101,"00:07:24,420","00:07:30,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"First, it did not say how we should define"
cs-410_5_1_102,"00:07:32,580","00:07:36,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,We clearly assume
cs-410_5_1_103,"00:07:36,190","00:07:38,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"Otherwise, there will be redundancy."
cs-410_5_1_104,"00:07:38,660","00:07:45,309",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"For example, if two synonyms or somehow"
cs-410_5_1_105,"00:07:45,309","00:07:50,382",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Then they would be defining
cs-410_5_1_106,"00:07:50,382","00:07:54,299",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,that would clearly cause redundancy here.
cs-410_5_1_107,"00:07:54,299","00:07:59,036",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,Or all the emphasizing of
cs-410_5_1_108,"00:07:59,036","00:08:03,997",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,because it would be as if
cs-410_5_1_109,"00:08:03,997","00:08:08,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,when you actually matched
cs-410_5_1_110,"00:08:11,420","00:08:16,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"Secondly, it did not say how we"
cs-410_5_1_111,"00:08:16,020","00:08:18,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,the query in this space.
cs-410_5_1_112,"00:08:18,200","00:08:22,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Basically that show you some examples
cs-410_5_1_113,"00:08:22,970","00:08:27,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,But where exactly should the vector for
cs-410_5_1_114,"00:08:29,050","00:08:33,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,So this is equivalent to how
cs-410_5_1_115,"00:08:33,930","00:08:39,237",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,How do you compute the lose
cs-410_5_1_116,"00:08:39,237","00:08:41,808",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"This is a very important question,"
cs-410_5_1_117,"00:08:41,808","00:08:47,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,because term weight in the query vector
cs-410_5_1_118,"00:08:48,820","00:08:51,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"So depending on how you assign the weight,"
cs-410_5_1_119,"00:08:51,460","00:08:55,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,you might prefer some terms
cs-410_5_1_120,"00:08:56,630","00:08:59,472",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"Similarly, the total word in"
cs-410_5_1_121,"00:08:59,472","00:09:03,559",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,It indicates how well the term
cs-410_5_1_122,"00:09:03,559","00:09:08,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,If you got it wrong then you clearly
cs-410_5_1_123,"00:09:10,150","00:09:15,343",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"Finally, how to define the similarity"
cs-410_5_1_124,"00:09:15,343","00:09:20,018",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,So these questions must be addressed
cs-410_5_1_125,"00:09:20,018","00:09:24,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,function that we can actually
cs-410_5_1_126,"00:09:25,920","00:09:31,767",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,So how do we solve these problems
cs-410_5_1_127,"00:09:31,767","00:09:38,702",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,is the main topic of the next lecture.
cs-410_5_1_128,"00:09:38,702","00:09:44,589",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,[MUSIC]
cs-410_1_12_1,"00:00:00,401","00:00:07,552",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_1_12_2,"00:00:07,552","00:00:10,524",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_1_12_3,"00:00:10,524","00:00:16,097",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,Contextual Text Mining called Contextual
cs-410_1_12_4,"00:00:19,162","00:00:23,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In this lecture, we're going to continue"
cs-410_1_12_5,"00:00:23,930","00:00:28,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,And we're going to introduce Contextual
cs-410_1_12_6,"00:00:28,990","00:00:32,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,as exchanging of POS for
cs-410_1_12_7,"00:00:34,390","00:00:40,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,Recall that in contextual text mining
cs-410_1_12_8,"00:00:40,310","00:00:42,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,in consideration of the context so
cs-410_1_12_9,"00:00:42,285","00:00:46,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,that we can associate the topics with a
cs-410_1_12_10,"00:00:48,240","00:00:54,033",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"So in this approach, contextual"
cs-410_1_12_11,"00:00:54,033","00:00:58,487",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"or CPLSA, the main idea is to"
cs-410_1_12_12,"00:00:58,487","00:01:01,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,context variables into a generating model.
cs-410_1_12_13,"00:01:03,150","00:01:06,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,Recall that before when we generate
cs-410_1_12_14,"00:01:06,860","00:01:10,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"wIth some topics, and"
cs-410_1_12_15,"00:01:10,730","00:01:18,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"But here, we're going to add context"
cs-410_1_12_16,"00:01:18,130","00:01:23,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,and also the content of topics
cs-410_1_12_17,"00:01:23,500","00:01:27,607",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"Or in other words, we're going to let"
cs-410_1_12_18,"00:01:27,607","00:01:28,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,the content of a topic.
cs-410_1_12_19,"00:01:31,172","00:01:37,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,The consequences that this will enable
cs-410_1_12_20,"00:01:37,370","00:01:41,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"Make the topics more interesting,"
cs-410_1_12_21,"00:01:41,320","00:01:46,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,Because we can then have topics
cs-410_1_12_22,"00:01:46,120","00:01:49,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,specifically to a particular
cs-410_1_12_23,"00:01:49,070","00:01:50,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"For example, a particular time period."
cs-410_1_12_24,"00:01:52,020","00:01:55,639",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,"As an extension of PLSA model,"
cs-410_1_12_25,"00:01:55,639","00:02:01,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,CPLSA does the following changes.
cs-410_1_12_26,"00:02:01,330","00:02:05,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,Firstly it would model the conditional
cs-410_1_12_27,"00:02:07,110","00:02:12,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,That clearly suggests that the generation
cs-410_1_12_28,"00:02:12,990","00:02:16,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,and that allows us to bring
cs-410_1_12_29,"00:02:18,230","00:02:22,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"Secondly, it makes two specific"
cs-410_1_12_30,"00:02:22,300","00:02:24,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,of topics on context.
cs-410_1_12_31,"00:02:24,650","00:02:28,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,One is to assume that depending on
cs-410_1_12_32,"00:02:28,420","00:02:33,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"periods or different locations, we assume"
cs-410_1_12_33,"00:02:33,630","00:02:37,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,or different versions of word
cs-410_1_12_34,"00:02:38,540","00:02:42,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,And this assumption allows
cs-410_1_12_35,"00:02:42,260","00:02:45,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,variations of the same topic
cs-410_1_12_36,"00:02:46,500","00:02:53,059",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,The other is that we assume the topic
cs-410_1_12_37,"00:02:55,150","00:02:56,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,That means depending on the time or
cs-410_1_12_38,"00:02:56,810","00:02:59,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"location, we might cover"
cs-410_1_12_39,"00:03:00,670","00:03:03,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"Again, this dependency"
cs-410_1_12_40,"00:03:03,890","00:03:08,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,capture the association of
cs-410_1_12_41,"00:03:08,680","00:03:14,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,We can still use the EM algorithm to solve
cs-410_1_12_42,"00:03:16,280","00:03:22,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"And in this case, the estimated parameters"
cs-410_1_12_43,"00:03:22,520","00:03:23,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"And in particular,"
cs-410_1_12_44,"00:03:23,590","00:03:29,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,a lot of conditional probabilities
cs-410_1_12_45,"00:03:29,940","00:03:33,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,And this is what allows you
cs-410_1_12_46,"00:03:33,090","00:03:34,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,So this is the basic idea.
cs-410_1_12_47,"00:03:35,750","00:03:41,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Now, we don't have time to"
cs-410_1_12_48,"00:03:41,470","00:03:45,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,but there are references here that you
cs-410_1_12_49,"00:03:45,700","00:03:52,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,Here I just want to explain the high
cs-410_1_12_50,"00:03:52,120","00:03:55,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,Particularly I want to explain
cs-410_1_12_51,"00:03:55,610","00:04:00,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,Of text data that has context
cs-410_1_12_52,"00:04:01,550","00:04:05,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"So as you see here, we can assume"
cs-410_1_12_53,"00:04:05,660","00:04:11,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"For example, some topics might represent"
cs-410_1_12_54,"00:04:11,410","00:04:14,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,donation Or the city of New Orleans.
cs-410_1_12_55,"00:04:14,270","00:04:18,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,Now this example is in the context
cs-410_1_12_56,"00:04:18,803","00:04:20,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,that hit New Orleans.
cs-410_1_12_57,"00:04:22,915","00:04:27,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,Now as you can see we
cs-410_1_12_58,"00:04:27,400","00:04:31,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,views associated with each of the topics.
cs-410_1_12_59,"00:04:31,548","00:04:36,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,"And these are shown as View 1,"
cs-410_1_12_60,"00:04:36,530","00:04:41,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,Each view is a different
cs-410_1_12_61,"00:04:41,475","00:04:44,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,And these views are tied
cs-410_1_12_62,"00:04:44,715","00:04:50,125",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,"For example, tied to the location Texas,"
cs-410_1_12_63,"00:04:50,125","00:04:54,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,or the occupation of the author
cs-410_1_12_64,"00:04:56,205","00:05:01,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"Now, on the right side, now we assume"
cs-410_1_12_65,"00:05:01,560","00:05:04,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,So the time is known to be July 2005.
cs-410_1_12_66,"00:05:04,370","00:05:06,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"The location is Texas, etc."
cs-410_1_12_67,"00:05:06,710","00:05:11,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,And such context information is
cs-410_1_12_68,"00:05:11,410","00:05:13,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,So we're not going to just model the text.
cs-410_1_12_69,"00:05:15,100","00:05:20,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,And so one idea here is to model
cs-410_1_12_70,"00:05:20,980","00:05:21,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,various content.
cs-410_1_12_71,"00:05:21,920","00:05:25,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,And this gives us different views
cs-410_1_12_72,"00:05:27,720","00:05:32,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,Now on the bottom you will see the theme
cs-410_1_12_73,"00:05:32,360","00:05:39,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,according to these context
cs-410_1_12_74,"00:05:39,310","00:05:44,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"of a location like Texas, people might"
cs-410_1_12_75,"00:05:44,320","00:05:46,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,That's New Orleans.
cs-410_1_12_76,"00:05:46,130","00:05:47,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,That's visualized here.
cs-410_1_12_77,"00:05:47,690","00:05:50,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"But in a certain time period,"
cs-410_1_12_78,"00:05:50,930","00:05:56,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,maybe Particular topic and
cs-410_1_12_79,"00:05:56,280","00:06:00,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,So this variation is
cs-410_1_12_80,"00:06:00,980","00:06:07,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,So to generate the searcher document With
cs-410_1_12_81,"00:06:08,695","00:06:14,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,And this view of course now could
cs-410_1_12_82,"00:06:14,055","00:06:17,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"Let's say, we have taken this"
cs-410_1_12_83,"00:06:17,080","00:06:18,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,In the middle.
cs-410_1_12_84,"00:06:18,310","00:06:21,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"So now, we will have a specific"
cs-410_1_12_85,"00:06:21,850","00:06:25,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"Now, you can see some probabilities"
cs-410_1_12_86,"00:06:26,710","00:06:28,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Now, once we have chosen a view,"
cs-410_1_12_87,"00:06:28,830","00:06:34,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,now the situation will be very similar
cs-410_1_12_88,"00:06:34,400","00:06:38,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,We assume we have got word distribution
cs-410_1_12_89,"00:06:39,870","00:06:43,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"And then next, we will also choose"
cs-410_1_12_90,"00:06:43,070","00:06:47,988",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,we're going to choose a particular
cs-410_1_12_91,"00:06:47,988","00:06:55,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,"before is fixed in PLSA, and"
cs-410_1_12_92,"00:06:55,305","00:06:57,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,Each document has just one
cs-410_1_12_93,"00:06:58,885","00:07:03,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"Now here, because we consider context, so"
cs-410_1_12_94,"00:07:03,925","00:07:08,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,of Topics can vary depending on the
cs-410_1_12_95,"00:07:10,020","00:07:13,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"So, for example,"
cs-410_1_12_96,"00:07:13,470","00:07:19,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,Let's say in this case we picked
cs-410_1_12_97,"00:07:20,590","00:07:23,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,Now with the coverage and
cs-410_1_12_98,"00:07:23,440","00:07:26,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,we can generate a document in
cs-410_1_12_99,"00:07:26,590","00:07:32,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"So what it means, we're going to"
cs-410_1_12_100,"00:07:32,450","00:07:34,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,to choose one of these three topics.
cs-410_1_12_101,"00:07:34,880","00:07:38,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,Let's say we have picked the yellow topic.
cs-410_1_12_102,"00:07:38,230","00:07:43,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,Then we'll draw a word from this
cs-410_1_12_103,"00:07:44,760","00:07:46,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"Okay, so"
cs-410_1_12_104,"00:07:46,880","00:07:50,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,And then next time we might
cs-410_1_12_105,"00:07:50,840","00:07:53,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"we'll get donate, etc."
cs-410_1_12_106,"00:07:53,640","00:07:55,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,Until we generate all the words.
cs-410_1_12_107,"00:07:55,550","00:07:58,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,And this is basically
cs-410_1_12_108,"00:08:00,200","00:08:05,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,So the main difference is
cs-410_1_12_109,"00:08:05,220","00:08:11,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"And the word distribution,"
cs-410_1_12_110,"00:08:11,250","00:08:16,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,in other words we have extra switches
cs-410_1_12_111,"00:08:16,050","00:08:20,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,control the choices of different views
cs-410_1_12_112,"00:08:22,010","00:08:25,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And naturally the model we have
cs-410_1_12_113,"00:08:25,430","00:08:29,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,But once we can estimate those
cs-410_1_12_114,"00:08:29,010","00:08:33,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,then we will be able to understand
cs-410_1_12_115,"00:08:33,080","00:08:36,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,or context specific coverages of topics.
cs-410_1_12_116,"00:08:36,020","00:08:38,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,And this is precisely what we
cs-410_1_12_117,"00:08:40,450","00:08:42,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,So here are some simple results.
cs-410_1_12_118,"00:08:42,950","00:08:44,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,From using such a model.
cs-410_1_12_119,"00:08:44,340","00:08:48,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"Not necessary exactly the same model,"
cs-410_1_12_120,"00:08:48,240","00:08:50,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,So on this slide you see
cs-410_1_12_121,"00:08:50,860","00:08:54,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,comparing news articles about Iraq War and
cs-410_1_12_122,"00:08:56,315","00:09:02,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,Now we have about 30 articles on Iraq
cs-410_1_12_123,"00:09:02,855","00:09:08,852",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"And in this case,"
cs-410_1_12_124,"00:09:08,852","00:09:11,332",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,It's covered in both sets of articles and
cs-410_1_12_125,"00:09:11,332","00:09:17,352",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,the differences of variations of
cs-410_1_12_126,"00:09:18,622","00:09:23,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,So in this case the context is explicitly
cs-410_1_12_127,"00:09:25,040","00:09:30,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,And we see the results here
cs-410_1_12_128,"00:09:30,420","00:09:36,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,theme that's corresponding to
cs-410_1_12_129,"00:09:36,040","00:09:42,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,And there is a common theme indicting that
cs-410_1_12_130,"00:09:42,260","00:09:45,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,It's a common topic covered
cs-410_1_12_131,"00:09:45,630","00:09:48,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,And that's indicated by the high
cs-410_1_12_132,"00:09:48,970","00:09:49,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,nations.
cs-410_1_12_133,"00:09:51,160","00:09:54,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"Now if you know the background,"
cs-410_1_12_134,"00:09:54,680","00:10:00,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,this topic is indeed very
cs-410_1_12_135,"00:10:00,340","00:10:04,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,If you look at the column further and
cs-410_1_12_136,"00:10:04,900","00:10:09,336",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,two cells of word
cs-410_1_12_137,"00:10:09,336","00:10:14,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,collection specific variations
cs-410_1_12_138,"00:10:14,790","00:10:16,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"So it indicates that the Iraq War,"
cs-410_1_12_139,"00:10:16,660","00:10:21,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,United Nations was more involved
cs-410_1_12_140,"00:10:21,060","00:10:25,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,the Afghanistan War it was more involved
cs-410_1_12_141,"00:10:25,710","00:10:29,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,It's a different variation of
cs-410_1_12_142,"00:10:30,100","00:10:33,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,So this shows that by
cs-410_1_12_143,"00:10:33,140","00:10:36,215",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,In this case different the walls or
cs-410_1_12_144,"00:10:36,215","00:10:40,034",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,We can have topical variations
cs-410_1_12_145,"00:10:40,034","00:10:45,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,to review the differences of coverage
cs-410_1_12_146,"00:10:46,290","00:10:50,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,Now similarly if you look at
cs-410_1_12_147,"00:10:50,200","00:10:52,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"it has to do with the killing of people,"
cs-410_1_12_148,"00:10:52,710","00:10:56,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,it's not surprising if you know
cs-410_1_12_149,"00:10:56,320","00:10:59,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,"All the wars involve killing of people,"
cs-410_1_12_150,"00:10:59,660","00:11:03,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,imagine if you are not familiar
cs-410_1_12_151,"00:11:03,640","00:11:05,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,"We have a lot of text articles, and"
cs-410_1_12_152,"00:11:05,120","00:11:10,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,such a technique can reveal the common
cs-410_1_12_153,"00:11:10,230","00:11:14,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,It can be used to review common topics
cs-410_1_12_154,"00:11:14,715","00:11:19,581",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,If you look at of course in
cs-410_1_12_155,"00:11:19,581","00:11:26,143",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,you see variations of killing of people
cs-410_1_12_156,"00:11:28,279","00:11:31,582",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,And here is another example of results
cs-410_1_12_157,"00:11:31,582","00:11:36,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,obtained from blog articles
cs-410_1_12_158,"00:11:37,470","00:11:42,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,"In this case,"
cs-410_1_12_159,"00:11:42,320","00:11:46,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,the trends of topics over time.
cs-410_1_12_160,"00:11:47,240","00:11:52,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,And the top one shows just
cs-410_1_12_161,"00:11:52,980","00:11:58,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"One is oil price, and one is about"
cs-410_1_12_162,"00:12:00,060","00:12:06,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,Now these topics are obtained from
cs-410_1_12_163,"00:12:07,300","00:12:09,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,And people talk about these topics.
cs-410_1_12_164,"00:12:09,395","00:12:12,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=729,And end up teaching to some other topics.
cs-410_1_12_165,"00:12:12,370","00:12:15,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,But the visualisation shows
cs-410_1_12_166,"00:12:15,000","00:12:18,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,we can have conditional
cs-410_1_12_167,"00:12:18,020","00:12:19,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,Given a topic.
cs-410_1_12_168,"00:12:19,660","00:12:23,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,So this allows us to plot
cs-410_1_12_169,"00:12:23,420","00:12:26,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,the curve is like what you're seeing here.
cs-410_1_12_170,"00:12:26,000","00:12:31,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,"We see that, initially, the two"
cs-410_1_12_171,"00:12:31,560","00:12:40,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,But later we see the topic of New Orleans
cs-410_1_12_172,"00:12:40,010","00:12:44,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=760,And this turns out to be
cs-410_1_12_173,"00:12:44,060","00:12:49,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"the time period when another hurricane,"
cs-410_1_12_174,"00:12:49,010","00:12:52,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,And that apparently triggered more
cs-410_1_12_175,"00:12:54,900","00:13:00,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,The bottom curve shows
cs-410_1_12_176,"00:13:00,010","00:13:05,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,about flooding of the city by block
cs-410_1_12_177,"00:13:05,320","00:13:11,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,And it also shows some shift of
cs-410_1_12_178,"00:13:11,620","00:13:19,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,people's migrating from the state
cs-410_1_12_179,"00:13:20,570","00:13:25,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,So in this case we can see the time can
cs-410_1_12_180,"00:13:25,650","00:13:26,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,topics.
cs-410_1_12_181,"00:13:27,780","00:13:33,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,These are some additional
cs-410_1_12_182,"00:13:33,070","00:13:37,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,In this case it was about
cs-410_1_12_183,"00:13:37,850","00:13:41,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And there was some criticism about
cs-410_1_12_184,"00:13:41,690","00:13:42,649",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,in the case of Hurricane Katrina.
cs-410_1_12_185,"00:13:44,020","00:13:48,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,And the discussion now is
cs-410_1_12_186,"00:13:48,280","00:13:54,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,And these visualizations show the coverage
cs-410_1_12_187,"00:13:54,260","00:13:59,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,And initially it's covered
cs-410_1_12_188,"00:13:59,610","00:14:05,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,"in the South, but then gradually"
cs-410_1_12_189,"00:14:05,530","00:14:09,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,"But in week four,"
cs-410_1_12_190,"00:14:09,760","00:14:14,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,we see a pattern that's very similar
cs-410_1_12_191,"00:14:14,370","00:14:18,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,And that's when again
cs-410_1_12_192,"00:14:18,700","00:14:22,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=858,So such a technique would allow
cs-410_1_12_193,"00:14:22,540","00:14:24,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,to examine their issues of topics.
cs-410_1_12_194,"00:14:24,960","00:14:27,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=864,And of course the moral
cs-410_1_12_195,"00:14:27,280","00:14:30,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,you can apply this to any
cs-410_1_12_196,"00:14:30,980","00:14:32,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,To review spatial temporal patterns.
cs-410_1_12_197,"00:14:34,460","00:14:37,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,His view found another application
cs-410_1_12_198,"00:14:37,390","00:14:41,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,where we look at the use of the model for
cs-410_1_12_199,"00:14:43,290","00:14:46,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,So here we're looking at the research
cs-410_1_12_200,"00:14:46,370","00:14:49,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,"IR, particularly SIGIR papers."
cs-410_1_12_201,"00:14:49,480","00:14:53,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,And the topic we are focusing on
cs-410_1_12_202,"00:14:53,180","00:14:58,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,And you can see the top words with high
cs-410_1_12_203,"00:14:59,580","00:15:04,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,And then we hope to examine
cs-410_1_12_204,"00:15:04,290","00:15:08,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,"One is a start of TREC, for"
cs-410_1_12_205,"00:15:08,290","00:15:11,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,This is a major evaluation
cs-410_1_12_206,"00:15:11,459","00:15:16,722",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,"government, and was launched in 1992 or"
cs-410_1_12_207,"00:15:16,722","00:15:20,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,And that is known to have made a impact on
cs-410_1_12_208,"00:15:20,690","00:15:22,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,the topics of research
cs-410_1_12_209,"00:15:23,870","00:15:28,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,The other is the publication of
cs-410_1_12_210,"00:15:28,680","00:15:31,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,This is about a language model
cs-410_1_12_211,"00:15:31,850","00:15:36,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,It's also known to have made a high
cs-410_1_12_212,"00:15:36,440","00:15:39,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,So we hope to use this kind of
cs-410_1_12_213,"00:15:39,780","00:15:44,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,The idea here is simply to
cs-410_1_12_214,"00:15:44,090","00:15:48,585",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,And use these events to divide
cs-410_1_12_215,"00:15:48,585","00:15:51,397",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,For the event and
cs-410_1_12_216,"00:15:51,397","00:15:54,417",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=951,And then we can compare
cs-410_1_12_217,"00:15:54,417","00:15:57,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=954,"The and the variations, etc."
cs-410_1_12_218,"00:15:57,875","00:16:02,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,"So in this case,"
cs-410_1_12_219,"00:16:02,750","00:16:07,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,retrieval models was mostly a vector
cs-410_1_12_220,"00:16:07,120","00:16:08,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"But the after Trec,"
cs-410_1_12_221,"00:16:08,800","00:16:13,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,apparently the study of retrieval models
cs-410_1_12_222,"00:16:13,975","00:16:18,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,That seems to suggest some
cs-410_1_12_223,"00:16:18,440","00:16:22,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,"example, email was used in"
cs-410_1_12_224,"00:16:22,980","00:16:26,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,subtopical retrieval was another
cs-410_1_12_225,"00:16:28,200","00:16:32,461",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,"On the bottom,"
cs-410_1_12_226,"00:16:32,461","00:16:36,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,with the propagation of
cs-410_1_12_227,"00:16:36,300","00:16:40,631",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=996,"Before, we have those classic"
cs-410_1_12_228,"00:16:40,631","00:16:44,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"logic model, Boolean etc., but after 1998,"
cs-410_1_12_229,"00:16:44,600","00:16:50,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1004,we see clear dominance of language
cs-410_1_12_230,"00:16:50,430","00:16:54,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"And we see words like language model,"
cs-410_1_12_231,"00:16:54,580","00:17:00,764",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,So this technique here can use events as
cs-410_1_12_232,"00:17:00,764","00:17:03,403",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1020,Again the technique is generals so
cs-410_1_12_233,"00:17:03,403","00:17:07,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,you can use this to analyze
cs-410_1_12_234,"00:17:07,370","00:17:10,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,Here are some suggested readings.
cs-410_1_12_235,"00:17:11,940","00:17:20,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,The first is paper about simple staging of
cs-410_1_12_236,"00:17:21,270","00:17:24,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,It's to perform comparative
cs-410_1_12_237,"00:17:24,610","00:17:27,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,extract common topics shared
cs-410_1_12_238,"00:17:27,410","00:17:29,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,And there are variations
cs-410_1_12_239,"00:17:31,010","00:17:35,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1051,The second one is the main
cs-410_1_12_240,"00:17:35,540","00:17:38,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,Was a discussion of a lot of applications.
cs-410_1_12_241,"00:17:38,830","00:17:44,889",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,The third one has a lot of details
cs-410_1_12_242,"00:17:44,889","00:17:47,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,the Hurricane Katrina example.
cs-410_1_12_243,"00:17:47,679","00:17:57,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1067,[MUSIC]
cs-410_5_5_1,"00:00:00,000","00:00:03,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_5_2,"00:00:07,481","00:00:09,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the Web Indexing.
cs-410_5_5_3,"00:00:11,980","00:00:16,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture, we will continue"
cs-410_5_5_4,"00:00:16,740","00:00:20,741",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,we're going to talk about how
cs-410_5_5_5,"00:00:24,457","00:00:29,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"So once we crawl the web,"
cs-410_5_5_6,"00:00:29,720","00:00:33,489",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,The next step is to use the indexer
cs-410_5_5_7,"00:00:36,540","00:00:41,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,"In general, we can use the same"
cs-410_5_5_8,"00:00:41,150","00:00:45,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,creating an index and that is what we
cs-410_5_5_9,"00:00:45,060","00:00:48,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,but there are there are new
cs-410_5_5_10,"00:00:48,718","00:00:55,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"For web scale indexing, and the two main"
cs-410_5_5_11,"00:00:55,100","00:00:57,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"The index would be so large,"
cs-410_5_5_12,"00:00:57,450","00:01:03,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,that it cannot actually fit into
cs-410_5_5_13,"00:01:03,220","00:01:05,879",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,So we have to store the data
cs-410_5_5_14,"00:01:06,910","00:01:10,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Also, because the data is so"
cs-410_5_5_15,"00:01:10,900","00:01:15,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"process the data in parallel, so"
cs-410_5_5_16,"00:01:15,700","00:01:20,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Now to address these challenges,"
cs-410_5_5_17,"00:01:20,410","00:01:25,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,One is the Google File System that's
cs-410_5_5_18,"00:01:25,430","00:01:30,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,programmers manage files stored
cs-410_5_5_19,"00:01:32,000","00:01:33,159",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,The second is MapReduce.
cs-410_5_5_20,"00:01:33,159","00:01:37,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,This is a general software framework for
cs-410_5_5_21,"00:01:38,960","00:01:44,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,Hadoop is the most well known open
cs-410_5_5_22,"00:01:44,830","00:01:47,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,Now used in many applications.
cs-410_5_5_23,"00:01:50,000","00:01:52,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"So, this is the architecture"
cs-410_5_5_24,"00:01:53,790","00:01:56,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,It uses a very simple centralized
cs-410_5_5_25,"00:01:56,930","00:02:01,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,management mechanism to manage
cs-410_5_5_26,"00:02:01,210","00:02:05,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"Files, so"
cs-410_5_5_27,"00:02:05,590","00:02:09,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,look up a table to know where
cs-410_5_5_28,"00:02:11,040","00:02:16,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,The application client will then
cs-410_5_5_29,"00:02:16,250","00:02:21,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,that obtains specific locations of
cs-410_5_5_30,"00:02:22,890","00:02:31,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,And once the GFS file kind obtained
cs-410_5_5_31,"00:02:31,450","00:02:37,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,then the application client can talk
cs-410_5_5_32,"00:02:37,880","00:02:43,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"data actually sits directly, so"
cs-410_5_5_33,"00:02:43,230","00:02:43,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,In the network.
cs-410_5_5_34,"00:02:46,020","00:02:53,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,So when this file system stores
cs-410_5_5_35,"00:02:53,290","00:02:59,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"with great fixed sizes of chunks, so"
cs-410_5_5_36,"00:03:00,720","00:03:01,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,Many chunks.
cs-410_5_5_37,"00:03:01,460","00:03:05,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,"Each chunk is 64 MB, so it's pretty big."
cs-410_5_5_38,"00:03:05,120","00:03:09,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And that's appropriate for
cs-410_5_5_39,"00:03:09,080","00:03:12,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,These chunks are replicated
cs-410_5_5_40,"00:03:12,510","00:03:17,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,So this is something that the programmer
cs-410_5_5_41,"00:03:17,210","00:03:22,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,and it's all taken care
cs-410_5_5_42,"00:03:22,210","00:03:24,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"So from the application perspective,"
cs-410_5_5_43,"00:03:24,110","00:03:28,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,the programmer would see this
cs-410_5_5_44,"00:03:28,250","00:03:32,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,And the programmer doesn't have to
cs-410_5_5_45,"00:03:32,510","00:03:35,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,can just invoke high level.
cs-410_5_5_46,"00:03:35,535","00:03:38,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,Operators to process the file.
cs-410_5_5_47,"00:03:39,975","00:03:44,915",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,And another feature is that the data
cs-410_5_5_48,"00:03:44,915","00:03:45,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,and chunk servers.
cs-410_5_5_49,"00:03:45,865","00:03:48,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So it's efficient in this sense.
cs-410_5_5_50,"00:03:51,190","00:03:54,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"On top of the Google file system, Google"
cs-410_5_5_51,"00:03:54,590","00:03:59,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,also proposed MapReduce as a general
cs-410_5_5_52,"00:03:59,220","00:04:05,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Now, this is very useful to support"
cs-410_5_5_53,"00:04:06,670","00:04:10,618",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"And so, this framework is,"
cs-410_5_5_54,"00:04:12,116","00:04:16,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,Hiding a lot of low-level
cs-410_5_5_55,"00:04:16,170","00:04:21,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"As a result, the programmer can make"
cs-410_5_5_56,"00:04:21,950","00:04:26,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,that can be run a large
cs-410_5_5_57,"00:04:28,990","00:04:33,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,So some of the low level details
cs-410_5_5_58,"00:04:33,930","00:04:39,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,the specific and network communications or
cs-410_5_5_59,"00:04:39,410","00:04:44,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,where the task are executed.
cs-410_5_5_60,"00:04:44,080","00:04:46,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,All these details are hidden
cs-410_5_5_61,"00:04:47,880","00:04:52,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,There is also a nice feature which
cs-410_5_5_62,"00:04:52,560","00:04:56,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"If one server is broken,"
cs-410_5_5_63,"00:04:56,490","00:05:01,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"the server is down, and"
cs-410_5_5_64,"00:05:01,140","00:05:05,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,Then the MapReduce mapper will know
cs-410_5_5_65,"00:05:05,300","00:05:11,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,So it automatically dispatches a task
cs-410_5_5_66,"00:05:11,600","00:05:15,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"And therefore, again the program"
cs-410_5_5_67,"00:05:15,400","00:05:17,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,here's how MapReduce works.
cs-410_5_5_68,"00:05:17,570","00:05:23,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,The input data would be separated
cs-410_5_5_69,"00:05:23,330","00:05:26,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,Now what exactly is in the value
cs-410_5_5_70,"00:05:26,460","00:05:31,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,it's actually a fairly general framework
cs-410_5_5_71,"00:05:31,520","00:05:35,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,into different parts and each part
cs-410_5_5_72,"00:05:37,100","00:05:40,984",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,Each key value pair would be and
cs-410_5_5_73,"00:05:40,984","00:05:44,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"The program was right the map function,"
cs-410_5_5_74,"00:05:45,890","00:05:50,043",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,And then the map function will
cs-410_5_5_75,"00:05:50,043","00:05:53,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,then generate a number of
cs-410_5_5_76,"00:05:53,870","00:05:58,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"Of course, the new key is usually"
cs-410_5_5_77,"00:05:58,370","00:06:02,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,that's given to the map as input.
cs-410_5_5_78,"00:06:02,070","00:06:06,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,And these key value pairs
cs-410_5_5_79,"00:06:06,000","00:06:10,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,all the outputs of all the map
cs-410_5_5_80,"00:06:12,540","00:06:16,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,and then there will be for
cs-410_5_5_81,"00:06:16,670","00:06:20,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"And the result is that,"
cs-410_5_5_82,"00:06:20,600","00:06:24,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,with the same key will be
cs-410_5_5_83,"00:06:24,260","00:06:30,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,So now we've got a pair of of a key and
cs-410_5_5_84,"00:06:31,630","00:06:34,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,So this would then be sent
cs-410_5_5_85,"00:06:36,330","00:06:41,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Now, of course, each reduce function"
cs-410_5_5_86,"00:06:41,330","00:06:45,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,so we will send these output values to
cs-410_5_5_87,"00:06:45,990","00:06:50,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,multiple reduce functions
cs-410_5_5_88,"00:06:52,220","00:06:57,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,A reduce function would then
cs-410_5_5_89,"00:06:57,980","00:07:04,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,a key in a set of values to produce
cs-410_5_5_90,"00:07:04,920","00:07:08,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,So these output values would
cs-410_5_5_91,"00:07:08,670","00:07:11,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,to form the final output.
cs-410_5_5_92,"00:07:12,420","00:07:17,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"And so, this is the general"
cs-410_5_5_93,"00:07:17,210","00:07:23,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,Now the programmer only needs to write
cs-410_5_5_94,"00:07:23,290","00:07:28,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,Everything else is actually taken
cs-410_5_5_95,"00:07:28,090","00:07:32,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So you can see the program really
cs-410_5_5_96,"00:07:32,920","00:07:38,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"And with such a framework, the input data"
cs-410_5_5_97,"00:07:38,570","00:07:42,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"which is processing parallel first by map,"
cs-410_5_5_98,"00:07:42,780","00:07:50,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,then being the process after
cs-410_5_5_99,"00:07:50,130","00:07:54,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,The much more reduced if I'm
cs-410_5_5_100,"00:07:55,720","00:08:00,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,the different keys and
cs-410_5_5_101,"00:08:00,390","00:08:02,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,"So it achieves some,"
cs-410_5_5_102,"00:08:05,410","00:08:10,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,it achieves the purpose of parallel
cs-410_5_5_103,"00:08:10,510","00:08:13,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,So let's take a look at a simple example.
cs-410_5_5_104,"00:08:13,620","00:08:15,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,And that's Word Counting.
cs-410_5_5_105,"00:08:16,620","00:08:21,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"The input is containing words,"
cs-410_5_5_106,"00:08:21,570","00:08:25,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,and the output that we want to generate is
cs-410_5_5_107,"00:08:25,990","00:08:27,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,So it's the Word Count.
cs-410_5_5_108,"00:08:28,270","00:08:32,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,We know this kind of counting
cs-410_5_5_109,"00:08:32,940","00:08:38,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,assess the popularity of a word in
cs-410_5_5_110,"00:08:38,290","00:08:41,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,achieving a factor of IDF wading for
cs-410_5_5_111,"00:08:42,880","00:08:44,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,So how can we solve this problem?
cs-410_5_5_112,"00:08:44,200","00:08:49,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"Well, one natural thought is that,"
cs-410_5_5_113,"00:08:49,200","00:08:53,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,done in parallel by simply counting
cs-410_5_5_114,"00:08:53,860","00:08:57,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,and then in the end we just
cs-410_5_5_115,"00:08:57,000","00:09:01,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,And that's precisely the idea of
cs-410_5_5_116,"00:09:02,900","00:09:06,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,We can parallelize on
cs-410_5_5_117,"00:09:07,670","00:09:13,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"So more specifically, we can assume"
cs-410_5_5_118,"00:09:14,120","00:09:20,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,a key value pair that represents the line
cs-410_5_5_119,"00:09:20,450","00:09:25,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"So the first line, for"
cs-410_5_5_120,"00:09:25,760","00:09:32,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,that is another word by word and
cs-410_5_5_121,"00:09:32,240","00:09:36,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,So this key value pair would
cs-410_5_5_122,"00:09:36,250","00:09:40,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,The Map Function then would just
cs-410_5_5_123,"00:09:41,700","00:09:43,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"And in this case,"
cs-410_5_5_124,"00:09:43,880","00:09:46,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,Each world gets a count of one and
cs-410_5_5_125,"00:09:46,360","00:09:52,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,these are the output that you see here
cs-410_5_5_126,"00:09:52,770","00:09:56,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,So the map function is really
cs-410_5_5_127,"00:09:56,270","00:10:00,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,what the pseudocode looks
cs-410_5_5_128,"00:10:00,450","00:10:05,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,you see it simply needs to iterate
cs-410_5_5_129,"00:10:05,370","00:10:08,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,And then just collect the function
cs-410_5_5_130,"00:10:09,390","00:10:14,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,which means it would then send the word
cs-410_5_5_131,"00:10:14,080","00:10:18,686",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,The collector would then try to
cs-410_5_5_132,"00:10:18,686","00:10:21,205",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"different Map Functions, right?"
cs-410_5_5_133,"00:10:21,205","00:10:25,937",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,So the function is very simple and
cs-410_5_5_134,"00:10:25,937","00:10:30,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,this function as a way to
cs-410_5_5_135,"00:10:31,620","00:10:34,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,"Of course, the second line will be"
cs-410_5_5_136,"00:10:34,780","00:10:36,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,which we will produce a single output.
cs-410_5_5_137,"00:10:36,990","00:10:40,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,"Okay, now the output from the map"
cs-410_5_5_138,"00:10:40,800","00:10:45,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,send it to a collector and the collector
cs-410_5_5_139,"00:10:45,550","00:10:50,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,"So at this stage, you can see,"
cs-410_5_5_140,"00:10:50,220","00:10:53,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,Each pair is a word and
cs-410_5_5_141,"00:10:53,850","00:10:58,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,"So, once we see all these pairs."
cs-410_5_5_142,"00:10:58,960","00:11:03,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"Then we can sort them based on the key,"
cs-410_5_5_143,"00:11:03,570","00:11:08,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,So we will collect all the counts
cs-410_5_5_144,"00:11:09,610","00:11:11,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,"And similarly, we do that for other words."
cs-410_5_5_145,"00:11:11,790","00:11:13,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"Like Hadoop, Hello, etc."
cs-410_5_5_146,"00:11:13,620","00:11:19,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,So each word now is attached to
cs-410_5_5_147,"00:11:20,700","00:11:27,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,And these counts represent the occurrences
cs-410_5_5_148,"00:11:27,860","00:11:33,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,So now we have got a new pair of a key and
cs-410_5_5_149,"00:11:33,330","00:11:38,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,this pair will then be fed into reduce
cs-410_5_5_150,"00:11:38,610","00:11:44,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,would have to finish the job of counting
cs-410_5_5_151,"00:11:44,450","00:11:47,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"Now, it has all ready got all"
cs-410_5_5_152,"00:11:47,020","00:11:50,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,all it needs to do is
cs-410_5_5_153,"00:11:50,370","00:11:53,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,So the reduce function here
cs-410_5_5_154,"00:11:53,810","00:11:57,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,"You have a counter, and"
cs-410_5_5_155,"00:11:57,130","00:11:59,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,That you'll see in this array.
cs-410_5_5_156,"00:11:59,260","00:12:02,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,"And that,"
cs-410_5_5_157,"00:12:02,884","00:12:07,203",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"And then finally, you output the P and"
cs-410_5_5_158,"00:12:07,203","00:12:11,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,And that's precisely what we want as
cs-410_5_5_159,"00:12:12,220","00:12:14,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,"So you can see,"
cs-410_5_5_160,"00:12:14,830","00:12:16,842",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,To building an Invert index.
cs-410_5_5_161,"00:12:16,842","00:12:21,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"And if you think about it,"
cs-410_5_5_162,"00:12:21,050","00:12:24,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,"And we have already got a dictionary,"
cs-410_5_5_163,"00:12:24,410","00:12:26,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,We have got the count.
cs-410_5_5_164,"00:12:26,440","00:12:32,776",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,But what's missing is
cs-410_5_5_165,"00:12:32,776","00:12:38,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,frequency counts of words
cs-410_5_5_166,"00:12:38,240","00:12:43,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,So we can modify this slightly to
cs-410_5_5_167,"00:12:43,420","00:12:45,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,here's one way to do that.
cs-410_5_5_168,"00:12:45,800","00:12:51,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,"So in this case, we can assume the input"
cs-410_5_5_169,"00:12:51,490","00:12:56,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,"which denotes the document ID,"
cs-410_5_5_170,"00:12:56,510","00:13:02,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,"denoting the screen for that document,"
cs-410_5_5_171,"00:13:02,420","00:13:05,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=782,"And so, the map function would do"
cs-410_5_5_172,"00:13:05,740","00:13:07,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,seen in the word campaign example.
cs-410_5_5_173,"00:13:07,910","00:13:14,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=787,It simply groups all the counts of
cs-410_5_5_174,"00:13:14,640","00:13:18,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,And it would then generate
cs-410_5_5_175,"00:13:18,010","00:13:21,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"Each key is a word, and"
cs-410_5_5_176,"00:13:21,140","00:13:27,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=801,the value is the count of this word in
cs-410_5_5_177,"00:13:27,650","00:13:32,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"Now, you can easily see why we need to"
cs-410_5_5_178,"00:13:32,640","00:13:36,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,"in inverted index, we would like to"
cs-410_5_5_179,"00:13:36,690","00:13:41,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=816,"should keep track of it, and this can then"
cs-410_5_5_180,"00:13:41,290","00:13:46,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,Now similarly another document D2
cs-410_5_5_181,"00:13:46,710","00:13:50,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,"So in the end, again, there is a sorting"
cs-410_5_5_182,"00:13:50,890","00:13:55,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,"And then we will have just a key,"
cs-410_5_5_183,"00:13:55,690","00:14:00,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,associated with all the documents
cs-410_5_5_184,"00:14:00,340","00:14:02,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,Or all the documents where java occurred.
cs-410_5_5_185,"00:14:04,500","00:14:09,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"And the counts, so"
cs-410_5_5_186,"00:14:09,520","00:14:11,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,And this will be collected together.
cs-410_5_5_187,"00:14:11,880","00:14:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,"And this will be, so"
cs-410_5_5_188,"00:14:15,840","00:14:20,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,So now you can see the reduce function
cs-410_5_5_189,"00:14:20,010","00:14:21,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,an inverted index entry.
cs-410_5_5_190,"00:14:21,800","00:14:27,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,So it's just the word and all
cs-410_5_5_191,"00:14:27,280","00:14:30,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,the frequencies of the word
cs-410_5_5_192,"00:14:30,900","00:14:35,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,So all you need to do is
cs-410_5_5_193,"00:14:37,670","00:14:40,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,into a continuous chunk of data.
cs-410_5_5_194,"00:14:40,380","00:14:43,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,And this can be done
cs-410_5_5_195,"00:14:43,650","00:14:47,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,So basically the reduce function
cs-410_5_5_196,"00:14:47,520","00:14:48,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,Work.
cs-410_5_5_197,"00:14:49,450","00:14:53,647",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,"And so, this is a pseudo-code for"
cs-410_5_5_198,"00:14:53,647","00:14:58,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,[INAUDIBLE] that's construction.
cs-410_5_5_199,"00:14:58,010","00:15:05,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,"Here we see two functions,"
cs-410_5_5_200,"00:15:05,290","00:15:13,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=905,And a programmer would specify these two
cs-410_5_5_201,"00:15:13,440","00:15:18,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,And you can see basically they
cs-410_5_5_202,"00:15:18,990","00:15:22,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,"In the case of map, it's going to count"
cs-410_5_5_203,"00:15:22,870","00:15:27,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,the occurrences of a word
cs-410_5_5_204,"00:15:27,040","00:15:34,232",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And it would output all the counts
cs-410_5_5_205,"00:15:34,232","00:15:40,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,"So, this is the reduce function,"
cs-410_5_5_206,"00:15:40,350","00:15:47,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,simply concatenates all the input
cs-410_5_5_207,"00:15:47,380","00:15:53,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=947,and then put them together as
cs-410_5_5_208,"00:15:53,580","00:15:58,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,So this is a very simple
cs-410_5_5_209,"00:15:58,250","00:16:03,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=958,it would allow us to construct an inverted
cs-410_5_5_210,"00:16:03,360","00:16:06,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,the data can be processed
cs-410_5_5_211,"00:16:06,950","00:16:11,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,And program doesn't have to
cs-410_5_5_212,"00:16:12,080","00:16:18,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,So this is how we can do parallel
cs-410_5_5_213,"00:16:20,040","00:16:21,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"So to summarize,"
cs-410_5_5_214,"00:16:21,960","00:16:26,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=981,web scale indexing requires some
cs-410_5_5_215,"00:16:26,040","00:16:29,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,Standard traditional indexing techniques.
cs-410_5_5_216,"00:16:29,230","00:16:32,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"Mainly, we have to store"
cs-410_5_5_217,"00:16:32,800","00:16:37,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,And this is usually done by using a filing
cs-410_5_5_218,"00:16:37,990","00:16:40,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,But this should be through a file system.
cs-410_5_5_219,"00:16:40,240","00:16:45,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"And secondly, it requires creating"
cs-410_5_5_220,"00:16:45,320","00:16:50,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,large and takes long time to create
cs-410_5_5_221,"00:16:50,340","00:16:53,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"So if we can do it in parallel,"
cs-410_5_5_222,"00:16:53,790","00:16:56,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,this is done by using
cs-410_5_5_223,"00:16:57,850","00:17:02,182",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,Note that both the GFS and
cs-410_5_5_224,"00:17:02,182","00:17:05,251",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,they can also support
cs-410_5_5_225,"00:17:07,795","00:17:17,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,[MUSIC]
cs-410_2_5_1,"00:00:00,012","00:00:07,558",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_5_2,"00:00:07,558","00:00:10,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the feedback
cs-410_2_5_3,"00:00:12,910","00:00:18,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we continue talking"
cs-410_2_5_4,"00:00:18,040","00:00:21,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"Particularly, we're going to talk about"
cs-410_2_5_5,"00:00:23,930","00:00:29,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"As we have discussed before,"
cs-410_2_5_6,"00:00:29,210","00:00:34,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,of text retrieval system is removed from
cs-410_2_5_7,"00:00:34,890","00:00:37,467",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,We will have positive examples.
cs-410_2_5_8,"00:00:37,467","00:00:40,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,Those are the documents that
cs-410_2_5_9,"00:00:40,669","00:00:42,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,be charged with being relevant.
cs-410_2_5_10,"00:00:42,610","00:00:45,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,All the documents that
cs-410_2_5_11,"00:00:45,160","00:00:46,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,We also have negative examples.
cs-410_2_5_12,"00:00:46,910","00:00:49,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,Those are documents known
cs-410_2_5_13,"00:00:49,590","00:00:52,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,They can also be the documents
cs-410_2_5_14,"00:00:55,350","00:00:58,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,The general method in
cs-410_2_5_15,"00:00:58,570","00:01:02,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,feedback is to modify our query vector.
cs-410_2_5_16,"00:01:04,010","00:01:08,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,We want to place the query vector in
cs-410_2_5_17,"00:01:10,120","00:01:11,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,And what does that mean exactly?
cs-410_2_5_18,"00:01:11,520","00:01:14,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"Well, if we think about the query vector"
cs-410_2_5_19,"00:01:14,930","00:01:17,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,something to the vector elements.
cs-410_2_5_20,"00:01:17,270","00:01:21,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,"And in general,"
cs-410_2_5_21,"00:01:21,240","00:01:27,129",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Or we might just weight of old terms or
cs-410_2_5_22,"00:01:29,230","00:01:32,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"As a result, in general,"
cs-410_2_5_23,"00:01:32,780","00:01:35,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,We often call this query expansion.
cs-410_2_5_24,"00:01:37,960","00:01:40,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,The most effective method in
cs-410_2_5_25,"00:01:40,920","00:01:44,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"is called the Rocchio Feedback, which was"
cs-410_2_5_26,"00:01:47,490","00:01:49,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,So the idea is quite simple.
cs-410_2_5_27,"00:01:49,110","00:01:53,402",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,We illustrate this idea by
cs-410_2_5_28,"00:01:53,402","00:01:58,231",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,of all the documents in the collection and
cs-410_2_5_29,"00:01:58,231","00:02:03,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,So now we can see the query
cs-410_2_5_30,"00:02:03,935","00:02:07,428",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,and these are all the documents.
cs-410_2_5_31,"00:02:07,428","00:02:11,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,So when we use the query back there and
cs-410_2_5_32,"00:02:11,230","00:02:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"the most similar documents,"
cs-410_2_5_33,"00:02:14,780","00:02:18,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,that these documents would be
cs-410_2_5_34,"00:02:18,960","00:02:22,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"And these process are relevant documents,"
cs-410_2_5_35,"00:02:22,512","00:02:27,762",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"these are relevant documents,"
cs-410_2_5_36,"00:02:27,762","00:02:32,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,And then these minuses are negative
cs-410_2_5_37,"00:02:34,310","00:02:40,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,So our goal here is trying to move
cs-410_2_5_38,"00:02:40,150","00:02:42,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,to improve the retrieval accuracy.
cs-410_2_5_39,"00:02:42,780","00:02:48,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,"By looking at this diagram,"
cs-410_2_5_40,"00:02:48,390","00:02:50,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,Where should we move the query vector so
cs-410_2_5_41,"00:02:50,650","00:02:53,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,that we can improve
cs-410_2_5_42,"00:02:53,930","00:02:56,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"Intuitively, where do you"
cs-410_2_5_43,"00:02:58,050","00:03:01,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"If you want to think more,"
cs-410_2_5_44,"00:03:02,980","00:03:10,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,"If you think about this picture, you can"
cs-410_2_5_45,"00:03:10,090","00:03:15,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,case you want the query vector to be as
cs-410_2_5_46,"00:03:15,520","00:03:20,462",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"That means ideally, you want to place"
cs-410_2_5_47,"00:03:20,462","00:03:24,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,Or we want to move the query
cs-410_2_5_48,"00:03:26,510","00:03:29,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,Now so what exactly is this point?
cs-410_2_5_49,"00:03:29,100","00:03:35,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Well, if you want these relevant"
cs-410_2_5_50,"00:03:35,710","00:03:41,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,you want this to be in the center of
cs-410_2_5_51,"00:03:41,340","00:03:44,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,Because then if you draw
cs-410_2_5_52,"00:03:44,710","00:03:47,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,you'll get all these relevant documents.
cs-410_2_5_53,"00:03:47,240","00:03:52,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So that means we can move the query
cs-410_2_5_54,"00:03:52,250","00:03:54,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,all the relevant document vectors.
cs-410_2_5_55,"00:03:55,680","00:03:59,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,And this is basically the idea of Rocchio.
cs-410_2_5_56,"00:03:59,106","00:04:03,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Of course, you can consider"
cs-410_2_5_57,"00:04:03,645","00:04:07,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,we want to move away from
cs-410_2_5_58,"00:04:07,040","00:04:11,971",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,Now your match that we're talking about
cs-410_2_5_59,"00:04:11,971","00:04:14,202",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,away from other vectors.
cs-410_2_5_60,"00:04:14,202","00:04:18,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,It just means that we have this formula.
cs-410_2_5_61,"00:04:18,340","00:04:22,891",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,Here you can see this is
cs-410_2_5_62,"00:04:22,891","00:04:29,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,this average basically is the centroid
cs-410_2_5_63,"00:04:29,680","00:04:32,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"When we take the average of these vectors,"
cs-410_2_5_64,"00:04:32,250","00:04:35,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,then were computing
cs-410_2_5_65,"00:04:35,580","00:04:41,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"Similarly, this is the average of"
cs-410_2_5_66,"00:04:41,070","00:04:46,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So it's essentially of
cs-410_2_5_67,"00:04:46,080","00:04:51,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"And we have these three parameters here,"
cs-410_2_5_68,"00:04:51,710","00:04:55,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,They are controlling
cs-410_2_5_69,"00:04:55,200","00:04:57,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"When we add these two vectors together,"
cs-410_2_5_70,"00:04:57,560","00:05:02,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we're moving the query vector
cs-410_2_5_71,"00:05:03,620","00:05:05,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,This is when we add them together.
cs-410_2_5_72,"00:05:05,740","00:05:08,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"When we subtracted this part,"
cs-410_2_5_73,"00:05:08,350","00:05:14,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,we kind of move the query
cs-410_2_5_74,"00:05:14,660","00:05:18,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,So this is the main idea
cs-410_2_5_75,"00:05:18,420","00:05:20,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"And after we have done this,"
cs-410_2_5_76,"00:05:20,720","00:05:25,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,we will get a new query vector which
cs-410_2_5_77,"00:05:25,710","00:05:31,905",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"This new query vector,"
cs-410_2_5_78,"00:05:31,905","00:05:38,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,original query vector toward this
cs-410_2_5_79,"00:05:38,878","00:05:42,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,away from the non-relevant value.
cs-410_2_5_80,"00:05:45,110","00:05:48,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"Okay, so let's take a look at the example."
cs-410_2_5_81,"00:05:48,200","00:05:51,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,This is the example that
cs-410_2_5_82,"00:05:51,360","00:05:55,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,Only that I deemed that display
cs-410_2_5_83,"00:05:55,600","00:05:59,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,I only showed the vector
cs-410_2_5_84,"00:05:59,210","00:06:03,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,We have five documents here and we have
cs-410_2_5_85,"00:06:04,760","00:06:09,667",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"to read in the documents here, right."
cs-410_2_5_86,"00:06:09,667","00:06:12,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And they're displayed in red.
cs-410_2_5_87,"00:06:12,650","00:06:14,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And these are the term vectors.
cs-410_2_5_88,"00:06:14,760","00:06:18,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,Now I have just assumed some of weights.
cs-410_2_5_89,"00:06:18,190","00:06:20,549",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"A lot of terms,"
cs-410_2_5_90,"00:06:20,549","00:06:22,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,Now these are negative arguments.
cs-410_2_5_91,"00:06:22,745","00:06:23,952",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,There are two here.
cs-410_2_5_92,"00:06:23,952","00:06:26,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,There is another one here.
cs-410_2_5_93,"00:06:26,120","00:06:32,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Now in this Rocchio method, we first"
cs-410_2_5_94,"00:06:32,520","00:06:37,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"And so let's see,"
cs-410_2_5_95,"00:06:37,540","00:06:42,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"the positive documents, we simply just,"
cs-410_2_5_96,"00:06:42,910","00:06:48,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,We just add this with this one
cs-410_2_5_97,"00:06:48,490","00:06:51,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And then that's down here and
cs-410_2_5_98,"00:06:51,560","00:06:54,801",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,And then we're going to add
cs-410_2_5_99,"00:06:54,801","00:06:56,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,then just take the average.
cs-410_2_5_100,"00:06:56,580","00:06:58,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,And so we do this for all this.
cs-410_2_5_101,"00:06:58,790","00:07:02,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"In the end, what we have is this one."
cs-410_2_5_102,"00:07:02,520","00:07:08,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"This is the average vector of these two,"
cs-410_2_5_103,"00:07:10,030","00:07:13,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,Let's also look at the centroid
cs-410_2_5_104,"00:07:13,770","00:07:15,052",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,This is basically the same.
cs-410_2_5_105,"00:07:15,052","00:07:18,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,We're going to take the average
cs-410_2_5_106,"00:07:18,150","00:07:22,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,And these are the corresponding
cs-410_2_5_107,"00:07:22,420","00:07:23,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,on and so forth.
cs-410_2_5_108,"00:07:23,020","00:07:25,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,"So in the end, we have this one."
cs-410_2_5_109,"00:07:26,230","00:07:29,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,Now in the Rocchio feedback
cs-410_2_5_110,"00:07:29,340","00:07:32,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,these with the original
cs-410_2_5_111,"00:07:32,920","00:07:36,083",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,So now let's see how we
cs-410_2_5_112,"00:07:36,083","00:07:37,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"Well, that's basically this."
cs-410_2_5_113,"00:07:38,830","00:07:42,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,So we have a parameter alpha
cs-410_2_5_114,"00:07:42,385","00:07:45,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,query times weight that's one.
cs-410_2_5_115,"00:07:45,210","00:07:49,626",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,And now we have beta to control
cs-410_2_5_116,"00:07:49,626","00:07:52,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"centroid of the weight, that's 1.5."
cs-410_2_5_117,"00:07:52,820","00:07:54,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,That comes from here.
cs-410_2_5_118,"00:07:54,285","00:08:00,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"All right, so this goes here."
cs-410_2_5_119,"00:08:00,400","00:08:07,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,And we also have this negative
cs-410_2_5_120,"00:08:07,555","00:08:14,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"And this way, it has come from,"
cs-410_2_5_121,"00:08:14,520","00:08:19,051",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,And we do exactly the same for
cs-410_2_5_122,"00:08:22,244","00:08:23,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And this is our new vector.
cs-410_2_5_123,"00:08:25,700","00:08:31,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,And we're going to use this new query
cs-410_2_5_124,"00:08:31,530","00:08:33,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"You can imagine what would happen, right?"
cs-410_2_5_125,"00:08:33,840","00:08:38,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,Because of the movement that this one
cs-410_2_5_126,"00:08:38,000","00:08:42,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,better because we moved
cs-410_2_5_127,"00:08:42,520","00:08:47,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,And it's going to penalize these black
cs-410_2_5_128,"00:08:47,290","00:08:49,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So this is precisely what
cs-410_2_5_129,"00:08:50,820","00:08:57,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,Now of course if we apply this method in
cs-410_2_5_130,"00:08:58,240","00:09:04,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,and that is the original query has
cs-410_2_5_131,"00:09:06,410","00:09:08,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,But after we do query explaining and
cs-410_2_5_132,"00:09:08,480","00:09:13,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"merging, we'll have many times"
cs-410_2_5_133,"00:09:13,210","00:09:16,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,So the calculation will
cs-410_2_5_134,"00:09:18,090","00:09:22,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"In practice,"
cs-410_2_5_135,"00:09:22,160","00:09:25,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,only retain the terms
cs-410_2_5_136,"00:09:27,000","00:09:29,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,So let's talk about how we
cs-410_2_5_137,"00:09:30,660","00:09:34,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,I just mentioned that they're
cs-410_2_5_138,"00:09:34,220","00:09:37,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,Consider only a small number of
cs-410_2_5_139,"00:09:37,400","00:09:38,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,the centroid vector.
cs-410_2_5_140,"00:09:38,690","00:09:39,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,This is for efficiency concern.
cs-410_2_5_141,"00:09:41,390","00:09:45,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"I also said here that negative examples,"
cs-410_2_5_142,"00:09:45,580","00:09:49,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"tend not to be very useful, especially"
cs-410_2_5_143,"00:09:50,860","00:09:52,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Now you can think about why.
cs-410_2_5_144,"00:09:55,320","00:09:59,771",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,One reason is because negative documents
cs-410_2_5_145,"00:09:59,771","00:10:00,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,directions.
cs-410_2_5_146,"00:10:00,645","00:10:02,391",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,"So, when you take the average,"
cs-410_2_5_147,"00:10:02,391","00:10:06,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,it doesn't really tell you where
cs-410_2_5_148,"00:10:06,860","00:10:10,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,Whereas positive documents
cs-410_2_5_149,"00:10:10,110","00:10:14,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,And they will point you to
cs-410_2_5_150,"00:10:14,569","00:10:19,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,So that also means that sometimes we don't
cs-410_2_5_151,"00:10:19,090","00:10:24,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,"But note that in some cases, in difficult"
cs-410_2_5_152,"00:10:24,580","00:10:26,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,negative feedback after is very useful.
cs-410_2_5_153,"00:10:27,550","00:10:29,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,Another thing is to avoid over-fitting.
cs-410_2_5_154,"00:10:29,370","00:10:34,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,That means we have to keep relatively
cs-410_2_5_155,"00:10:34,425","00:10:35,724",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,Why?
cs-410_2_5_156,"00:10:35,724","00:10:42,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,Because the sample that we see in
cs-410_2_5_157,"00:10:42,250","00:10:45,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,We don't want to overly
cs-410_2_5_158,"00:10:45,580","00:10:49,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,And the original query terms
cs-410_2_5_159,"00:10:49,390","00:10:51,753",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,Those terms are heightened by the user and
cs-410_2_5_160,"00:10:51,753","00:10:55,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,the user has decided that those
cs-410_2_5_161,"00:10:55,850","00:11:02,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,So in order to prevent
cs-410_2_5_162,"00:11:02,530","00:11:08,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"drifting, prevent topic drifting due to"
cs-410_2_5_163,"00:11:08,910","00:11:12,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,We generally would have to keep a pretty
cs-410_2_5_164,"00:11:12,740","00:11:13,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,it was safe to do that.
cs-410_2_5_165,"00:11:15,040","00:11:18,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,And this is especially true for
cs-410_2_5_166,"00:11:18,910","00:11:20,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,"Now, this method can be used for"
cs-410_2_5_167,"00:11:20,910","00:11:23,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,both relevance feedback and
cs-410_2_5_168,"00:11:23,790","00:11:28,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"In the case of pseudo-feedback, the prime"
cs-410_2_5_169,"00:11:28,780","00:11:32,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,value because the relevant examples
cs-410_2_5_170,"00:11:32,930","00:11:36,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,They're not as reliable as
cs-410_2_5_171,"00:11:36,780","00:11:40,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"In the case of relevance feedback,"
cs-410_2_5_172,"00:11:40,830","00:11:43,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,"So those parameters,"
cs-410_2_5_173,"00:11:45,020","00:11:48,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,And the Rocchio Method is
cs-410_2_5_174,"00:11:48,550","00:11:51,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,It's still a very popular method for
cs-410_2_5_175,"00:11:51,961","00:12:01,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,[MUSIC]
cs-410_8_5_1,"00:00:00,000","00:00:06,073",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_8_5_2,"00:00:06,073","00:00:13,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,we talked about PageRank as
cs-410_8_5_3,"00:00:14,155","00:00:21,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"Now, we also looked at some other examples"
cs-410_8_5_4,"00:00:21,245","00:00:24,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,"So there is another algorithm called HITS,"
cs-410_8_5_5,"00:00:24,425","00:00:28,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,that going to compute the scores for
cs-410_8_5_6,"00:00:28,257","00:00:33,167",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,The intuitions are pages that are widely
cs-410_8_5_7,"00:00:33,167","00:00:38,577",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,whereas pages that cite many
cs-410_8_5_8,"00:00:38,577","00:00:42,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,I think that the most interesting
cs-410_8_5_9,"00:00:42,650","00:00:46,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,is it's going to use
cs-410_8_5_10,"00:00:46,930","00:00:51,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,to kind of help improve the scoring for
cs-410_8_5_11,"00:00:51,470","00:00:53,318",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"And so here's the idea,"
cs-410_8_5_12,"00:00:53,318","00:00:57,809",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,it was assumed that good
cs-410_8_5_13,"00:00:58,870","00:01:03,847",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,That means if you are cited by many
cs-410_8_5_14,"00:01:03,847","00:01:07,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"that inquiry says, you're an authority."
cs-410_8_5_15,"00:01:07,266","00:01:11,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"And similarly, good hubs are those"
cs-410_8_5_16,"00:01:11,740","00:01:15,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,So if you pointed to a lot
cs-410_8_5_17,"00:01:15,560","00:01:17,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,then your hubs score would be increased.
cs-410_8_5_18,"00:01:17,880","00:01:22,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,So then you will have literally reinforced
cs-410_8_5_19,"00:01:22,115","00:01:22,968",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,some good hubs.
cs-410_8_5_20,"00:01:22,968","00:01:27,635",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,And so you have pointed to some good
cs-410_8_5_21,"00:01:27,635","00:01:30,544",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,whereas those authority
cs-410_8_5_22,"00:01:30,544","00:01:34,736",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,improved because they
cs-410_8_5_23,"00:01:34,736","00:01:39,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,And this is algorithms is also general it
cs-410_8_5_24,"00:01:39,380","00:01:40,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,network analysis.
cs-410_8_5_25,"00:01:40,730","00:01:43,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"So just briefly, here's how it works."
cs-410_8_5_26,"00:01:43,170","00:01:47,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"We first also construct a matrix, but this"
cs-410_8_5_27,"00:01:47,090","00:01:49,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,matrix and
cs-410_8_5_28,"00:01:49,750","00:01:54,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"So if there's a link there's a 1,"
cs-410_8_5_29,"00:01:54,100","00:01:56,185",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"Again, it's the same graph."
cs-410_8_5_30,"00:01:56,185","00:02:01,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And then we're going to
cs-410_8_5_31,"00:02:01,335","00:02:06,955",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,as the sum of the authority scores of
cs-410_8_5_32,"00:02:08,270","00:02:09,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"So whether you are hub,"
cs-410_8_5_33,"00:02:09,620","00:02:14,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,really depends on whether you are pointing
cs-410_8_5_34,"00:02:14,430","00:02:17,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,That's what it says in the first equation.
cs-410_8_5_35,"00:02:17,080","00:02:22,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"In the second equation,"
cs-410_8_5_36,"00:02:22,130","00:02:27,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,as a sum of the hub scores of all
cs-410_8_5_37,"00:02:27,350","00:02:30,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,So whether you are good authority
cs-410_8_5_38,"00:02:30,260","00:02:33,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,pages that are pointing
cs-410_8_5_39,"00:02:33,420","00:02:37,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,So you can see this forms
cs-410_8_5_40,"00:02:38,770","00:02:44,586",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"Now, these three questions can be"
cs-410_8_5_41,"00:02:44,586","00:02:50,707",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,So what we get here is then the hub
cs-410_8_5_42,"00:02:50,707","00:02:55,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,of the adjacency matrix and
cs-410_8_5_43,"00:02:55,770","00:03:00,026",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,and this is basically the first equation.
cs-410_8_5_44,"00:03:00,026","00:03:05,292",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"And similarly, the second equation"
cs-410_8_5_45,"00:03:05,292","00:03:11,034",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,vector is equal to the product of
cs-410_8_5_46,"00:03:11,034","00:03:15,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"Now, these are just different ways"
cs-410_8_5_47,"00:03:15,820","00:03:19,967",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,But what's interesting is that
cs-410_8_5_48,"00:03:19,967","00:03:26,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,you can also plug in the authority
cs-410_8_5_49,"00:03:26,680","00:03:31,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"So if you do that, you have actually"
cs-410_8_5_50,"00:03:31,500","00:03:33,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,and you get the equations
cs-410_8_5_51,"00:03:34,980","00:03:39,032",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,The hubs score vector is
cs-410_8_5_52,"00:03:39,032","00:03:43,522",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,by a transpose multiplied
cs-410_8_5_53,"00:03:43,522","00:03:47,511",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,"Similarly, we can do a transformation"
cs-410_8_5_54,"00:03:47,511","00:03:49,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,just the authorities also.
cs-410_8_5_55,"00:03:49,440","00:03:54,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,So although we frame the problem
cs-410_8_5_56,"00:03:54,370","00:03:58,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,we can actually eliminate one of them to
cs-410_8_5_57,"00:03:59,530","00:04:03,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Now, the difference between this and page"
cs-410_8_5_58,"00:04:03,810","00:04:07,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,a multiplication of the adjacency
cs-410_8_5_59,"00:04:07,960","00:04:09,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,So this is different from page rank.
cs-410_8_5_60,"00:04:11,250","00:04:15,373",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"But mathematically, then we will"
cs-410_8_5_61,"00:04:15,373","00:04:19,777",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"So in HITS,"
cs-410_8_5_62,"00:04:19,777","00:04:22,215",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,"Let's say, 1 for all these values, and"
cs-410_8_5_63,"00:04:22,215","00:04:26,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,then we would iteratively apply
cs-410_8_5_64,"00:04:26,580","00:04:33,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,And this is equivalent to multiply
cs-410_8_5_65,"00:04:34,720","00:04:37,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,So the arrows of these is exactly
cs-410_8_5_66,"00:04:37,740","00:04:43,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,But here because the adjacency
cs-410_8_5_67,"00:04:43,035","00:04:47,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,So what we have to do is after each
cs-410_8_5_68,"00:04:47,463","00:04:50,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,this would allow us to
cs-410_8_5_69,"00:04:50,980","00:04:53,671",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,Otherwise they would grow larger and
cs-410_8_5_70,"00:04:53,671","00:04:57,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"And if we do that, and"
cs-410_8_5_71,"00:04:58,360","00:05:03,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"That was the computer, the hubs scores,"
cs-410_8_5_72,"00:05:03,920","00:05:08,647",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,And these scores can then be used in
cs-410_8_5_73,"00:05:09,860","00:05:14,525",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"So to summarize in this lecture, we have"
cs-410_8_5_74,"00:05:14,525","00:05:19,302",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"In particular,"
cs-410_8_5_75,"00:05:19,302","00:05:23,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,increase the text
cs-410_8_5_76,"00:05:23,737","00:05:25,959",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,And we also talk about the PageRank and
cs-410_8_5_77,"00:05:25,959","00:05:29,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,page anchor as two major
cs-410_8_5_78,"00:05:29,380","00:05:35,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,Both can generate scores for web pages
cs-410_8_5_79,"00:05:35,930","00:05:39,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,Note that PageRank and
cs-410_8_5_80,"00:05:39,600","00:05:46,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,So they have many applications in
cs-410_8_5_81,"00:05:46,663","00:05:56,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,[MUSIC]
cs-410_4_5_1,"00:00:07,440","00:00:09,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about Web Search.
cs-410_4_5_2,"00:00:11,950","00:00:14,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture,"
cs-410_4_5_3,"00:00:14,750","00:00:19,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,of the most important applications of
cs-410_4_5_4,"00:00:19,150","00:00:21,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,So let's first look at some
cs-410_4_5_5,"00:00:21,520","00:00:23,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,opportunities in web search.
cs-410_4_5_6,"00:00:23,380","00:00:26,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"Now, many informational"
cs-410_4_5_7,"00:00:26,010","00:00:29,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,had been developed
cs-410_4_5_8,"00:00:29,010","00:00:33,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"So when the web was born,"
cs-410_4_5_9,"00:00:33,890","00:00:39,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,those algorithms to major application
cs-410_4_5_10,"00:00:39,890","00:00:45,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"So naturally, there have to be some"
cs-410_4_5_11,"00:00:45,780","00:00:53,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,search algorithms to address new
cs-410_4_5_12,"00:00:53,460","00:00:56,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,So here are some general challenges.
cs-410_4_5_13,"00:00:56,210","00:00:58,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"First, this is a scalability challenge."
cs-410_4_5_14,"00:00:58,510","00:01:00,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,How to handle the size of the web and
cs-410_4_5_15,"00:01:00,200","00:01:02,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,ensure completeness of
cs-410_4_5_16,"00:01:03,870","00:01:07,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,How to serve many users quickly and
cs-410_4_5_17,"00:01:07,820","00:01:10,801",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,And so that's one major challenge and
cs-410_4_5_18,"00:01:10,801","00:01:16,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,before the web was born the scale
cs-410_4_5_19,"00:01:16,480","00:01:20,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,The second problem is that there's
cs-410_4_5_20,"00:01:20,190","00:01:21,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,there are often spams.
cs-410_4_5_21,"00:01:21,960","00:01:24,334",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,The third challenge is
cs-410_4_5_22,"00:01:24,334","00:01:31,879",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,The new pages are constantly create and
cs-410_4_5_23,"00:01:31,879","00:01:36,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,so it makes it harder to
cs-410_4_5_24,"00:01:36,281","00:01:40,391",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,So these are some of the challenges
cs-410_4_5_25,"00:01:40,391","00:01:42,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,deal with high quality web searching.
cs-410_4_5_26,"00:01:44,090","00:01:47,492",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,On the other hand there are also some
cs-410_4_5_27,"00:01:47,492","00:01:49,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,leverage to include the search results.
cs-410_4_5_28,"00:01:49,930","00:01:53,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"There are many additional heuristics,"
cs-410_4_5_29,"00:01:55,070","00:02:00,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,using links that we can
cs-410_4_5_30,"00:02:00,020","00:02:03,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,Now everything that we talked about
cs-410_4_5_31,"00:02:03,510","00:02:04,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,are general algorithms.
cs-410_4_5_32,"00:02:05,630","00:02:11,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,They can be applied to any search
cs-410_4_5_33,"00:02:11,050","00:02:15,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"On the other hand, they also don't take"
cs-410_4_5_34,"00:02:15,890","00:02:21,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,of pages or documents in the specific
cs-410_4_5_35,"00:02:21,375","00:02:23,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"Web pages are linked with each other,"
cs-410_4_5_36,"00:02:23,855","00:02:28,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,the linking is something
cs-410_4_5_37,"00:02:28,645","00:02:33,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"So, because of these challenges and"
cs-410_4_5_38,"00:02:33,610","00:02:39,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,that have been developed for
cs-410_4_5_39,"00:02:39,110","00:02:41,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,One is parallel indexing and searching and
cs-410_4_5_40,"00:02:41,390","00:02:44,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,this is to address
cs-410_4_5_41,"00:02:44,410","00:02:49,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,"In particular, Google's imaging of"
cs-410_4_5_42,"00:02:49,930","00:02:53,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,has been very helpful in that aspect.
cs-410_4_5_43,"00:02:53,590","00:02:56,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"Second, there are techniques"
cs-410_4_5_44,"00:02:56,680","00:03:00,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"addressing the problem of spams,"
cs-410_4_5_45,"00:03:00,460","00:03:03,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,We'll have to prevent those spam
cs-410_4_5_46,"00:03:04,680","00:03:07,338",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And there are also techniques
cs-410_4_5_47,"00:03:07,338","00:03:10,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,And we're going to use a lot
cs-410_4_5_48,"00:03:10,520","00:03:15,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,that it's not easy to spam the search
cs-410_4_5_49,"00:03:15,410","00:03:19,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,And the third line of techniques is link
cs-410_4_5_50,"00:03:19,810","00:03:24,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,analysis and these are techniques that can
cs-410_4_5_51,"00:03:24,730","00:03:30,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,allow us to improve such results
cs-410_4_5_52,"00:03:30,780","00:03:35,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"And in general in web searching,"
cs-410_4_5_53,"00:03:35,230","00:03:37,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,ranking not just for link analysis.
cs-410_4_5_54,"00:03:37,690","00:03:43,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,But also exploring all kinds
cs-410_4_5_55,"00:03:43,300","00:03:47,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,anchor text that describes
cs-410_4_5_56,"00:03:47,730","00:03:51,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"So, here's a picture showing"
cs-410_4_5_57,"00:03:51,310","00:03:55,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"Basically, this is the web on the left and"
cs-410_4_5_58,"00:03:55,820","00:04:00,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,we're going to help this user to get
cs-410_4_5_59,"00:04:00,550","00:04:05,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,And the first component is a Crawler that
cs-410_4_5_60,"00:04:05,410","00:04:09,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,component is Indexer that would take
cs-410_4_5_61,"00:04:10,920","00:04:15,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,The third component there is a Retriever
cs-410_4_5_62,"00:04:15,720","00:04:19,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,answer user's query by talking
cs-410_4_5_63,"00:04:19,840","00:04:24,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,And then the search results will be given
cs-410_4_5_64,"00:04:24,790","00:04:29,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"show those results, it allows"
cs-410_4_5_65,"00:04:29,090","00:04:32,417",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"So, we're going to talk about"
cs-410_4_5_66,"00:04:32,417","00:04:37,552",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,"First of all, we're going to talk about"
cs-410_4_5_67,"00:04:37,552","00:04:42,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,software robot that would do something
cs-410_4_5_68,"00:04:42,459","00:04:44,954",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"To build a toy crawler is relatively easy,"
cs-410_4_5_69,"00:04:44,954","00:04:47,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,because you just need to start
cs-410_4_5_70,"00:04:47,875","00:04:51,912",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,And then fetch pages from the web and
cs-410_4_5_71,"00:04:51,912","00:04:53,517",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,figure out new links.
cs-410_4_5_72,"00:04:53,517","00:05:00,994",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,And then add them to the priority que and
cs-410_4_5_73,"00:05:00,994","00:05:04,764",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,But to be able to real crawler
cs-410_4_5_74,"00:05:04,764","00:05:09,249",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,there are some complicated issues
cs-410_4_5_75,"00:05:09,249","00:05:13,736",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"For example robustness,"
cs-410_4_5_76,"00:05:13,736","00:05:18,722",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,what if there's a trap that generates
cs-410_4_5_77,"00:05:18,722","00:05:23,456",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,that might attract your crawler to
cs-410_4_5_78,"00:05:23,456","00:05:26,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,to fetch dynamic generated pages?
cs-410_4_5_79,"00:05:26,700","00:05:30,093",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,The results of this issue
cs-410_4_5_80,"00:05:30,093","00:05:35,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,you don't want to overload one particular
cs-410_4_5_81,"00:05:35,668","00:05:39,158",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,you have to respect the robot
cs-410_4_5_82,"00:05:39,158","00:05:43,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,You also need to handle different
cs-410_4_5_83,"00:05:43,340","00:05:46,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"PDF files,"
cs-410_4_5_84,"00:05:46,019","00:05:50,189",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,And you have to also
cs-410_4_5_85,"00:05:50,189","00:05:56,237",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,sometimes those are CGI scripts and
cs-410_4_5_86,"00:05:56,237","00:06:01,139",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"etc, and sometimes you have"
cs-410_4_5_87,"00:06:01,139","00:06:03,866",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,they also create challenges.
cs-410_4_5_88,"00:06:03,866","00:06:08,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,And you ideally should also recognize
cs-410_4_5_89,"00:06:08,795","00:06:11,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,to duplicate those pages.
cs-410_4_5_90,"00:06:11,475","00:06:15,398",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"And finally, you may be interested"
cs-410_4_5_91,"00:06:15,398","00:06:19,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,Those are URLs that may not be linked
cs-410_4_5_92,"00:06:19,935","00:06:24,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"the URL to a shorter path, you might"
cs-410_4_5_93,"00:06:27,008","00:06:29,298",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,So what are the Major Crawling Strategies?
cs-410_4_5_94,"00:06:29,298","00:06:30,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"In general,"
cs-410_4_5_95,"00:06:30,040","00:06:36,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,Breadth-First is most common because
cs-410_4_5_96,"00:06:36,560","00:06:41,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,You would not keep probing a particular
cs-410_4_5_97,"00:06:42,635","00:06:47,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,Also parallel crawling is very
cs-410_4_5_98,"00:06:47,009","00:06:48,554",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,easy to parallelize.
cs-410_4_5_99,"00:06:48,554","00:06:50,887",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And there is some variations
cs-410_4_5_100,"00:06:50,887","00:06:54,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,and one interesting variation
cs-410_4_5_101,"00:06:54,560","00:06:59,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,"In this case, we're going to crawl just"
cs-410_4_5_102,"00:06:59,850","00:07:04,316",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,"For example,"
cs-410_4_5_103,"00:07:04,316","00:07:07,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,And this is typically going to
cs-410_4_5_104,"00:07:07,885","00:07:12,953",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,then you can use the query to get some
cs-410_4_5_105,"00:07:12,953","00:07:17,052",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,And then you can start it with those
cs-410_4_5_106,"00:07:17,052","00:07:19,544",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,"The one channel in crawling,"
cs-410_4_5_107,"00:07:19,544","00:07:24,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,is you will find the new
cs-410_4_5_108,"00:07:24,230","00:07:28,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,people probably are creating
cs-410_4_5_109,"00:07:28,732","00:07:33,502",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,And this is very challenging if
cs-410_4_5_110,"00:07:33,502","00:07:35,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,linked to any old pages.
cs-410_4_5_111,"00:07:35,930","00:07:41,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"If they are, then you can probably find"
cs-410_4_5_112,"00:07:41,655","00:07:46,946",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,so these are also some interesting
cs-410_4_5_113,"00:07:46,946","00:07:51,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,"And finally, we might face the scenario"
cs-410_4_5_114,"00:07:51,257","00:07:53,157",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"repeated crawling, right."
cs-410_4_5_115,"00:07:53,157","00:07:56,528",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"Let's say,"
cs-410_4_5_116,"00:07:56,528","00:07:59,448",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,and you first crawl a lot
cs-410_4_5_117,"00:07:59,448","00:08:03,816",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"But then,"
cs-410_4_5_118,"00:08:03,816","00:08:08,968",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,in the future you just need
cs-410_4_5_119,"00:08:08,968","00:08:13,277",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"In general, you don't have to"
cs-410_4_5_120,"00:08:13,277","00:08:14,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,It's not necessary.
cs-410_4_5_121,"00:08:16,650","00:08:21,563",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"So in this case, your goal is to"
cs-410_4_5_122,"00:08:21,563","00:08:26,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,by using minimum resources
cs-410_4_5_123,"00:08:27,490","00:08:33,986",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"So, this is actually a very"
cs-410_4_5_124,"00:08:33,986","00:08:40,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,"and this is a open research question,"
cs-410_4_5_125,"00:08:40,250","00:08:46,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,standard algorithms established yet
cs-410_4_5_126,"00:08:47,300","00:08:51,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"But in general, you can imagine,"
cs-410_4_5_127,"00:08:53,640","00:08:57,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,So the two major factors that
cs-410_4_5_128,"00:08:57,040","00:09:00,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,first will this page
cs-410_4_5_129,"00:09:00,760","00:09:03,411",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,And do I have to quote this page again?
cs-410_4_5_130,"00:09:03,411","00:09:07,726",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,If the page is a static page and
cs-410_4_5_131,"00:09:07,726","00:09:12,703",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,you probably don't have to re-crawl it
cs-410_4_5_132,"00:09:12,703","00:09:14,401",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,will changed frequently.
cs-410_4_5_133,"00:09:14,401","00:09:20,152",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"On the other hand, if it's a sports score"
cs-410_4_5_134,"00:09:20,152","00:09:25,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,you may need to re-crawl it and
cs-410_4_5_135,"00:09:25,840","00:09:30,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"The other factor to consider is,"
cs-410_4_5_136,"00:09:30,956","00:09:35,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"If it is, then it means that"
cs-410_4_5_137,"00:09:35,485","00:09:40,809",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,then thus it's more important to
cs-410_4_5_138,"00:09:40,809","00:09:45,439",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,Compared with another page that has
cs-410_4_5_139,"00:09:45,439","00:09:49,609",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"a year, then even though that"
cs-410_4_5_140,"00:09:49,609","00:09:55,164",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,It's probably not that necessary to
cs-410_4_5_141,"00:09:55,164","00:10:01,697",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,not as urgent as to maintain the freshness
cs-410_4_5_142,"00:10:01,697","00:10:05,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"So to summarize, web search is one of"
cs-410_4_5_143,"00:10:05,275","00:10:08,689",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,retrieval and there are some new
cs-410_4_5_144,"00:10:08,689","00:10:10,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"efficiency, quality information."
cs-410_4_5_145,"00:10:10,463","00:10:15,671",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,There are also new opportunities
cs-410_4_5_146,"00:10:15,671","00:10:16,765",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,"layout, etc."
cs-410_4_5_147,"00:10:17,890","00:10:22,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,A crawler is an essential component
cs-410_4_5_148,"00:10:22,500","00:10:24,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"in general, you can find two scenarios."
cs-410_4_5_149,"00:10:24,360","00:10:28,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,One is initial crawling and
cs-410_4_5_150,"00:10:30,100","00:10:32,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,of the web if you are doing
cs-410_4_5_151,"00:10:32,970","00:10:37,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,focused crawling if you want to just
cs-410_4_5_152,"00:10:38,610","00:10:43,262",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"And then, there is another scenario that's"
cs-410_4_5_153,"00:10:43,262","00:10:44,611",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,incremental crawling.
cs-410_4_5_154,"00:10:44,611","00:10:48,692",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"In this case,"
cs-410_4_5_155,"00:10:48,692","00:10:52,588",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,try to use minimum resource
cs-410_4_5_156,"00:10:54,486","00:11:04,486",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,[MUSIC]
cs-410_6_5_1,"00:00:00,025","00:00:06,042",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is about
cs-410_6_5_2,"00:00:06,042","00:00:12,849",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,link analysis for web search.
cs-410_6_5_3,"00:00:12,849","00:00:18,236",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we're going to talk"
cs-410_6_5_4,"00:00:18,236","00:00:23,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,focusing on how to do link analysis and
cs-410_6_5_5,"00:00:23,310","00:00:31,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,The main topic of this lecture is to look
cs-410_6_5_6,"00:00:32,420","00:00:35,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,In the previous lecture we talked
cs-410_6_5_7,"00:00:35,660","00:00:42,992",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,"Now that we have index, we want to see"
cs-410_6_5_8,"00:00:42,992","00:00:44,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,The web.
cs-410_6_5_9,"00:00:44,900","00:00:48,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"Now standard IR models,"
cs-410_6_5_10,"00:00:48,320","00:00:51,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"In fact,"
cs-410_6_5_11,"00:00:51,410","00:00:54,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"improve, for supporting web search."
cs-410_6_5_12,"00:00:54,390","00:00:56,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,But they aren't sufficient.
cs-410_6_5_13,"00:00:56,380","00:00:58,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And mainly for the following reasons.
cs-410_6_5_14,"00:00:58,550","00:01:02,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"First, on the web, we tend to have"
cs-410_6_5_15,"00:01:02,630","00:01:07,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"example, people might search for"
cs-410_6_5_16,"00:01:07,150","00:01:11,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,And this is different from
cs-410_6_5_17,"00:01:11,230","00:01:15,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,where people are primarily interested
cs-410_6_5_18,"00:01:15,870","00:01:19,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,So this kind of query is often
cs-410_6_5_19,"00:01:19,070","00:01:23,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,The purpose is to navigate into
cs-410_6_5_20,"00:01:23,250","00:01:28,255",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,So for such queries we might benefit
cs-410_6_5_21,"00:01:28,255","00:01:33,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"Secondly, documents have additional"
cs-410_6_5_22,"00:01:33,020","00:01:37,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"are web format,"
cs-410_6_5_23,"00:01:37,220","00:01:40,538",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"such as the layout, the title,"
cs-410_6_5_24,"00:01:40,538","00:01:45,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,So this has provided opportunity to use
cs-410_6_5_25,"00:01:45,340","00:01:49,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,extra context information of
cs-410_6_5_26,"00:01:49,800","00:01:52,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"And finally,"
cs-410_6_5_27,"00:01:52,440","00:01:56,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,That means we have to consider
cs-410_6_5_28,"00:01:56,570","00:01:58,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,the range in the algorithm.
cs-410_6_5_29,"00:01:58,400","00:02:03,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,This would give us a more robust way
cs-410_6_5_30,"00:02:03,540","00:02:09,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,any spammer to just manipulate the one
cs-410_6_5_31,"00:02:10,500","00:02:11,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"So as a result,"
cs-410_6_5_32,"00:02:11,370","00:02:15,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,people have made a number of major
cs-410_6_5_33,"00:02:16,830","00:02:21,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,One line is to exploit
cs-410_6_5_34,"00:02:23,020","00:02:24,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And that's the main topic of this lecture.
cs-410_6_5_35,"00:02:26,380","00:02:31,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,People have also proposed algorithms to
cs-410_6_5_36,"00:02:31,260","00:02:35,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,Feedback information the form of
cs-410_6_5_37,"00:02:35,370","00:02:40,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,in the category of feedback techniques and
cs-410_6_5_38,"00:02:40,720","00:02:45,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,In general in web search the ranking
cs-410_6_5_39,"00:02:45,009","00:02:49,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,algorithms to combine
cs-410_6_5_40,"00:02:49,750","00:02:55,509",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,Many of them are based on
cs-410_6_5_41,"00:02:55,509","00:03:03,341",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,as BM25 that we talked about [INAUDIBLE]
cs-410_6_5_42,"00:03:03,341","00:03:09,217",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,to provide additional features
cs-410_6_5_43,"00:03:09,217","00:03:13,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,but link information
cs-410_6_5_44,"00:03:13,364","00:03:17,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,they provide additional scoring signals.
cs-410_6_5_45,"00:03:17,660","00:03:21,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,So let's look at links in
cs-410_6_5_46,"00:03:21,080","00:03:26,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,So this is a snapshot of some
cs-410_6_5_47,"00:03:26,450","00:03:30,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,So we can see there are many links that
cs-410_6_5_48,"00:03:30,790","00:03:35,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"And in this case, you can also"
cs-410_6_5_49,"00:03:35,730","00:03:40,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,a description of a link that's pointing
cs-410_6_5_50,"00:03:40,400","00:03:42,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,"Now, this description text"
cs-410_6_5_51,"00:03:44,460","00:03:48,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"Now if you think about this text,"
cs-410_6_5_52,"00:03:48,920","00:03:53,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,because it provides some extra
cs-410_6_5_53,"00:03:53,865","00:03:59,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"So for example, if someone wants"
cs-410_6_5_54,"00:03:59,685","00:04:04,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,the person might say the biggest
cs-410_6_5_55,"00:04:04,555","00:04:07,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"then the link to Amazon, right?"
cs-410_6_5_56,"00:04:07,695","00:04:11,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"So, the description here after is very"
cs-410_6_5_57,"00:04:11,855","00:04:14,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,the query box when they are looking for
cs-410_6_5_58,"00:04:14,350","00:04:19,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,And that's why it's very useful for
cs-410_6_5_59,"00:04:19,950","00:04:25,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,Suppose someone types in
cs-410_6_5_60,"00:04:25,058","00:04:27,517",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,biggest online bookstore.
cs-410_6_5_61,"00:04:27,517","00:04:35,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,All right the query would match
cs-410_6_5_62,"00:04:35,980","00:04:39,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,And then this actually
cs-410_6_5_63,"00:04:39,650","00:04:44,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,matching the page that's being
cs-410_6_5_64,"00:04:44,090","00:04:45,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,a entry page.
cs-410_6_5_65,"00:04:45,650","00:04:50,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,So if you match anchor text that
cs-410_6_5_66,"00:04:50,120","00:04:58,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,actually that provides good evidence for
cs-410_6_5_67,"00:04:58,080","00:05:00,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,So anchor text is very useful.
cs-410_6_5_68,"00:05:00,480","00:05:03,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,If you look at the bottom part of this
cs-410_6_5_69,"00:05:03,970","00:05:08,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,patterns of some links and these links
cs-410_6_5_70,"00:05:08,380","00:05:09,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"So for example,"
cs-410_6_5_71,"00:05:09,230","00:05:14,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,on the right side you'll see this
cs-410_6_5_72,"00:05:14,200","00:05:17,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,Now that means many other pages
cs-410_6_5_73,"00:05:17,180","00:05:20,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,This shows that this page is quite useful.
cs-410_6_5_74,"00:05:21,370","00:05:24,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,On the left side you can see this
cs-410_6_5_75,"00:05:24,710","00:05:25,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,many other pages.
cs-410_6_5_76,"00:05:25,920","00:05:29,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,So this is a director page
cs-410_6_5_77,"00:05:29,040","00:05:31,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,actually see a lot of other pages.
cs-410_6_5_78,"00:05:32,670","00:05:35,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So we can call the first
cs-410_6_5_79,"00:05:35,990","00:05:41,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"the second case half page, but this means"
cs-410_6_5_80,"00:05:41,250","00:05:44,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,One is to provide extra text for matching.
cs-410_6_5_81,"00:05:44,080","00:05:49,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,The other is to provide some
cs-410_6_5_82,"00:05:49,750","00:05:53,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,to characterize how likely a page is
cs-410_6_5_83,"00:05:55,820","00:06:02,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,So people then of course and proposed
cs-410_6_5_84,"00:06:02,530","00:06:08,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,Google's PageRank which was the main
cs-410_6_5_85,"00:06:08,030","00:06:13,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,is a good example and
cs-410_6_5_86,"00:06:13,360","00:06:17,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"popularity, basically to score authority."
cs-410_6_5_87,"00:06:17,070","00:06:21,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,So the intuitions here are links
cs-410_6_5_88,"00:06:21,640","00:06:24,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,Now think about one page
cs-410_6_5_89,"00:06:24,030","00:06:27,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,this is very similar to one
cs-410_6_5_90,"00:06:27,440","00:06:30,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"So, of course then,"
cs-410_6_5_91,"00:06:30,360","00:06:33,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,then we can assume this page
cs-410_6_5_92,"00:06:35,120","00:06:36,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,So that's a very good intuition.
cs-410_6_5_93,"00:06:38,060","00:06:42,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,Now PageRank is essentially to take
cs-410_6_5_94,"00:06:42,950","00:06:46,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,implement with the principal approach.
cs-410_6_5_95,"00:06:46,650","00:06:51,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,"Intuitively, it is essentially doing"
cs-410_6_5_96,"00:06:51,980","00:06:56,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,It just improves the simple
cs-410_6_5_97,"00:06:56,390","00:06:59,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,One it will consider indirect citations.
cs-410_6_5_98,"00:06:59,420","00:07:04,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,So that means you don't just look
cs-410_6_5_99,"00:07:04,010","00:07:08,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,You also look at what are those
cs-410_6_5_100,"00:07:08,550","00:07:13,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,If those pages themselves have a lot
cs-410_6_5_101,"00:07:13,530","00:07:16,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"In some sense,"
cs-410_6_5_102,"00:07:16,750","00:07:20,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,But if those pages that
cs-410_6_5_103,"00:07:20,080","00:07:25,095",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,being pointed to by other pages they
cs-410_6_5_104,"00:07:25,095","00:07:27,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,"then well, you don't get that much."
cs-410_6_5_105,"00:07:27,360","00:07:29,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,So that's the idea of
cs-410_6_5_106,"00:07:29,830","00:07:31,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,"All right, so"
cs-410_6_5_107,"00:07:31,770","00:07:37,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,you can also understand this idea by
cs-410_6_5_108,"00:07:37,060","00:07:42,082",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"If you're cited by let's say ten papers,"
cs-410_6_5_109,"00:07:42,082","00:07:48,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,are just workshop papers or some papers
cs-410_6_5_110,"00:07:49,580","00:07:54,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"So although you've got ten in-links,"
cs-410_6_5_111,"00:07:54,340","00:07:59,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,are cited by ten papers that themselves
cs-410_6_5_112,"00:08:01,770","00:08:06,563",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,And so in this case where we would
cs-410_6_5_113,"00:08:06,563","00:08:08,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,page does that.
cs-410_6_5_114,"00:08:08,530","00:08:12,174",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,The other idea is it's
cs-410_6_5_115,"00:08:13,810","00:08:21,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,Assume that basically every page is having
cs-410_6_5_116,"00:08:21,120","00:08:23,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,Essentially you are trying to
cs-410_6_5_117,"00:08:23,950","00:08:27,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,links that will link all
cs-410_6_5_118,"00:08:27,510","00:08:32,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,that you actually get the pseudo
cs-410_6_5_119,"00:08:34,300","00:08:36,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,The reason why they want to do that.
cs-410_6_5_120,"00:08:36,760","00:08:41,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Is this will allow them
cs-410_6_5_121,"00:08:41,980","00:08:47,348",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,elegantly with linear algebra technique.
cs-410_6_5_122,"00:08:47,348","00:08:52,354",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"So, I think maybe the best"
cs-410_6_5_123,"00:08:52,354","00:08:58,172",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,the PageRank is to think
cs-410_6_5_124,"00:08:58,172","00:09:04,549",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,probability of random surfer
cs-410_6_5_125,"00:09:04,549","00:09:14,549",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,[MUSIC]
cs-410_7_5_1,"00:00:00,049","00:00:03,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_7_5_2,"00:00:09,183","00:00:12,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,So let's take a look at this in detail.
cs-410_7_5_3,"00:00:12,025","00:00:17,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,So in this random surfing
cs-410_7_5_4,"00:00:17,269","00:00:22,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,random surfer would choose
cs-410_7_5_5,"00:00:22,575","00:00:25,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So this is a small graph here.
cs-410_7_5_6,"00:00:25,225","00:00:29,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"That's of course, over simplification"
cs-410_7_5_7,"00:00:29,490","00:00:35,207",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,But let's say there are four
cs-410_7_5_8,"00:00:35,207","00:00:41,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,And let's assume that a random surfer or
cs-410_7_5_9,"00:00:41,360","00:00:46,373",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,And then the random
cs-410_7_5_10,"00:00:46,373","00:00:50,439",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,just randomly jumping to any page or
cs-410_7_5_11,"00:00:50,439","00:00:55,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,follow a link and
cs-410_7_5_12,"00:00:56,440","00:00:59,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"So if the random surfer is at d1,"
cs-410_7_5_13,"00:01:01,100","00:01:06,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,then there is some probability that
cs-410_7_5_14,"00:01:06,260","00:01:09,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Now there are two outlinks here,"
cs-410_7_5_15,"00:01:09,510","00:01:12,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,the other is pointing to d4.
cs-410_7_5_16,"00:01:12,740","00:01:19,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,So the random surfer could pick any
cs-410_7_5_17,"00:01:19,020","00:01:25,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,But it also assumes that the random so
cs-410_7_5_18,"00:01:25,800","00:01:30,586",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,So the random surfing which decide
cs-410_7_5_19,"00:01:30,586","00:01:34,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,simply randomly jump
cs-410_7_5_20,"00:01:34,050","00:01:39,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"So if it does that, it would be able"
cs-410_7_5_21,"00:01:39,760","00:01:45,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"though there's no link you actually,"
cs-410_7_5_22,"00:01:46,170","00:01:49,713",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,So this is to assume that
cs-410_7_5_23,"00:01:49,713","00:01:54,852",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,Imagine a random surfer is
cs-410_7_5_24,"00:01:54,852","00:01:59,989",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,then we can ask the question how
cs-410_7_5_25,"00:01:59,989","00:02:05,864",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,would actually reach a particular
cs-410_7_5_26,"00:02:05,864","00:02:09,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,That's the average probability of
cs-410_7_5_27,"00:02:09,824","00:02:13,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,this probability is precisely
cs-410_7_5_28,"00:02:13,830","00:02:17,558",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,So the page rank score of
cs-410_7_5_29,"00:02:17,558","00:02:21,644",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,probability that the surfer
cs-410_7_5_30,"00:02:21,644","00:02:27,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"Now intuitively, this would basically"
cs-410_7_5_31,"00:02:27,220","00:02:30,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"Because if a page has a lot of inlinks,"
cs-410_7_5_32,"00:02:30,970","00:02:34,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,then it would have a higher
cs-410_7_5_33,"00:02:34,580","00:02:37,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,Because there will be more
cs-410_7_5_34,"00:02:37,650","00:02:39,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,follow a link to come to this page.
cs-410_7_5_35,"00:02:41,290","00:02:45,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,And this is why the random surfing model
cs-410_7_5_36,"00:02:45,030","00:02:48,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,actually captures the ID
cs-410_7_5_37,"00:02:48,510","00:02:52,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,Note that it also considers
cs-410_7_5_38,"00:02:52,700","00:02:59,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Because if the page is that point then
cs-410_7_5_39,"00:02:59,690","00:03:04,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,That would mean the random surfer would
cs-410_7_5_40,"00:03:04,550","00:03:07,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"therefore, it increase"
cs-410_7_5_41,"00:03:07,680","00:03:13,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,So this is just a nice way to capture
cs-410_7_5_42,"00:03:13,580","00:03:18,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"So mathematically, how can we compute this"
cs-410_7_5_43,"00:03:18,440","00:03:22,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,we need to take a look at how this
cs-410_7_5_44,"00:03:22,390","00:03:25,184",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,So first of all let's take a look
cs-410_7_5_45,"00:03:25,184","00:03:29,437",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,And this is just metrics with
cs-410_7_5_46,"00:03:29,437","00:03:33,273",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,the random surfer would go
cs-410_7_5_47,"00:03:33,273","00:03:37,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,So each rule stands for a starting page.
cs-410_7_5_48,"00:03:37,230","00:03:41,754",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"For example, rule one would"
cs-410_7_5_49,"00:03:41,754","00:03:44,581",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,to any of the other four pages from d1.
cs-410_7_5_50,"00:03:44,581","00:03:53,097",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,And here we see there are only
cs-410_7_5_51,"00:03:53,097","00:04:01,492",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,So this is because if you look at
cs-410_7_5_52,"00:04:01,492","00:04:05,918",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,There is no link from d1 or d2.
cs-410_7_5_53,"00:04:05,918","00:04:10,579",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,So we've got 0s for the first 2
cs-410_7_5_54,"00:04:10,579","00:04:15,762",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,columns and 0.5 for d3 and d4.
cs-410_7_5_55,"00:04:15,762","00:04:19,416",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"In general, the M in this matrix,"
cs-410_7_5_56,"00:04:19,416","00:04:24,586",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,M sub ij is the probability
cs-410_7_5_57,"00:04:24,586","00:04:29,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"And obviously for each rule,"
cs-410_7_5_58,"00:04:29,668","00:04:36,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,because the surfer would have to go to
cs-410_7_5_59,"00:04:36,115","00:04:39,196",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,So this is a transition metric.
cs-410_7_5_60,"00:04:39,196","00:04:43,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,Now how can we compute the probability
cs-410_7_5_61,"00:04:44,900","00:04:49,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,Well if you look at the surf
cs-410_7_5_62,"00:04:50,910","00:04:56,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,we can compute the probability
cs-410_7_5_63,"00:04:56,280","00:05:01,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"So here on the left hand side,"
cs-410_7_5_64,"00:05:02,170","00:05:08,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"visiting page dj at time plus 1,"
cs-410_7_5_65,"00:05:08,540","00:05:14,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"On the right hand side, you can see"
cs-410_7_5_66,"00:05:14,740","00:05:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,of at page di at time t.
cs-410_7_5_67,"00:05:21,408","00:05:26,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,So you can see the subscript
cs-410_7_5_68,"00:05:26,020","00:05:34,314",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,that indicates that's the probability that
cs-410_7_5_69,"00:05:34,314","00:05:38,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"So the equation basically,"
cs-410_7_5_70,"00:05:38,500","00:05:43,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,possibilities of reaching
cs-410_7_5_71,"00:05:43,790","00:05:45,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,What are these two possibilities?
cs-410_7_5_72,"00:05:45,510","00:05:48,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,Well one is through random surfing and
cs-410_7_5_73,"00:05:48,200","00:05:51,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"one is through following a link,"
cs-410_7_5_74,"00:05:53,500","00:05:56,612",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,So the first part captures the probability
cs-410_7_5_75,"00:05:56,612","00:06:01,373",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,that the random surfer would reach
cs-410_7_5_76,"00:06:01,373","00:06:06,283",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,And you can see the random
cs-410_7_5_77,"00:06:06,283","00:06:10,455",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,with probability 1 minus
cs-410_7_5_78,"00:06:10,455","00:06:14,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,And so
cs-410_7_5_79,"00:06:14,200","00:06:18,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,But the main party is realist
cs-410_7_5_80,"00:06:18,250","00:06:22,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,that the surfer could have been at time t.
cs-410_7_5_81,"00:06:23,760","00:06:27,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,There are n pages so
cs-410_7_5_82,"00:06:27,890","00:06:31,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Inside the sum is a product
cs-410_7_5_83,"00:06:31,730","00:06:36,763",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,One is the probability that the surfer
cs-410_7_5_84,"00:06:36,763","00:06:42,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"was at di at time t, that's p sub t of di."
cs-410_7_5_85,"00:06:42,115","00:06:47,422",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,The other is the transition
cs-410_7_5_86,"00:06:47,422","00:06:52,217",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,"And so in order to reach this dj page,"
cs-410_7_5_87,"00:06:52,217","00:06:57,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,the surfer must first be at di at time t.
cs-410_7_5_88,"00:06:57,880","00:07:03,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,"And then also, would also have to"
cs-410_7_5_89,"00:07:03,090","00:07:09,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,So the probability is the probability
cs-410_7_5_90,"00:07:09,090","00:07:15,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,the probability of going from that
cs-410_7_5_91,"00:07:15,950","00:07:20,792",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"The second part is a similar sum, the only"
cs-410_7_5_92,"00:07:20,792","00:07:23,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,probability is a uniform
cs-410_7_5_93,"00:07:23,980","00:07:27,708",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,1 over n and
cs-410_7_5_94,"00:07:27,708","00:07:31,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,of reaching this page
cs-410_7_5_95,"00:07:32,630","00:07:37,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,So the form is exactly the same and
cs-410_7_5_96,"00:07:37,520","00:07:43,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,see on why PageRank is essentially assumed
cs-410_7_5_97,"00:07:43,310","00:07:49,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,If you think about this 1 over n as
cs-410_7_5_98,"00:07:49,070","00:07:55,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,that has all the elements being
cs-410_7_5_99,"00:07:55,320","00:07:59,621",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Then you can see very clearly
cs-410_7_5_100,"00:07:59,621","00:08:01,784",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,because they are of the same form.
cs-410_7_5_101,"00:08:01,784","00:08:07,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,We can imagine there's a different
cs-410_7_5_102,"00:08:07,310","00:08:11,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,that uniform metrics where
cs-410_7_5_103,"00:08:11,340","00:08:16,347",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,And in this sense PageRank uses
cs-410_7_5_104,"00:08:16,347","00:08:22,312",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,ensuring that there's no zero entry
cs-410_7_5_105,"00:08:22,312","00:08:28,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,Now of course this is the time dependent
cs-410_7_5_106,"00:08:28,530","00:08:32,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"Now we can imagine, if we'll compute"
cs-410_7_5_107,"00:08:32,480","00:08:36,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,the average of probabilities probably
cs-410_7_5_108,"00:08:36,420","00:08:38,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,without considering the time index.
cs-410_7_5_109,"00:08:38,320","00:08:41,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,So let's drop the time index and
cs-410_7_5_110,"00:08:42,910","00:08:47,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"Now this would give us any equations,"
cs-410_7_5_111,"00:08:47,100","00:08:49,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,each page we have such equation.
cs-410_7_5_112,"00:08:49,520","00:08:52,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,And if you look at the what
cs-410_7_5_113,"00:08:52,800","00:08:55,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,there are also precisely n variables.
cs-410_7_5_114,"00:08:58,280","00:09:03,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"So this basically means,"
cs-410_7_5_115,"00:09:04,600","00:09:10,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,n equations with n variables and
cs-410_7_5_116,"00:09:10,260","00:09:16,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"So basically, now the problem boils"
cs-410_7_5_117,"00:09:16,420","00:09:20,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,"And here, I also show"
cs-410_7_5_118,"00:09:20,950","00:09:26,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,It's the vector p here equals a matrix or
cs-410_7_5_119,"00:09:26,690","00:09:31,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,the transpose of the matrix here and
cs-410_7_5_120,"00:09:32,580","00:09:36,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"Now, if you still remember some knowledge"
cs-410_7_5_121,"00:09:36,890","00:09:42,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"and then you will realize, this is"
cs-410_7_5_122,"00:09:42,140","00:09:47,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,"When multiply the metrics by this vector,"
cs-410_7_5_123,"00:09:47,690","00:09:52,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,this can be solved by
cs-410_7_5_124,"00:09:54,700","00:09:57,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,So because the equations here
cs-410_7_5_125,"00:09:57,380","00:10:02,002",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,on the back are basically
cs-410_7_5_126,"00:10:02,002","00:10:09,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,So you'll see the relation between the
cs-410_7_5_127,"00:10:09,170","00:10:13,844",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,And this iterative approach or
cs-410_7_5_128,"00:10:13,844","00:10:19,093",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,we simply start with s
cs-410_7_5_129,"00:10:19,093","00:10:24,242",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,And then we repeatedly
cs-410_7_5_130,"00:10:24,242","00:10:29,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,multiplying the metrics
cs-410_7_5_131,"00:10:31,360","00:10:37,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,I also show a concrete example here.
cs-410_7_5_132,"00:10:37,820","00:10:40,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,So you can see this now.
cs-410_7_5_133,"00:10:40,130","00:10:43,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,"If we assume alpha is 0.2,"
cs-410_7_5_134,"00:10:43,368","00:10:49,066",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,then with the example that
cs-410_7_5_135,"00:10:49,066","00:10:54,393",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,we have the original
cs-410_7_5_136,"00:10:54,393","00:10:59,943",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"That includes the graph, the actual links"
cs-410_7_5_137,"00:10:59,943","00:11:04,856",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"metrics, uniform transition metrics"
cs-410_7_5_138,"00:11:04,856","00:11:09,707",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,And we can combine them together with
cs-410_7_5_139,"00:11:09,707","00:11:12,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,metric that would be like this.
cs-410_7_5_140,"00:11:12,260","00:11:13,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"So essentially,"
cs-410_7_5_141,"00:11:13,320","00:11:18,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,we can imagine now the web looks like
cs-410_7_5_142,"00:11:18,300","00:11:22,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,They're all virtual links
cs-410_7_5_143,"00:11:22,300","00:11:27,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,The page we're on now would just
cs-410_7_5_144,"00:11:27,210","00:11:30,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,then just computed the updating of this
cs-410_7_5_145,"00:11:30,270","00:11:34,899",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,p vector by using this
cs-410_7_5_146,"00:11:36,600","00:11:40,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,Now if you rewrite this
cs-410_7_5_147,"00:11:42,020","00:11:45,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"terms of individual equations,"
cs-410_7_5_148,"00:11:46,530","00:11:51,435",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,"And this is basically,"
cs-410_7_5_149,"00:11:51,435","00:11:54,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,this particular pages and page score.
cs-410_7_5_150,"00:11:54,385","00:11:59,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,So you can also see if you want to compute
cs-410_7_5_151,"00:11:59,364","00:12:04,617",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,You basically multiply
cs-410_7_5_152,"00:12:04,617","00:12:09,379",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,and we'll take the third
cs-410_7_5_153,"00:12:09,379","00:12:13,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=729,And that will give us the value for
cs-410_7_5_154,"00:12:16,000","00:12:20,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,So this is how we updated the vector
cs-410_7_5_155,"00:12:20,170","00:12:23,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,these guys for this.
cs-410_7_5_156,"00:12:23,270","00:12:28,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,And then we just revise
cs-410_7_5_157,"00:12:28,080","00:12:31,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,set of scores and
cs-410_7_5_158,"00:12:33,150","00:12:37,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,So we just repeatedly apply this and
cs-410_7_5_159,"00:12:37,590","00:12:41,432",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,"And when the matrix is like this,"
cs-410_7_5_160,"00:12:41,432","00:12:43,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,it can be guaranteed to converge.
cs-410_7_5_161,"00:12:44,790","00:12:49,765",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,And at that point the we will just have
cs-410_7_5_162,"00:12:49,765","00:12:53,101",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,We typically go to sets of
cs-410_7_5_163,"00:12:55,300","00:12:58,543",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So interestingly,"
cs-410_7_5_164,"00:12:58,543","00:13:03,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,also interpreted as propagating
cs-410_7_5_165,"00:13:03,296","00:13:08,847",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,Or if you look at this formula and
cs-410_7_5_166,"00:13:08,847","00:13:13,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"can you imagine,"
cs-410_7_5_167,"00:13:13,440","00:13:17,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,essentially propagating
cs-410_7_5_168,"00:13:17,479","00:13:19,801",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,"I hope you will see that indeed,"
cs-410_7_5_169,"00:13:19,801","00:13:24,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,we can imagine we have values
cs-410_7_5_170,"00:13:24,565","00:13:30,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,So we can have values here and
cs-410_7_5_171,"00:13:30,220","00:13:35,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,And then we're going to use these
cs-410_7_5_172,"00:13:35,170","00:13:42,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,And if you look at the equation here
cs-410_7_5_173,"00:13:42,290","00:13:48,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,to combine the scores of the pages that
cs-410_7_5_174,"00:13:48,890","00:13:54,067",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,So we'll look at all the pages
cs-410_7_5_175,"00:13:54,067","00:14:00,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,then combine this score and propagate the
cs-410_7_5_176,"00:14:00,916","00:14:06,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,To look at the scores that we present
cs-410_7_5_177,"00:14:06,145","00:14:11,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,surfer would be visiting the other
cs-410_7_5_178,"00:14:11,410","00:14:16,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,And then just do
cs-410_7_5_179,"00:14:16,275","00:14:21,409",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,"the probability of reaching this page, d1."
cs-410_7_5_180,"00:14:21,409","00:14:23,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,So there are two interpretations here.
cs-410_7_5_181,"00:14:23,910","00:14:26,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,One is just the matrix multiplication.
cs-410_7_5_182,"00:14:26,364","00:14:31,498",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,We repeat the multiplying
cs-410_7_5_183,"00:14:31,498","00:14:35,204",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,The other is to just think
cs-410_7_5_184,"00:14:35,204","00:14:38,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,these scores repeatedly on the web.
cs-410_7_5_185,"00:14:38,180","00:14:43,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,"So in practice, the combination of"
cs-410_7_5_186,"00:14:43,150","00:14:48,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,Because the matrices is fast and there
cs-410_7_5_187,"00:14:48,820","00:14:53,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=888,So that you avoid actually
cs-410_7_5_188,"00:14:53,820","00:14:55,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,all those elements.
cs-410_7_5_189,"00:14:56,670","00:15:00,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=896,Sometimes you may also normalize the
cs-410_7_5_190,"00:15:00,670","00:15:05,249",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=900,"different form of the equation, but"
cs-410_7_5_191,"00:15:06,290","00:15:09,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=906,The results of this potential
cs-410_7_5_192,"00:15:10,740","00:15:17,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,"In that case, if a page does not have"
cs-410_7_5_193,"00:15:17,540","00:15:22,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,these pages would not sum to 1.
cs-410_7_5_194,"00:15:22,250","00:15:26,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,"Basically, the probability of reaching the"
cs-410_7_5_195,"00:15:26,349","00:15:29,246",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,"1, mainly because we have lost"
cs-410_7_5_196,"00:15:29,246","00:15:33,588",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=929,One would assume there's some probability
cs-410_7_5_197,"00:15:33,588","00:15:37,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,"the links, but"
cs-410_7_5_198,"00:15:37,160","00:15:42,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=937,And one possible solution is simply to use
cs-410_7_5_199,"00:15:42,980","00:15:45,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,and that could easily fix this.
cs-410_7_5_200,"00:15:46,740","00:15:50,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=946,"Basically, that's to say alpha would"
cs-410_7_5_201,"00:15:50,750","00:15:54,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,"In that case,"
cs-410_7_5_202,"00:15:54,130","00:15:57,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=954,randomly jump to another page
cs-410_7_5_203,"00:15:59,120","00:16:05,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"There are many extensions of PageRank, one"
cs-410_7_5_204,"00:16:05,060","00:16:11,639",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,Note that PageRank doesn't merely
cs-410_7_5_205,"00:16:11,639","00:16:15,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,So we can make PageRank specific however.
cs-410_7_5_206,"00:16:15,370","00:16:19,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=975,"So for example,"
cs-410_7_5_207,"00:16:19,260","00:16:22,567",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,we can simply assume
cs-410_7_5_208,"00:16:22,567","00:16:25,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,The surfer is not randomly
cs-410_7_5_209,"00:16:25,660","00:16:32,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,"Instead, he's going to jump to only those"
cs-410_7_5_210,"00:16:32,320","00:16:36,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,"For example, if the query is not sports"
cs-410_7_5_211,"00:16:36,780","00:16:40,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=996,"doing random jumping, it's going"
cs-410_7_5_212,"00:16:40,670","00:16:45,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"By doing this, then we can buy"
cs-410_7_5_213,"00:16:45,350","00:16:49,054",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,And then if you know the current
cs-410_7_5_214,"00:16:49,054","00:16:53,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,then you can use this specialized
cs-410_7_5_215,"00:16:53,368","00:16:57,754",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,That would be better than if you
cs-410_7_5_216,"00:16:57,754","00:17:01,877",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,PageRank is also a channel that can be
cs-410_7_5_217,"00:17:01,877","00:17:06,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,network analysis particularly for
cs-410_7_5_218,"00:17:06,100","00:17:09,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1026,You can imagine if you compute
cs-410_7_5_219,"00:17:09,970","00:17:14,356",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1029,"social network, where a link"
cs-410_7_5_220,"00:17:14,356","00:17:18,744",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1034,"a relation, you would get some"
cs-410_7_5_221,"00:17:18,744","00:17:28,744",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,[MUSIC]
cs-410_1_5_1,"00:00:00,012","00:00:07,436",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_5_2,"00:00:07,436","00:00:10,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the feedback
cs-410_1_5_3,"00:00:12,910","00:00:17,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"So in this lecture, we will continue with"
cs-410_1_5_4,"00:00:18,840","00:00:22,103",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In particular, we're going to talk"
cs-410_1_5_5,"00:00:24,866","00:00:28,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,This is a diagram that shows
cs-410_1_5_6,"00:00:30,685","00:00:34,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,We can see the user would type in a query.
cs-410_1_5_7,"00:00:37,365","00:00:41,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"And then, the query would be"
cs-410_1_5_8,"00:00:41,965","00:00:46,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"search engine, and"
cs-410_1_5_9,"00:00:46,015","00:00:47,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,These results would be issued to the user.
cs-410_1_5_10,"00:00:49,475","00:00:52,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,"Now, after the user has"
cs-410_1_5_11,"00:00:52,760","00:00:55,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,the user can actually make judgements.
cs-410_1_5_12,"00:00:55,410","00:00:59,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"So for example, the user says,"
cs-410_1_5_13,"00:00:59,009","00:01:03,097",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,this document is not very useful and
cs-410_1_5_14,"00:01:03,097","00:01:07,921",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"Now, this is called a relevance judgment"
cs-410_1_5_15,"00:01:07,921","00:01:12,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,got some feedback information from
cs-410_1_5_16,"00:01:12,510","00:01:14,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"And this can be very useful to the system,"
cs-410_1_5_17,"00:01:14,930","00:01:18,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,knowing what exactly is
cs-410_1_5_18,"00:01:18,320","00:01:22,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,So the feedback module would
cs-410_1_5_19,"00:01:22,790","00:01:26,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,also use the document collection
cs-410_1_5_20,"00:01:26,970","00:01:30,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,Typically it would involve
cs-410_1_5_21,"00:01:30,720","00:01:34,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,the system can now render the results
cs-410_1_5_22,"00:01:34,960","00:01:36,897",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,So this is called relevance feedback.
cs-410_1_5_23,"00:01:36,897","00:01:42,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,The feedback is based on relevance
cs-410_1_5_24,"00:01:42,470","00:01:44,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"Now, these judgements are reliable but"
cs-410_1_5_25,"00:01:44,660","00:01:50,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,the users generally don't want to make
cs-410_1_5_26,"00:01:50,350","00:01:54,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,So the down side is that it involves
cs-410_1_5_27,"00:01:57,250","00:02:00,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,There's another form of feedback
cs-410_1_5_28,"00:02:00,920","00:02:03,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"blind feedback,"
cs-410_1_5_29,"00:02:03,800","00:02:08,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"In this case, we can see once"
cs-410_1_5_30,"00:02:08,380","00:02:11,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,in fact we don't have to invoke users.
cs-410_1_5_31,"00:02:11,340","00:02:13,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So you can see there's
cs-410_1_5_32,"00:02:14,730","00:02:19,846",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,And we simply assume that the top
cs-410_1_5_33,"00:02:19,846","00:02:23,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,Let's say we have assumed
cs-410_1_5_34,"00:02:25,250","00:02:31,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"And then, we will then use this"
cs-410_1_5_35,"00:02:31,000","00:02:33,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,and to improve the query.
cs-410_1_5_36,"00:02:34,110","00:02:35,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"Now, you might wonder,"
cs-410_1_5_37,"00:02:35,821","00:02:40,887",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,how could this help if we simply
cs-410_1_5_38,"00:02:40,887","00:02:46,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"Well, you can imagine these top"
cs-410_1_5_39,"00:02:46,490","00:02:52,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,similar to relevant documents
cs-410_1_5_40,"00:02:52,070","00:02:53,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,They look like relevant documents.
cs-410_1_5_41,"00:02:53,480","00:02:59,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,So it's possible to learn some related
cs-410_1_5_42,"00:02:59,350","00:03:03,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"In fact, you may recall that we"
cs-410_1_5_43,"00:03:03,610","00:03:08,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"analyze what association, to learn"
cs-410_1_5_44,"00:03:09,480","00:03:13,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"And there, what we did is we"
cs-410_1_5_45,"00:03:13,040","00:03:15,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,all the documents that contain computer.
cs-410_1_5_46,"00:03:15,500","00:03:18,761",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,So imagine now the query
cs-410_1_5_47,"00:03:18,761","00:03:23,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"And then, the result will be those"
cs-410_1_5_48,"00:03:23,870","00:03:29,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,And what we can do then is
cs-410_1_5_49,"00:03:29,040","00:03:31,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,They can match computer very well.
cs-410_1_5_50,"00:03:31,860","00:03:36,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,And we're going to count
cs-410_1_5_51,"00:03:36,890","00:03:42,126",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"And then, we're going to then use"
cs-410_1_5_52,"00:03:42,126","00:03:47,794",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,the terms that are frequent in this set
cs-410_1_5_53,"00:03:47,794","00:03:52,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So if we make a contrast between
cs-410_1_5_54,"00:03:52,364","00:03:57,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,is that related to terms
cs-410_1_5_55,"00:03:57,360","00:03:58,528",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,As we have seen before.
cs-410_1_5_56,"00:03:58,528","00:04:04,786",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,And these related words can then be added
cs-410_1_5_57,"00:04:04,786","00:04:08,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,And this would help us bring the documents
cs-410_1_5_58,"00:04:08,770","00:04:11,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,match other words like program and
cs-410_1_5_59,"00:04:11,640","00:04:16,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So this is very effective for
cs-410_1_5_60,"00:04:18,590","00:04:21,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,"But of course, pseudo-relevancy"
cs-410_1_5_61,"00:04:21,790","00:04:24,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,We have to arbitrarily set a cut off.
cs-410_1_5_62,"00:04:24,050","00:04:27,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,So there's also something in
cs-410_1_5_63,"00:04:27,010","00:04:31,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"In this case,"
cs-410_1_5_64,"00:04:31,120","00:04:33,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,we don't have to ask
cs-410_1_5_65,"00:04:33,510","00:04:38,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"Instead, we're going to observe how the"
cs-410_1_5_66,"00:04:38,730","00:04:41,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,So in this case we'll look
cs-410_1_5_67,"00:04:41,760","00:04:43,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So the user clicked on this one.
cs-410_1_5_68,"00:04:43,930","00:04:45,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,And the user viewed this one.
cs-410_1_5_69,"00:04:45,620","00:04:47,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,And the user skipped this one.
cs-410_1_5_70,"00:04:47,480","00:04:49,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,And the user viewed this one again.
cs-410_1_5_71,"00:04:50,410","00:04:56,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"Now, this also is a clue about whether"
cs-410_1_5_72,"00:04:56,880","00:05:01,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,And we can even assume that we're
cs-410_1_5_73,"00:05:01,540","00:05:05,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"here in this document,"
cs-410_1_5_74,"00:05:05,930","00:05:10,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,instead of the actual
cs-410_1_5_75,"00:05:10,810","00:05:15,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,The link they are saying web search
cs-410_1_5_76,"00:05:15,370","00:05:20,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,If the user tries to fetch this
cs-410_1_5_77,"00:05:20,250","00:05:25,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,we can assume these displayed
cs-410_1_5_78,"00:05:25,400","00:05:29,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,is interesting to you so
cs-410_1_5_79,"00:05:29,310","00:05:31,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,And this is called interesting feedback.
cs-410_1_5_80,"00:05:31,830","00:05:35,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"And we can, again,"
cs-410_1_5_81,"00:05:35,400","00:05:39,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,This is a very important
cs-410_1_5_82,"00:05:39,760","00:05:42,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"Now, think about the Google and Bing and"
cs-410_1_5_83,"00:05:42,080","00:05:46,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,they can collect a lot of user
cs-410_1_5_84,"00:05:46,990","00:05:51,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,So they would observe what documents
cs-410_1_5_85,"00:05:51,320","00:05:54,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,And this information is very valuable.
cs-410_1_5_86,"00:05:54,040","00:05:57,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,And they can use this to
cs-410_1_5_87,"00:05:59,040","00:06:03,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"So to summarize, we talked about"
cs-410_1_5_88,"00:06:03,625","00:06:07,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,Relevant feedback where the user
cs-410_1_5_89,"00:06:07,280","00:06:11,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"It takes some user effort, but"
cs-410_1_5_90,"00:06:11,200","00:06:15,931",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,We talk about the pseudo feedback where
cs-410_1_5_91,"00:06:15,931","00:06:17,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,will be relevant.
cs-410_1_5_92,"00:06:17,310","00:06:20,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,We don't have to involve the user
cs-410_1_5_93,"00:06:20,540","00:06:23,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,actually before we return
cs-410_1_5_94,"00:06:24,850","00:06:28,014",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,And the third is implicit feedback
cs-410_1_5_95,"00:06:29,685","00:06:31,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"Where we involve the users, but"
cs-410_1_5_96,"00:06:31,530","00:06:34,887",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,the user doesn't have to make
cs-410_1_5_97,"00:06:34,887","00:06:36,118",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,Make judgement.
cs-410_1_5_98,"00:06:36,118","00:06:46,118",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,[MUSIC]
cs-410_3_5_1,"00:00:00,000","00:00:07,194",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_5_2,"00:00:07,194","00:00:10,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the feedback in
cs-410_3_5_3,"00:00:12,540","00:00:17,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we will continue the"
cs-410_3_5_4,"00:00:17,520","00:00:18,089",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"In particular,"
cs-410_3_5_5,"00:00:18,089","00:00:20,659",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,we're going to talk about the feedback
cs-410_3_5_6,"00:00:23,450","00:00:29,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So we derive the query likelihood ranking
cs-410_3_5_7,"00:00:30,410","00:00:35,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"As a basic retrieval function,"
cs-410_3_5_8,"00:00:35,860","00:00:39,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,But if we think about the feedback
cs-410_3_5_9,"00:00:39,920","00:00:44,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"use query likelihood to perform feedback,"
cs-410_3_5_10,"00:00:44,730","00:00:49,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,a lot of times the feedback information is
cs-410_3_5_11,"00:00:49,620","00:00:53,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,But we assume the query has
cs-410_3_5_12,"00:00:53,260","00:00:56,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,from a language model in
cs-410_3_5_13,"00:00:56,850","00:01:03,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,It's kind of unnatural to sample
cs-410_3_5_14,"00:01:03,170","00:01:10,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"As a result, researchers proposed a way"
cs-410_3_5_15,"00:01:10,330","00:01:14,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,and it's called Kullback-Leibler
cs-410_3_5_16,"00:01:15,450","00:01:20,422",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,And this model is actually going
cs-410_3_5_17,"00:01:20,422","00:01:25,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,retrieval function much
cs-410_3_5_18,"00:01:25,780","00:01:32,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,Yet this form of the language model
cs-410_3_5_19,"00:01:32,380","00:01:36,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"query likelihood, in the sense that it can"
cs-410_3_5_20,"00:01:38,180","00:01:39,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"And in this case,"
cs-410_3_5_21,"00:01:39,300","00:01:44,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,then feedback can be achieved through
cs-410_3_5_22,"00:01:44,140","00:01:48,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"This is very similar to Rocchio,"
cs-410_3_5_23,"00:01:50,000","00:01:55,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,So let's see what is this
cs-410_3_5_24,"00:01:55,720","00:02:02,306",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"So on the top, what you see is a query"
cs-410_3_5_25,"00:02:05,072","00:02:11,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"And then KL-divergence, or"
cs-410_3_5_26,"00:02:11,465","00:02:16,292",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,retrieval model is basically to generalize
cs-410_3_5_27,"00:02:16,292","00:02:21,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,the frequency part here
cs-410_3_5_28,"00:02:21,600","00:02:26,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,So basically it's the difference given
cs-410_3_5_29,"00:02:26,910","00:02:32,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,by the probabilistic model here to
cs-410_3_5_30,"00:02:32,260","00:02:34,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,versus the count of query words there.
cs-410_3_5_31,"00:02:35,820","00:02:42,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,And this difference allows us to plug in
cs-410_3_5_32,"00:02:42,610","00:02:45,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So this can be estimated
cs-410_3_5_33,"00:02:45,690","00:02:48,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,including using feedback information.
cs-410_3_5_34,"00:02:48,260","00:02:51,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"But this is called a KL-divergence,"
cs-410_3_5_35,"00:02:51,370","00:02:56,232",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,this can be interpreted as matching
cs-410_3_5_36,"00:02:56,232","00:03:02,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"One is the query model,"
cs-410_3_5_37,"00:03:02,770","00:03:06,317",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,One is the document
cs-410_3_5_38,"00:03:06,317","00:03:11,255",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,smooth them with a collection
cs-410_3_5_39,"00:03:11,255","00:03:15,377",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,And we are not going to talk
cs-410_3_5_40,"00:03:15,377","00:03:18,107",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,you'll find it in some references.
cs-410_3_5_41,"00:03:18,107","00:03:22,023",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"It's also called cross entropy because,"
cs-410_3_5_42,"00:03:22,023","00:03:26,207",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,we ignore some terms in
cs-410_3_5_43,"00:03:26,207","00:03:29,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,we will end up having
cs-410_3_5_44,"00:03:29,690","00:03:32,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,And both are terms of information theory.
cs-410_3_5_45,"00:03:34,390","00:03:38,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"But anyway, for our purposes here,"
cs-410_3_5_46,"00:03:38,650","00:03:42,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,you can just see the two
cs-410_3_5_47,"00:03:42,820","00:03:48,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,except that here we have a probability of
cs-410_3_5_48,"00:03:52,140","00:03:57,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,And here the sum is over all the words
cs-410_3_5_49,"00:03:57,730","00:04:02,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,also with the nonzero probability for
cs-410_3_5_50,"00:04:02,340","00:04:07,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"So it's kind of, again, a generalization"
cs-410_3_5_51,"00:04:09,930","00:04:15,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,Now you can also easily see we can recover
cs-410_3_5_52,"00:04:15,980","00:04:22,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,by simply setting this query model to the
cs-410_3_5_53,"00:04:23,450","00:04:26,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,This is very easy to
cs-410_3_5_54,"00:04:26,510","00:04:30,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,into here you can eliminate this
cs-410_3_5_55,"00:04:30,005","00:04:33,486",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,And then you will get exactly like that.
cs-410_3_5_56,"00:04:33,486","00:04:35,879",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,So you can see the equivalence.
cs-410_3_5_57,"00:04:35,879","00:04:41,581",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,And that's also why this KL-divergence
cs-410_3_5_58,"00:04:41,581","00:04:47,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"of query likelihood, because we can cover"
cs-410_3_5_59,"00:04:47,085","00:04:49,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,But it would also allow us
cs-410_3_5_60,"00:04:50,770","00:04:56,104",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,So this is how we can use the
cs-410_3_5_61,"00:04:56,104","00:05:00,183",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,The picture shows that we first
cs-410_3_5_62,"00:05:00,183","00:05:04,836",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"then we estimate a query language model,"
cs-410_3_5_63,"00:05:04,836","00:05:07,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,This is often denoted by a D here.
cs-410_3_5_64,"00:05:09,560","00:05:14,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,But this basically means this is
cs-410_3_5_65,"00:05:14,690","00:05:19,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,because we compute a vector for the
cs-410_3_5_66,"00:05:19,010","00:05:22,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"the query, and"
cs-410_3_5_67,"00:05:22,450","00:05:26,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,Only that these vectors are of special
cs-410_3_5_68,"00:05:27,910","00:05:31,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,And then we get the results and
cs-410_3_5_69,"00:05:31,680","00:05:37,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,Let's assume they are mostly
cs-410_3_5_70,"00:05:37,420","00:05:40,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,although we could also consider
cs-410_3_5_71,"00:05:40,400","00:05:44,974",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"So what we could do is, like in Rocchio,"
cs-410_3_5_72,"00:05:44,974","00:05:48,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,model called the feedback
cs-410_3_5_73,"00:05:48,570","00:05:52,568",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Again, this is going to be another vector"
cs-410_3_5_74,"00:05:52,568","00:05:53,227",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,in Rocchio.
cs-410_3_5_75,"00:05:53,227","00:05:58,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,And then this model can be combined
cs-410_3_5_76,"00:05:58,060","00:06:02,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"a linear interpolation, and"
cs-410_3_5_77,"00:06:02,800","00:06:06,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"just like, again, in Rocchio."
cs-410_3_5_78,"00:06:06,260","00:06:10,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,So here we can see the parameter alpha
cs-410_3_5_79,"00:06:10,270","00:06:14,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"If it's set to zero,"
cs-410_3_5_80,"00:06:14,170","00:06:19,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"If it's set to one, we get full feedback"
cs-410_3_5_81,"00:06:19,050","00:06:21,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"And this is generally not desirable,"
cs-410_3_5_82,"00:06:21,820","00:06:26,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,So unless you are absolutely sure you
cs-410_3_5_83,"00:06:26,370","00:06:29,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,then the query terms are not important.
cs-410_3_5_84,"00:06:31,180","00:06:34,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"So of course, the main question here is,"
cs-410_3_5_85,"00:06:34,870","00:06:39,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"This is the big question here, and"
cs-410_3_5_86,"00:06:39,340","00:06:41,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,So here we will talk about
cs-410_3_5_87,"00:06:41,760","00:06:43,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,"there are many approaches, of course."
cs-410_3_5_88,"00:06:43,260","00:06:45,891",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,This approach is based
cs-410_3_5_89,"00:06:45,891","00:06:47,823",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,I'm going to show you how it works.
cs-410_3_5_90,"00:06:47,823","00:06:50,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,This will use a generative mixture model.
cs-410_3_5_91,"00:06:50,560","00:06:55,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,So this picture shows that
cs-410_3_5_92,"00:06:55,030","00:06:57,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,the feedback model that
cs-410_3_5_93,"00:06:58,080","00:07:00,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,And the basis is the feedback documents.
cs-410_3_5_94,"00:07:00,490","00:07:04,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,Let's say we are observing
cs-410_3_5_95,"00:07:04,110","00:07:09,012",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,These are the clicked documents by users
cs-410_3_5_96,"00:07:09,012","00:07:12,679",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,or are simply top ranked documents
cs-410_3_5_97,"00:07:14,710","00:07:17,834",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,Now imagine how we can
cs-410_3_5_98,"00:07:17,834","00:07:20,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,these documents by using language model.
cs-410_3_5_99,"00:07:20,630","00:07:23,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,One approach is simply to assume
cs-410_3_5_100,"00:07:23,330","00:07:26,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,these documents are generated
cs-410_3_5_101,"00:07:26,820","00:07:31,287",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"As we did before, what we could do"
cs-410_3_5_102,"00:07:31,287","00:07:34,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,here to here and
cs-410_3_5_103,"00:07:36,210","00:07:41,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,Now the question is whether this
cs-410_3_5_104,"00:07:41,260","00:07:45,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"Well, you can imagine the top"
cs-410_3_5_105,"00:07:45,430","00:07:46,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,What do you think?
cs-410_3_5_106,"00:07:48,280","00:07:51,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"Well, those words would be common words."
cs-410_3_5_107,"00:07:51,560","00:07:53,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"As we always see in a language model,"
cs-410_3_5_108,"00:07:53,770","00:07:57,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,the top ranked words are actually
cs-410_3_5_109,"00:07:57,850","00:08:02,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"So it's not very good for feedback,"
cs-410_3_5_110,"00:08:02,570","00:08:07,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,words to our query when we interpolate
cs-410_3_5_111,"00:08:08,880","00:08:13,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"So this was not good, so"
cs-410_3_5_112,"00:08:13,100","00:08:17,059",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"In particular, we are trying to"
cs-410_3_5_113,"00:08:17,059","00:08:21,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,And we have seen actually one way
cs-410_3_5_114,"00:08:21,855","00:08:27,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,language model in the case of
cs-410_3_5_115,"00:08:27,020","00:08:30,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,the words that are related
cs-410_3_5_116,"00:08:30,830","00:08:34,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,We could do that and that would be
cs-410_3_5_117,"00:08:34,590","00:08:39,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,are going to talk about another approach
cs-410_3_5_118,"00:08:39,160","00:08:43,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"In this case, we're going to say well,"
cs-410_3_5_119,"00:08:43,990","00:08:48,818",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,in these documents that should not
cs-410_3_5_120,"00:08:50,310","00:08:53,527",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"So now what we can do is to assume that,"
cs-410_3_5_121,"00:08:53,527","00:08:58,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,those words are generated from
cs-410_3_5_122,"00:08:58,019","00:09:02,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"they will generate those words like the,"
cs-410_3_5_123,"00:09:02,020","00:09:05,302",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"And if we use maximum likelihood estimate,"
cs-410_3_5_124,"00:09:05,302","00:09:10,182",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,note that if all the words here
cs-410_3_5_125,"00:09:10,182","00:09:15,681",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,then this model is forced to assign
cs-410_3_5_126,"00:09:15,681","00:09:19,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,because it occurs so frequently here.
cs-410_3_5_127,"00:09:19,620","00:09:25,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,Note that in order to reduce its
cs-410_3_5_128,"00:09:25,100","00:09:31,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"another model, which is this one,"
cs-410_3_5_129,"00:09:31,280","00:09:32,218",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"And in this case,"
cs-410_3_5_130,"00:09:32,218","00:09:37,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,it's not appropriate to use the background
cs-410_3_5_131,"00:09:37,200","00:09:42,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,goal because this model would assign high
cs-410_3_5_132,"00:09:43,370","00:09:46,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"So in this approach, then,"
cs-410_3_5_133,"00:09:46,000","00:09:50,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,we assume this machine that was generating
cs-410_3_5_134,"00:09:50,810","00:09:53,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,We have a source control up here.
cs-410_3_5_135,"00:09:53,630","00:09:59,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,Imagine we flip a coin here to
cs-410_3_5_136,"00:09:59,110","00:10:03,238",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"With probability of lambda,"
cs-410_3_5_137,"00:10:03,238","00:10:05,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,we're going to use
cs-410_3_5_138,"00:10:05,400","00:10:08,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,And we're going to do that in
cs-410_3_5_139,"00:10:08,540","00:10:12,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"With probability of 1 minus lambda,"
cs-410_3_5_140,"00:10:12,570","00:10:17,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"to use a known topic model, here,"
cs-410_3_5_141,"00:10:17,460","00:10:20,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,And we're going to then
cs-410_3_5_142,"00:10:20,100","00:10:25,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,If we make this assumption and this whole
cs-410_3_5_143,"00:10:25,450","00:10:30,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,this a mixture model because there are two
cs-410_3_5_144,"00:10:30,420","00:10:33,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,And we actually don't know when
cs-410_3_5_145,"00:10:35,770","00:10:40,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"So again,"
cs-410_3_5_146,"00:10:42,270","00:10:47,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,and we can still ask for words and it will
cs-410_3_5_147,"00:10:47,920","00:10:51,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,"And of course, which word will show up"
cs-410_3_5_148,"00:10:51,920","00:10:53,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,that distribution.
cs-410_3_5_149,"00:10:53,003","00:10:55,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,"In addition,"
cs-410_3_5_150,"00:10:55,780","00:10:58,751",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,because if you say lambda is very high and
cs-410_3_5_151,"00:10:58,751","00:11:02,769",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"always use the background distribution,"
cs-410_3_5_152,"00:11:02,769","00:11:07,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"Then if you say, well, lambda is"
cs-410_3_5_153,"00:11:07,260","00:11:12,353",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,So all of these
cs-410_3_5_154,"00:11:12,353","00:11:15,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"And then if you're thinking this way,"
cs-410_3_5_155,"00:11:15,108","00:11:19,206",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,basically we can do exactly
cs-410_3_5_156,"00:11:19,206","00:11:23,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,We're going to use maximum likelihood
cs-410_3_5_157,"00:11:23,445","00:11:25,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,to estimate the parameters.
cs-410_3_5_158,"00:11:25,760","00:11:30,201",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,Basically we're going to
cs-410_3_5_159,"00:11:30,201","00:11:33,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,that we can best explain all the data.
cs-410_3_5_160,"00:11:33,512","00:11:41,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,The difference now is that we are not
cs-410_3_5_161,"00:11:41,200","00:11:46,633",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,But rather we are going to ask this whole
cs-410_3_5_162,"00:11:46,633","00:11:50,049",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,Because it has got some help
cs-410_3_5_163,"00:11:50,049","00:11:54,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,it doesn't have to assign high
cs-410_3_5_164,"00:11:54,080","00:11:58,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,"As a result, it will then assign higher"
cs-410_3_5_165,"00:11:58,890","00:12:04,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,are common here but
cs-410_3_5_166,"00:12:04,950","00:12:06,877",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,So those would be common here.
cs-410_3_5_167,"00:12:11,321","00:12:14,907",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"And if they're common, they would"
cs-410_3_5_168,"00:12:14,907","00:12:17,661",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,according to a maximum
cs-410_3_5_169,"00:12:17,661","00:12:23,692",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"And if they are rare here,"
cs-410_3_5_170,"00:12:23,692","00:12:29,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,much help from this background model.
cs-410_3_5_171,"00:12:29,620","00:12:33,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"As a result, this topic model"
cs-410_3_5_172,"00:12:33,940","00:12:37,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"So the high probability words,"
cs-410_3_5_173,"00:12:37,410","00:12:41,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,would be those that are common here but
cs-410_3_5_174,"00:12:43,960","00:12:48,897",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,So this is basically a little bit
cs-410_3_5_175,"00:12:48,897","00:12:53,664",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,But this would allow us to achieve the
cs-410_3_5_176,"00:12:53,664","00:12:55,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,are meaningless in the feedback.
cs-410_3_5_177,"00:12:56,780","00:13:01,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,"So mathematically, what we have is"
cs-410_3_5_178,"00:13:01,200","00:13:04,794",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,"local likelihood,"
cs-410_3_5_179,"00:13:06,200","00:13:08,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,And note that we also have another
cs-410_3_5_180,"00:13:08,860","00:13:13,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,we assume that the lambda denotes
cs-410_3_5_181,"00:13:13,150","00:13:16,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,"So we are going to,"
cs-410_3_5_182,"00:13:16,010","00:13:21,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=796,Let's say 50% of the words are noise or
cs-410_3_5_183,"00:13:21,800","00:13:24,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=801,And this can then be
cs-410_3_5_184,"00:13:24,295","00:13:30,896",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,"If we assume this is fixed, then we only"
cs-410_3_5_185,"00:13:30,896","00:13:35,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,just like in the simple
cs-410_3_5_186,"00:13:35,090","00:13:39,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,"We have n parameters,"
cs-410_3_5_187,"00:13:39,090","00:13:41,289",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,And then the likelihood
cs-410_3_5_188,"00:13:42,760","00:13:47,643",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,It's very similar to the global
cs-410_3_5_189,"00:13:47,643","00:13:51,537",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=827,except that inside the logarithm
cs-410_3_5_190,"00:13:51,537","00:13:57,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,And this sum is because we
cs-410_3_5_191,"00:13:57,070","00:14:01,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,And which one is used would depend on
cs-410_3_5_192,"00:14:02,460","00:14:08,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,"But mathematically, this is the function"
cs-410_3_5_193,"00:14:08,790","00:14:10,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,So this is just a function.
cs-410_3_5_194,"00:14:10,510","00:14:13,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,All the other values are known except for
cs-410_3_5_195,"00:14:15,010","00:14:19,834",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,So we can then choose this
cs-410_3_5_196,"00:14:19,834","00:14:21,531",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"this log likelihood,"
cs-410_3_5_197,"00:14:21,531","00:14:27,357",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,the same idea as the maximum likelihood
cs-410_3_5_198,"00:14:27,357","00:14:30,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,We just have to solve this
cs-410_3_5_199,"00:14:30,060","00:14:34,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,We essentially would try all
cs-410_3_5_200,"00:14:34,460","00:14:37,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,that gives this whole thing
cs-410_3_5_201,"00:14:37,670","00:14:39,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,So it's a well-defined math problem.
cs-410_3_5_202,"00:14:40,900","00:14:45,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,"Once we have done that, we obtain this"
cs-410_3_5_203,"00:14:45,720","00:14:47,812",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,original query model to the feedback.
cs-410_3_5_204,"00:14:50,980","00:14:55,963",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,So here are some examples of
cs-410_3_5_205,"00:14:55,963","00:14:57,817",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=895,document collection.
cs-410_3_5_206,"00:14:57,817","00:15:01,673",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,And we do pseudo-feedback we just
cs-410_3_5_207,"00:15:01,673","00:15:03,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=901,we use this mixture model.
cs-410_3_5_208,"00:15:03,750","00:15:06,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,So the query is airport security.
cs-410_3_5_209,"00:15:06,090","00:15:11,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=906,What we do is we first retrieve ten
cs-410_3_5_210,"00:15:11,480","00:15:14,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,this is of course pseudo-feedback.
cs-410_3_5_211,"00:15:14,520","00:15:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,And then we're going to feed that
cs-410_3_5_212,"00:15:21,130","00:15:25,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,And these are the words
cs-410_3_5_213,"00:15:25,770","00:15:30,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,This is the probability of a word given
cs-410_3_5_214,"00:15:31,600","00:15:34,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,So in both cases you can see the highest
cs-410_3_5_215,"00:15:34,350","00:15:38,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,probability words include the very
cs-410_3_5_216,"00:15:38,480","00:15:40,208",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=938,"So airport security, for example,"
cs-410_3_5_217,"00:15:40,208","00:15:45,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,these query words still show up as high
cs-410_3_5_218,"00:15:45,450","00:15:48,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,because they occur frequently
cs-410_3_5_219,"00:15:48,850","00:15:53,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"But we also see beverage,"
cs-410_3_5_220,"00:15:53,830","00:15:59,436",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,"So these are relevant to this topic,"
cs-410_3_5_221,"00:15:59,436","00:16:05,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"if combined with original query, can help"
cs-410_3_5_222,"00:16:05,280","00:16:11,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,And also they can help us bring up
cs-410_3_5_223,"00:16:11,200","00:16:16,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"these other words, maybe, for example,"
cs-410_3_5_224,"00:16:18,070","00:16:20,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,So this is how pseudo-feedback works.
cs-410_3_5_225,"00:16:20,680","00:16:26,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,It shows that this model really works and
cs-410_3_5_226,"00:16:26,790","00:16:31,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,What's also interesting is that if
cs-410_3_5_227,"00:16:31,546","00:16:35,154",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,"you compare them,"
cs-410_3_5_228,"00:16:35,154","00:16:40,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,"when lambda is set to a small value,"
cs-410_3_5_229,"00:16:40,415","00:16:45,473",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"And that means, well,"
cs-410_3_5_230,"00:16:45,473","00:16:48,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,"Remember, lambda confuses the probability"
cs-410_3_5_231,"00:16:48,575","00:16:50,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1008,to generate the text.
cs-410_3_5_232,"00:16:50,925","00:16:53,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"If we don't rely much on background model,"
cs-410_3_5_233,"00:16:53,245","00:16:58,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,we still have to use this topic model
cs-410_3_5_234,"00:16:58,100","00:17:01,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,Whereas if we set lambda
cs-410_3_5_235,"00:17:01,340","00:17:05,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,we will use the background model
cs-410_3_5_236,"00:17:05,550","00:17:08,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,Then there's no burden on
cs-410_3_5_237,"00:17:08,930","00:17:11,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,in the feedback documents
cs-410_3_5_238,"00:17:11,790","00:17:17,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,"So as a result, the topic model"
cs-410_3_5_239,"00:17:17,430","00:17:20,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1037,It contains all the relevant
cs-410_3_5_240,"00:17:21,260","00:17:26,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,So this can be added to the original
cs-410_3_5_241,"00:17:28,140","00:17:29,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1048,"So to summarize,"
cs-410_3_5_242,"00:17:29,900","00:17:34,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,in this lecture we have talked about
cs-410_3_5_243,"00:17:34,470","00:17:38,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,"In general,"
cs-410_3_5_244,"00:17:38,290","00:17:43,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,"These examples can be assumed examples,"
cs-410_3_5_245,"00:17:43,610","00:17:48,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1063,like assume the top ten documents
cs-410_3_5_246,"00:17:48,770","00:17:51,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1068,"They could be based on user interactions,"
cs-410_3_5_247,"00:17:51,419","00:17:55,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,like feedback based on clickthroughs or
cs-410_3_5_248,"00:17:55,260","00:17:59,308",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1075,We talked about the three major
cs-410_3_5_249,"00:17:59,308","00:18:01,657",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,"pseudo feedback, and implicit feedback."
cs-410_3_5_250,"00:18:01,657","00:18:08,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1081,We talked about how to use Rocchio to
cs-410_3_5_251,"00:18:08,108","00:18:14,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,how to use query model estimation for
cs-410_3_5_252,"00:18:14,047","00:18:18,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1094,And we briefly talked about
cs-410_3_5_253,"00:18:19,790","00:18:21,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,There are many other methods.
cs-410_3_5_254,"00:18:21,650","00:18:22,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1101,"For example,"
cs-410_3_5_255,"00:18:22,170","00:18:26,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1102,the relevance model is a very effective
cs-410_3_5_256,"00:18:26,990","00:18:31,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,So you can read more about these
cs-410_3_5_257,"00:18:32,170","00:18:36,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,are listed at the end of this lecture.
cs-410_3_5_258,"00:18:36,200","00:18:38,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,So there are two additional readings here.
cs-410_3_5_259,"00:18:38,420","00:18:42,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1118,The first one is a book that
cs-410_3_5_260,"00:18:42,047","00:18:46,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1122,discussion of language models for
cs-410_3_5_261,"00:18:46,170","00:18:49,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1126,And the second one is a important research
cs-410_3_5_262,"00:18:49,745","00:18:54,549",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,paper that's about relevance
cs-410_3_5_263,"00:18:54,549","00:18:59,471",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1134,and it's a very effective way
cs-410_3_5_264,"00:18:59,471","00:19:09,471",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,[MUSIC]
cs-410_4_10_1,"00:00:00,012","00:00:08,521",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_10_2,"00:00:08,521","00:00:13,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,about how to use generative probabilistic
cs-410_4_10_3,"00:00:14,450","00:00:19,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,There are in general about two kinds
cs-410_4_10_4,"00:00:19,470","00:00:20,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,by using machine learning.
cs-410_4_10_5,"00:00:20,580","00:00:22,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,One is by generating probabilistic models.
cs-410_4_10_6,"00:00:22,970","00:00:25,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,The other is discriminative approaches.
cs-410_4_10_7,"00:00:25,750","00:00:29,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"In this lecture, we're going to"
cs-410_4_10_8,"00:00:29,740","00:00:34,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"In the next lecture, we're going to"
cs-410_4_10_9,"00:00:34,460","00:00:36,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,So the problem of text categorization
cs-410_4_10_10,"00:00:36,990","00:00:39,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,is actually a very similar
cs-410_4_10_11,"00:00:39,400","00:00:44,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"In that, we'll assume that each document"
cs-410_4_10_12,"00:00:44,820","00:00:48,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,The main difference is that in
cs-410_4_10_13,"00:00:48,500","00:00:51,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"what are the predefined categories are,"
cs-410_4_10_14,"00:00:51,810","00:00:54,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"In fact,"
cs-410_4_10_15,"00:00:55,780","00:00:58,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,We want to find such clusters in the data.
cs-410_4_10_16,"00:00:59,280","00:01:02,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,"But in the case of categorization,"
cs-410_4_10_17,"00:01:02,310","00:01:07,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,So we kind of have
cs-410_4_10_18,"00:01:07,470","00:01:11,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,then based on these categories and
cs-410_4_10_19,"00:01:11,810","00:01:18,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,a document to one of these categories or
cs-410_4_10_20,"00:01:18,710","00:01:21,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,But because of the similarity
cs-410_4_10_21,"00:01:21,160","00:01:26,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,we can actually get the document
cs-410_4_10_22,"00:01:26,930","00:01:30,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,And we understand how we can
cs-410_4_10_23,"00:01:30,630","00:01:35,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,text categorization from
cs-410_4_10_24,"00:01:35,930","00:01:41,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,"And so, this is a slide that we've talked"
cs-410_4_10_25,"00:01:41,620","00:01:47,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,where we assume there are multiple topics
cs-410_4_10_26,"00:01:47,550","00:01:49,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,Each topic is one cluster.
cs-410_4_10_27,"00:01:49,620","00:01:52,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"So once we estimated such a model,"
cs-410_4_10_28,"00:01:52,080","00:01:58,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,we faced a problem of deciding which
cs-410_4_10_29,"00:01:58,380","00:02:04,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,And this question boils down to decide
cs-410_4_10_30,"00:02:06,440","00:02:14,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"Now, suppose d has L words"
cs-410_4_10_31,"00:02:14,470","00:02:21,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"Now, how can you compute"
cs-410_4_10_32,"00:02:21,220","00:02:25,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,topic word distribution zeta i has
cs-410_4_10_33,"00:02:27,050","00:02:32,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"Well, in general, we use base"
cs-410_4_10_34,"00:02:32,980","00:02:40,167",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,you can see this prior information here
cs-410_4_10_35,"00:02:40,167","00:02:44,846",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,cluster has a higher prior
cs-410_4_10_36,"00:02:44,846","00:02:49,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,that the document has
cs-410_4_10_37,"00:02:49,920","00:02:52,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"And so, we should favor such a cluster."
cs-410_4_10_38,"00:02:52,470","00:02:54,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"The other is a likelihood part,"
cs-410_4_10_39,"00:02:56,080","00:02:58,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,And this has to do with whether
cs-410_4_10_40,"00:02:58,880","00:03:02,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,can explain the content
cs-410_4_10_41,"00:03:02,370","00:03:07,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,And we want to pick a topic
cs-410_4_10_42,"00:03:07,730","00:03:10,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"So more specifically,"
cs-410_4_10_43,"00:03:10,810","00:03:14,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,then choose which topic
cs-410_4_10_44,"00:03:14,650","00:03:18,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,"So more rigorously,"
cs-410_4_10_45,"00:03:18,740","00:03:22,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,So we're going to choose
cs-410_4_10_46,"00:03:22,630","00:03:27,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,This posterior probability at the top
cs-410_4_10_47,"00:03:27,660","00:03:33,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"because this one,"
cs-410_4_10_48,"00:03:33,020","00:03:36,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,That's our belief about
cs-410_4_10_49,"00:03:36,810","00:03:39,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,before we observe any document.
cs-410_4_10_50,"00:03:39,470","00:03:43,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,But this conditional probability here is
cs-410_4_10_51,"00:03:43,320","00:03:47,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,the posterior probability of the topic
cs-410_4_10_52,"00:03:49,758","00:03:54,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,And base wall allows us to update this
cs-410_4_10_53,"00:03:54,130","00:03:59,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"I have shown the details,"
cs-410_4_10_54,"00:03:59,040","00:04:04,792",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,the prior here is related to
cs-410_4_10_55,"00:04:05,960","00:04:10,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,And this is related to how
cs-410_4_10_56,"00:04:10,870","00:04:16,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"explains the document here, and"
cs-410_4_10_57,"00:04:16,200","00:04:21,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,So to find the topic that has the higher
cs-410_4_10_58,"00:04:21,470","00:04:26,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,posterior probability here it's
cs-410_4_10_59,"00:04:26,930","00:04:30,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"as we have seen also,"
cs-410_4_10_60,"00:04:32,300","00:04:37,159",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And we can then change the probability
cs-410_4_10_61,"00:04:37,159","00:04:42,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"the probability of each word, and"
cs-410_4_10_62,"00:04:42,019","00:04:46,382",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,an assumption about independence
cs-410_4_10_63,"00:04:46,382","00:04:49,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,So this is just something that you
cs-410_4_10_64,"00:04:50,680","00:04:56,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,And we now can see clearly how we
cs-410_4_10_65,"00:04:56,760","00:05:02,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,based on the information
cs-410_4_10_66,"00:05:02,270","00:05:05,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,these categories and
cs-410_4_10_67,"00:05:05,740","00:05:10,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,So this idea can be directly
cs-410_4_10_68,"00:05:10,690","00:05:16,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,And this is precisely what
cs-410_4_10_69,"00:05:16,360","00:05:19,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,So here it's most really
cs-410_4_10_70,"00:05:19,290","00:05:21,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,that we're looking at
cs-410_4_10_71,"00:05:21,910","00:05:26,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,So we assume that if theta i
cs-410_4_10_72,"00:05:26,620","00:05:31,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,"represents category i accurately,"
cs-410_4_10_73,"00:05:31,770","00:05:37,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,characterizes the content of
cs-410_4_10_74,"00:05:37,120","00:05:40,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Then, what we can do is precisely"
cs-410_4_10_75,"00:05:40,802","00:05:45,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,Namely we're going to assign
cs-410_4_10_76,"00:05:45,580","00:05:50,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,that has the highest probability
cs-410_4_10_77,"00:05:50,310","00:05:54,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"In other words, we're going to maximize"
cs-410_4_10_78,"00:05:56,620","00:05:59,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,And this is related to the prior and
cs-410_4_10_79,"00:05:59,910","00:06:04,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,the [INAUDIBLE] as you have
cs-410_4_10_80,"00:06:04,290","00:06:07,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"And so, naturally we can decompose"
cs-410_4_10_81,"00:06:07,400","00:06:11,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,this [INAUDIBLE] into
cs-410_4_10_82,"00:06:11,840","00:06:16,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"Now, here, I change the notation so"
cs-410_4_10_83,"00:06:16,770","00:06:20,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,product of all the words
cs-410_4_10_84,"00:06:20,970","00:06:25,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,even though the document
cs-410_4_10_85,"00:06:25,730","00:06:31,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,And the product is still accurately
cs-410_4_10_86,"00:06:31,810","00:06:35,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,of all the words in the document
cs-410_4_10_87,"00:06:37,150","00:06:39,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"When a word,"
cs-410_4_10_88,"00:06:39,120","00:06:42,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"The count would be 0, so"
cs-410_4_10_89,"00:06:42,840","00:06:48,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So if actively we'll just have the product
cs-410_4_10_90,"00:06:48,380","00:06:50,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"So basically, with Naive Bayes Classifier,"
cs-410_4_10_91,"00:06:50,940","00:06:55,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,we're going to score each category for
cs-410_4_10_92,"00:06:56,850","00:07:02,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"Now, you may notice that here it involves"
cs-410_4_10_93,"00:07:02,390","00:07:06,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,And this can cause and the four problem.
cs-410_4_10_94,"00:07:06,480","00:07:10,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,So one way to solve the problem is
cs-410_4_10_95,"00:07:10,269","00:07:13,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,which it doesn't changes all
cs-410_4_10_96,"00:07:13,530","00:07:15,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,But will helps us preserve precision.
cs-410_4_10_97,"00:07:15,732","00:07:20,519",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"And so, this is often the function"
cs-410_4_10_98,"00:07:20,519","00:07:24,611",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,score each category and
cs-410_4_10_99,"00:07:24,611","00:07:30,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,the category that has the highest
cs-410_4_10_100,"00:07:30,300","00:07:34,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,So this is called an Naive Bayes
cs-410_4_10_101,"00:07:34,870","00:07:39,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,understandable because we are applying
cs-410_4_10_102,"00:07:39,535","00:07:46,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,the posterior probability of the topic to
cs-410_4_10_103,"00:07:47,560","00:07:52,337",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Now, it's also called a naive because"
cs-410_4_10_104,"00:07:52,337","00:07:56,553",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,in the document is generated
cs-410_4_10_105,"00:07:56,553","00:08:00,932",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,assumption because in reality they're
cs-410_4_10_106,"00:08:00,932","00:08:05,241",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,"Once you see some word,"
cs-410_4_10_107,"00:08:05,241","00:08:08,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"For example,"
cs-410_4_10_108,"00:08:08,235","00:08:09,598",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Than that mixed category,"
cs-410_4_10_109,"00:08:09,598","00:08:13,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,they see more clustering more likely to
cs-410_4_10_110,"00:08:15,490","00:08:18,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,But this assumption allows
cs-410_4_10_111,"00:08:18,740","00:08:22,854",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,And it's actually quite effective for
cs-410_4_10_112,"00:08:22,854","00:08:26,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,But you should know that
cs-410_4_10_113,"00:08:26,370","00:08:29,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,have to make this assumption.
cs-410_4_10_114,"00:08:29,000","00:08:33,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"We could for example, assume that"
cs-410_4_10_115,"00:08:33,760","00:08:38,411",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,So that would make it a bigram analogy
cs-410_4_10_116,"00:08:38,411","00:08:43,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,And of course you can even use a mixture
cs-410_4_10_117,"00:08:43,019","00:08:45,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,like in each category.
cs-410_4_10_118,"00:08:45,120","00:08:49,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So in nature, they will be all using"
cs-410_4_10_119,"00:08:49,530","00:08:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,But the actual generating model for
cs-410_4_10_120,"00:08:54,760","00:08:59,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"And here, we just talk about very"
cs-410_4_10_121,"00:09:00,980","00:09:05,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"So now the question is,"
cs-410_4_10_122,"00:09:05,220","00:09:09,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,actually represents category i accurately?
cs-410_4_10_123,"00:09:09,520","00:09:13,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"Now in clustering,"
cs-410_4_10_124,"00:09:13,700","00:09:17,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,what are the distributions for
cs-410_4_10_125,"00:09:17,310","00:09:20,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,"But in our case,"
cs-410_4_10_126,"00:09:20,820","00:09:24,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,this theta i represents indeed category i.
cs-410_4_10_127,"00:09:25,960","00:09:27,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"Well if you think about the question, and"
cs-410_4_10_128,"00:09:27,510","00:09:33,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,you likely come up with the idea
cs-410_4_10_129,"00:09:34,800","00:09:36,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"Indeed in the textbook,"
cs-410_4_10_130,"00:09:36,050","00:09:40,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,we typically assume that there
cs-410_4_10_131,"00:09:40,140","00:09:47,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,those are the documents that unknown
cs-410_4_10_132,"00:09:47,810","00:09:51,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"In other words, these are the documents"
cs-410_4_10_133,"00:09:51,680","00:09:54,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,of course human experts must do that.
cs-410_4_10_134,"00:09:54,450","00:09:58,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,"In here, you see that T1"
cs-410_4_10_135,"00:09:58,960","00:10:03,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,that are known to have
cs-410_4_10_136,"00:10:03,020","00:10:07,187",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,And T2 represents the documents
cs-410_4_10_137,"00:10:07,187","00:10:09,699",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"generated from category 2, etc."
cs-410_4_10_138,"00:10:09,699","00:10:14,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"Now if you look at this picture,"
cs-410_4_10_139,"00:10:14,475","00:10:18,872",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,here is really a simplified
cs-410_4_10_140,"00:10:18,872","00:10:20,452",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"It's no longer mixed modal, why?"
cs-410_4_10_141,"00:10:20,452","00:10:25,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,Because we already know which distribution
cs-410_4_10_142,"00:10:25,350","00:10:29,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"There's no uncertainty here, there's"
cs-410_4_10_143,"00:10:30,980","00:10:35,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,So the estimation problem of
cs-410_4_10_144,"00:10:35,110","00:10:38,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"But in general,"
cs-410_4_10_145,"00:10:38,720","00:10:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,estimate these probabilities
cs-410_4_10_146,"00:10:42,380","00:10:46,799",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,And what other probability is that we have
cs-410_4_10_147,"00:10:46,799","00:10:48,553",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,Well there are two kinds.
cs-410_4_10_148,"00:10:48,553","00:10:53,114",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"So one is the prior,"
cs-410_4_10_149,"00:10:53,114","00:10:57,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,this indicates how popular
cs-410_4_10_150,"00:10:57,349","00:11:03,224",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,how likely will it have observed
cs-410_4_10_151,"00:11:03,224","00:11:05,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,The other kind is
cs-410_4_10_152,"00:11:05,990","00:11:10,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,we want to know what words have high
cs-410_4_10_153,"00:11:11,690","00:11:15,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,So the idea then is to just
cs-410_4_10_154,"00:11:15,570","00:11:17,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,to estimate these two probabilities.
cs-410_4_10_155,"00:11:18,830","00:11:23,261",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,"And in general, we can do this"
cs-410_4_10_156,"00:11:23,261","00:11:27,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,That's just because these documents
cs-410_4_10_157,"00:11:27,650","00:11:29,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,from a specific category.
cs-410_4_10_158,"00:11:29,565","00:11:30,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,"So once we know that,"
cs-410_4_10_159,"00:11:30,825","00:11:35,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,it's in some sense irrelevant of what
cs-410_4_10_160,"00:11:37,470","00:11:41,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,So now this is a statistical
cs-410_4_10_161,"00:11:41,737","00:11:44,209",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,We have observed some
cs-410_4_10_162,"00:11:44,209","00:11:47,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,we want to guess
cs-410_4_10_163,"00:11:47,220","00:11:49,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,We want to take our best
cs-410_4_10_164,"00:11:51,060","00:11:56,073",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,And this is a problem that we have seen
cs-410_4_10_165,"00:11:56,073","00:11:58,728",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,"Now, if you haven't thought"
cs-410_4_10_166,"00:11:58,728","00:12:00,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,haven't seen life based classifier.
cs-410_4_10_167,"00:12:00,775","00:12:04,649",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,It would be very useful for
cs-410_4_10_168,"00:12:04,649","00:12:07,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,to think about how to solve this problem.
cs-410_4_10_169,"00:12:07,690","00:12:10,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,So let me state the problem again.
cs-410_4_10_170,"00:12:10,680","00:12:13,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,"So let's just think about with category 1,"
cs-410_4_10_171,"00:12:13,310","00:12:18,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,we know there is one word of distribution
cs-410_4_10_172,"00:12:18,750","00:12:23,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,And we generate each word in the document
cs-410_4_10_173,"00:12:23,110","00:12:29,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,observed a set of n sub 1
cs-410_4_10_174,"00:12:29,350","00:12:32,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,These documents have been all
cs-410_4_10_175,"00:12:32,980","00:12:37,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,Namely have been all generated
cs-410_4_10_176,"00:12:37,500","00:12:40,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,"Now the question is,"
cs-410_4_10_177,"00:12:40,420","00:12:44,369",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=760,estimate of the probability of
cs-410_4_10_178,"00:12:44,369","00:12:49,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,And what would be your guess of
cs-410_4_10_179,"00:12:49,710","00:12:52,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"Of course,"
cs-410_4_10_180,"00:12:52,200","00:12:54,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,how likely are you to see
cs-410_4_10_181,"00:12:55,990","00:13:00,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So think for a moment, how do you"
cs-410_4_10_182,"00:13:00,970","00:13:06,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,all these documents that are known
cs-410_4_10_183,"00:13:06,430","00:13:08,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,to estimate all these parameters?
cs-410_4_10_184,"00:13:08,950","00:13:11,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"Now, if you spend some time"
cs-410_4_10_185,"00:13:11,280","00:13:15,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,it would help you understand
cs-410_4_10_186,"00:13:15,770","00:13:20,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,So do spend some time to make sure that
cs-410_4_10_187,"00:13:20,390","00:13:23,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,do you best to solve the problem yourself.
cs-410_4_10_188,"00:13:23,310","00:13:28,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,Now if you have thought about and
cs-410_4_10_189,"00:13:29,400","00:13:35,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,"First, what's the bases for estimating the"
cs-410_4_10_190,"00:13:35,880","00:13:40,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,Well this has to do with whether you
cs-410_4_10_191,"00:13:40,080","00:13:41,625",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,form that category.
cs-410_4_10_192,"00:13:41,625","00:13:44,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,"Intuitively, you have seen a lot"
cs-410_4_10_193,"00:13:44,870","00:13:46,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,very few in medical science.
cs-410_4_10_194,"00:13:46,770","00:13:51,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,Then you guess is that the probability
cs-410_4_10_195,"00:13:51,870","00:13:55,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,your prior on the category
cs-410_4_10_196,"00:13:57,130","00:14:01,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,And what about the basis for estimating
cs-410_4_10_197,"00:14:01,570","00:14:05,929",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,"Well the same, and you'll be just"
cs-410_4_10_198,"00:14:05,929","00:14:10,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,frequently in the documents that are known
cs-410_4_10_199,"00:14:10,645","00:14:12,799",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,likely have a higher probability.
cs-410_4_10_200,"00:14:12,799","00:14:15,493",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,And that's just a maximum
cs-410_4_10_201,"00:14:15,493","00:14:20,707",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,"Indeed, that's what we can do, so this"
cs-410_4_10_202,"00:14:20,707","00:14:24,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,"to answer the question,"
cs-410_4_10_203,"00:14:24,990","00:14:31,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=864,"Then we can simply normalize,"
cs-410_4_10_204,"00:14:31,210","00:14:36,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,So here you see N sub i denotes
cs-410_4_10_205,"00:14:37,950","00:14:41,313",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,And we simply just normalize these
cs-410_4_10_206,"00:14:41,313","00:14:46,929",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,"In other words, we make this"
cs-410_4_10_207,"00:14:46,929","00:14:52,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,of training intercept in each category
cs-410_4_10_208,"00:14:55,260","00:14:58,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=895,Now what about the word distribution?
cs-410_4_10_209,"00:14:58,010","00:14:59,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,"Well, we do the same."
cs-410_4_10_210,"00:14:59,460","00:15:03,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,Again this time we can do this for
cs-410_4_10_211,"00:15:03,940","00:15:08,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,"So let's say,"
cs-410_4_10_212,"00:15:08,640","00:15:12,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,So which word has a higher probability?
cs-410_4_10_213,"00:15:12,480","00:15:15,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=912,"Well, we simply count the word occurrences"
cs-410_4_10_214,"00:15:15,850","00:15:19,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=915,in the documents that are known
cs-410_4_10_215,"00:15:20,290","00:15:25,516",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,And then we put together all
cs-410_4_10_216,"00:15:25,516","00:15:30,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,And then we just normalize these
cs-410_4_10_217,"00:15:30,565","00:15:35,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=930,of all the words make all
cs-410_4_10_218,"00:15:35,660","00:15:39,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,"So in this case, you're going to see this"
cs-410_4_10_219,"00:15:39,000","00:15:43,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,the word in the collection of
cs-410_4_10_220,"00:15:43,870","00:15:48,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,that's denoted by c of w and T sub i.
cs-410_4_10_221,"00:15:49,710","00:15:55,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=949,"Now, you may notice that we"
cs-410_4_10_222,"00:15:55,110","00:16:01,715",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=955,estimate in the form of being
cs-410_4_10_223,"00:16:01,715","00:16:03,529",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=961,"And this is often sufficient,"
cs-410_4_10_224,"00:16:03,529","00:16:07,033",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,because we have some constraints
cs-410_4_10_225,"00:16:07,033","00:16:11,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,So the normalizer is
cs-410_4_10_226,"00:16:11,281","00:16:15,191",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"So in this case, it will be useful for"
cs-410_4_10_227,"00:16:15,191","00:16:19,711",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=975,what are the constraints on these
cs-410_4_10_228,"00:16:19,711","00:16:22,753",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,So once you figure out
cs-410_4_10_229,"00:16:22,753","00:16:25,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,you will know how to
cs-410_4_10_230,"00:16:25,410","00:16:32,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,And so this is a good exercise to
cs-410_4_10_231,"00:16:32,940","00:16:35,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,There is another issue in
cs-410_4_10_232,"00:16:35,940","00:16:41,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,In fact the smoothing is a general problem
cs-410_4_10_233,"00:16:41,720","00:16:43,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,"And this has to do with,"
cs-410_4_10_234,"00:16:43,420","00:16:47,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1003,what would happen if you have
cs-410_4_10_235,"00:16:47,540","00:16:51,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,So smoothing is an important technique
cs-410_4_10_236,"00:16:51,590","00:16:56,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,"In our case, the training data can be"
cs-410_4_10_237,"00:16:56,620","00:17:01,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1016,we use maximum likely estimator we often
cs-410_4_10_238,"00:17:01,140","00:17:03,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,That means if an event is not observed
cs-410_4_10_239,"00:17:03,770","00:17:06,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,then the estimated
cs-410_4_10_240,"00:17:06,440","00:17:11,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1026,"In this case, if we have not seen"
cs-410_4_10_241,"00:17:11,070","00:17:13,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,"let's say, category i."
cs-410_4_10_242,"00:17:13,590","00:17:18,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,Then our estimator would be zero for the
cs-410_4_10_243,"00:17:18,770","00:17:20,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,and this is generally not accurate.
cs-410_4_10_244,"00:17:20,800","00:17:25,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,So we have to do smoothing to make
cs-410_4_10_245,"00:17:25,380","00:17:30,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,The other reason for smoothing is that
cs-410_4_10_246,"00:17:30,990","00:17:35,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,and this is also generally true for
cs-410_4_10_247,"00:17:35,330","00:17:37,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"When the data set is small,"
cs-410_4_10_248,"00:17:37,346","00:17:41,827",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1057,we tend to rely on some prior
cs-410_4_10_249,"00:17:41,827","00:17:46,242",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1061,So in this case our [INAUDIBLE] says that
cs-410_4_10_250,"00:17:46,242","00:17:51,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1066,So smoothing allows us to inject
cs-410_4_10_251,"00:17:51,035","00:17:53,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,order has a real zero probability.
cs-410_4_10_252,"00:17:54,970","00:17:59,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1074,There is also a third reason which
cs-410_4_10_253,"00:17:59,230","00:18:00,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,we explain that in a moment.
cs-410_4_10_254,"00:18:00,850","00:18:05,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1080,And that is to help achieve
cs-410_4_10_255,"00:18:05,480","00:18:08,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,"And this is also called IDF weighting,"
cs-410_4_10_256,"00:18:08,160","00:18:13,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,inverse document frequency weighting that
cs-410_4_10_257,"00:18:14,740","00:18:15,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1094,So how do we do smoothing?
cs-410_4_10_258,"00:18:15,850","00:18:19,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,Well in general we add pseudo
cs-410_4_10_259,"00:18:19,220","00:18:21,607",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,we'll make sure that no event has 0 count.
cs-410_4_10_260,"00:18:22,790","00:18:27,676",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1102,So one possible way of smoothing
cs-410_4_10_261,"00:18:27,676","00:18:32,483",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1107,is to simply add a small non active
cs-410_4_10_262,"00:18:32,483","00:18:37,413",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,Let's pretend that every category
cs-410_4_10_263,"00:18:37,413","00:18:39,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1117,documents represented by delta.
cs-410_4_10_264,"00:18:40,990","00:18:45,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1120,And in the denominator we also add
cs-410_4_10_265,"00:18:45,660","00:18:48,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1125,we want the probability to some to 1.
cs-410_4_10_266,"00:18:48,730","00:18:54,427",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,So in total we've added delta k times
cs-410_4_10_267,"00:18:54,427","00:18:59,242",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1134,"Therefore in this sum,"
cs-410_4_10_268,"00:18:59,242","00:19:04,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,delta as a total pseudocount
cs-410_4_10_269,"00:19:06,420","00:19:09,568",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1146,"Now, it's interesting to think"
cs-410_4_10_270,"00:19:09,568","00:19:11,678",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1149,obvious data is a smoothing
cs-410_4_10_271,"00:19:11,678","00:19:16,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,Meaning that the larger data is and
cs-410_4_10_272,"00:19:16,285","00:19:19,505",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,that means we'll more
cs-410_4_10_273,"00:19:19,505","00:19:24,238",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1159,And we might indeed ignore the actual
cs-410_4_10_274,"00:19:24,238","00:19:25,587",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1164,set to infinity.
cs-410_4_10_275,"00:19:25,587","00:19:30,172",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1165,Imagine what would happen if there
cs-410_4_10_276,"00:19:30,172","00:19:39,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1170,"Well, we are going to say every category"
cs-410_4_10_277,"00:19:39,270","00:19:43,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1179,And then there's no distinction to them so
cs-410_4_10_278,"00:19:44,850","00:19:46,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,What if delta is 0?
cs-410_4_10_279,"00:19:46,200","00:19:51,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1186,"Well, we just go back to the original"
cs-410_4_10_280,"00:19:51,050","00:19:54,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1191,data to estimate to estimate
cs-410_4_10_281,"00:19:54,880","00:19:57,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1194,Now we can do the same for
cs-410_4_10_282,"00:19:57,610","00:20:01,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1197,"But in this case,"
cs-410_4_10_283,"00:20:01,670","00:20:05,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1201,to use a nonuniform seudocount for
cs-410_4_10_284,"00:20:05,750","00:20:09,372",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,So here you'll see we'll add
cs-410_4_10_285,"00:20:09,372","00:20:12,143",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,that's mule multiplied
cs-410_4_10_286,"00:20:12,143","00:20:15,781",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1212,the word given by a background
cs-410_4_10_287,"00:20:15,781","00:20:19,772",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1215,Now that background model in
cs-410_4_10_288,"00:20:19,772","00:20:22,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1219,a logic collection of tests.
cs-410_4_10_289,"00:20:22,070","00:20:26,588",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1222,Or in this case we will use the whole
cs-410_4_10_290,"00:20:26,588","00:20:29,693",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1226,estimate this background language model.
cs-410_4_10_291,"00:20:29,693","00:20:31,494",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1229,"But we don't have to use this one,"
cs-410_4_10_292,"00:20:31,494","00:20:35,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1231,we can use larger test data that
cs-410_4_10_293,"00:20:36,170","00:20:40,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1236,Now if we use such a background
cs-410_4_10_294,"00:20:40,110","00:20:43,605",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1240,we'll find that some words will
cs-410_4_10_295,"00:20:43,605","00:20:44,848",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1243,So what are those words?
cs-410_4_10_296,"00:20:44,848","00:20:48,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1244,Well those are the common words
cs-410_4_10_297,"00:20:48,580","00:20:50,328",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1248,the background average model.
cs-410_4_10_298,"00:20:50,328","00:20:53,314",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1250,So the pseudocounts added for
cs-410_4_10_299,"00:20:53,314","00:20:59,126",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1253,Real words on the other hand
cs-410_4_10_300,"00:20:59,126","00:21:03,443",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1259,Now this addition of background
cs-410_4_10_301,"00:21:03,443","00:21:06,382",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1263,smoothing of these word distributions.
cs-410_4_10_302,"00:21:06,382","00:21:10,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1266,We're going to bring the probability of
cs-410_4_10_303,"00:21:10,630","00:21:12,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1270,because of the background model.
cs-410_4_10_304,"00:21:12,880","00:21:17,769",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1272,Now this helps make the difference
cs-410_4_10_305,"00:21:17,769","00:21:21,312",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1277,such words smaller across categories.
cs-410_4_10_306,"00:21:21,312","00:21:26,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1281,Because every category has some help
cs-410_4_10_307,"00:21:26,005","00:21:29,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1286,"I get the, a,"
cs-410_4_10_308,"00:21:29,050","00:21:33,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1289,"Therefore, it's not always so"
cs-410_4_10_309,"00:21:33,890","00:21:38,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1293,has documents that contain a lot
cs-410_4_10_310,"00:21:38,320","00:21:41,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1298,the estimate is more influenced
cs-410_4_10_311,"00:21:41,600","00:21:44,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1301,And the consequence is that
cs-410_4_10_312,"00:21:44,360","00:21:48,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1304,such words tend not to influence
cs-410_4_10_313,"00:21:48,420","00:21:53,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1308,as words that have small probabilities
cs-410_4_10_314,"00:21:53,590","00:21:57,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1313,Those words don't get some help
cs-410_4_10_315,"00:21:57,070","00:22:02,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1317,So the difference would be primary because
cs-410_4_10_316,"00:22:02,080","00:22:04,089",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,in the training documents
cs-410_4_10_317,"00:22:05,400","00:22:09,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1325,We also see another smoothing parameter
cs-410_4_10_318,"00:22:09,970","00:22:13,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1329,smoothing and just like a delta does for
cs-410_4_10_319,"00:22:14,920","00:22:19,353",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1334,And you can easily understand why we
cs-410_4_10_320,"00:22:19,353","00:22:23,587",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1339,represents the sum of all the pseudocounts
cs-410_4_10_321,"00:22:25,689","00:22:29,051",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1345,So view is also a non
cs-410_4_10_322,"00:22:29,051","00:22:32,611",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1349,it's [INAUDIBLE] set to control smoothing.
cs-410_4_10_323,"00:22:32,611","00:22:35,409",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1352,Now there are some interesting
cs-410_4_10_324,"00:22:35,409","00:22:39,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1355,"First, let's think about when mu"
cs-410_4_10_325,"00:22:39,280","00:22:41,319",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1359,Well in this case
cs-410_4_10_326,"00:22:43,170","00:22:47,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1363,to the background language model we'll
cs-410_4_10_327,"00:22:47,640","00:22:52,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1367,So we will bring every word distribution
cs-410_4_10_328,"00:22:52,365","00:22:56,352",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1372,that essentially remove the difference
cs-410_4_10_329,"00:22:56,352","00:22:57,943",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1376,"Obviously, we don't want to do that."
cs-410_4_10_330,"00:22:57,943","00:23:02,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1377,The other special case is the thing
cs-410_4_10_331,"00:23:02,683","00:23:06,919",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1382,"suppose, we actually set"
cs-410_4_10_332,"00:23:06,919","00:23:10,218",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1386,"And let's say,"
cs-410_4_10_333,"00:23:10,218","00:23:16,121",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1390,"So each one has the same probability,"
cs-410_4_10_334,"00:23:16,121","00:23:22,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1396,going to be very similar to the one
cs-410_4_10_335,"00:23:22,030","00:23:25,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1402,It's because we're going to add
cs-410_4_10_336,"00:23:29,268","00:23:30,441",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1409,"So in general,"
cs-410_4_10_337,"00:23:30,441","00:23:35,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1410,in Naive Bayes categorization we
cs-410_4_10_338,"00:23:35,240","00:23:38,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1415,"And then once we have these probabilities,"
cs-410_4_10_339,"00:23:38,970","00:23:42,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1418,then we can compute the score for
cs-410_4_10_340,"00:23:42,960","00:23:44,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1422,For a document and
cs-410_4_10_341,"00:23:44,030","00:23:47,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1424,then choose the category where it was
cs-410_4_10_342,"00:23:49,250","00:23:53,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1429,"Now, it's useful to"
cs-410_4_10_343,"00:23:53,266","00:23:57,863",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1433,the Naive Bayes scoring
cs-410_4_10_344,"00:23:57,863","00:24:03,755",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1437,"So to understand that, and also to"
cs-410_4_10_345,"00:24:03,755","00:24:10,029",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1443,will actually achieve the effect of IDF
cs-410_4_10_346,"00:24:10,029","00:24:13,669",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1450,So suppose we have just two categories and
cs-410_4_10_347,"00:24:13,669","00:24:19,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1453,we're going to score based on
cs-410_4_10_348,"00:24:19,395","00:24:21,647",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1459,So this is the.
cs-410_4_10_349,"00:24:24,563","00:24:29,723",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1464,Lets say this is our scoring function for
cs-410_4_10_350,"00:24:29,723","00:24:33,072",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1469,"two categories, right?"
cs-410_4_10_351,"00:24:33,072","00:24:40,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1473,"So, this is a score of a document for"
cs-410_4_10_352,"00:24:40,100","00:24:44,409",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1480,And we're going to score based
cs-410_4_10_353,"00:24:44,409","00:24:47,196",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1484,"So if the ratio is larger,"
cs-410_4_10_354,"00:24:47,196","00:24:52,907",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1487,then it means it's more
cs-410_4_10_355,"00:24:52,907","00:24:57,779",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1492,So the larger the score is the more likely
cs-410_4_10_356,"00:24:57,779","00:25:01,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1497,the document is in category one.
cs-410_4_10_357,"00:25:01,800","00:25:03,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1501,"So by using Bayes' rule,"
cs-410_4_10_358,"00:25:03,810","00:25:08,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1503,"we can write down this ratio as follows,"
cs-410_4_10_359,"00:25:09,190","00:25:15,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1509,"Now, we generally take logarithm of this"
cs-410_4_10_360,"00:25:15,920","00:25:21,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1515,And this would then give us this
cs-410_4_10_361,"00:25:21,450","00:25:23,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1521,And here we see something
cs-410_4_10_362,"00:25:23,520","00:25:28,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1523,because this is our scoring function for
cs-410_4_10_363,"00:25:30,280","00:25:34,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1530,"And if you look at this function,"
cs-410_4_10_364,"00:25:34,718","00:25:38,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1534,The first part here is actually
cs-410_4_10_365,"00:25:38,860","00:25:40,409",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1538,And so this is a category bias.
cs-410_4_10_366,"00:25:41,870","00:25:43,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1541,It doesn't really depend on the document.
cs-410_4_10_367,"00:25:43,740","00:25:48,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1543,It just says which category is more
cs-410_4_10_368,"00:25:48,780","00:25:53,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1548,"this category slightly, right?"
cs-410_4_10_369,"00:25:53,240","00:25:58,114",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1553,"So, the second part has a sum"
cs-410_4_10_370,"00:25:58,114","00:26:03,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1558,"So, these are the words that"
cs-410_4_10_371,"00:26:03,590","00:26:06,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1563,in general we can consider all
cs-410_4_10_372,"00:26:06,510","00:26:08,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1566,So here we're going to
cs-410_4_10_373,"00:26:08,960","00:26:12,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1568,"about which category is more likely,"
cs-410_4_10_374,"00:26:12,280","00:26:16,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1572,So inside of the sum you can see
cs-410_4_10_375,"00:26:16,920","00:26:19,056",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1576,"The first, is a count of the word."
cs-410_4_10_376,"00:26:19,056","00:26:25,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1579,And this count of the word serves as
cs-410_4_10_377,"00:26:27,080","00:26:30,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1587,And this is what we can
cs-410_4_10_378,"00:26:30,390","00:26:33,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1590,The second part is
cs-410_4_10_379,"00:26:33,645","00:26:37,236",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1593,"here it's the weight on which word, right?"
cs-410_4_10_380,"00:26:37,236","00:26:42,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1597,This weight tells us to
cs-410_4_10_381,"00:26:42,500","00:26:47,487",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1602,this word helps contribute in our decision
cs-410_4_10_382,"00:26:47,487","00:26:51,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1607,to put this document in category one.
cs-410_4_10_383,"00:26:51,930","00:26:54,495",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1611,"Now remember,"
cs-410_4_10_384,"00:26:54,495","00:26:56,426",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1614,the more likely it's in category one.
cs-410_4_10_385,"00:26:56,426","00:27:01,493",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1616,"Now if you look at this ratio, basically,"
cs-410_4_10_386,"00:27:01,493","00:27:06,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1621,on the ratio of the probability of the
cs-410_4_10_387,"00:27:06,025","00:27:09,492",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1626,Essentially we're comparing
cs-410_4_10_388,"00:27:09,492","00:27:11,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1629,distributions.
cs-410_4_10_389,"00:27:11,080","00:27:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1631,"And if it's a higher according to theta 1,"
cs-410_4_10_390,"00:27:14,780","00:27:20,289",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1634,"then according to theta 2,"
cs-410_4_10_391,"00:27:20,289","00:27:23,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1640,And therefore it means when
cs-410_4_10_392,"00:27:23,860","00:27:27,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1643,we will say that it's more
cs-410_4_10_393,"00:27:27,940","00:27:30,237",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1647,"And the more we observe such a word,"
cs-410_4_10_394,"00:27:30,237","00:27:34,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1650,the more likely the document
cs-410_4_10_395,"00:27:35,210","00:27:38,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1655,"If, on the other hand,"
cs-410_4_10_396,"00:27:38,940","00:27:42,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1658,theta 1 is smaller than the probability
cs-410_4_10_397,"00:27:42,720","00:27:45,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1662,then you can see that
cs-410_4_10_398,"00:27:45,220","00:27:51,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1665,"Therefore, this is negative evidence for"
cs-410_4_10_399,"00:27:51,390","00:27:54,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1671,That means the more we
cs-410_4_10_400,"00:27:54,010","00:27:56,587",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1674,the more likely the document
cs-410_4_10_401,"00:27:58,270","00:28:01,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1678,"So this formula now makes a little sense,"
cs-410_4_10_402,"00:28:01,350","00:28:05,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1681,So we're going to aggregate all
cs-410_4_10_403,"00:28:05,050","00:28:07,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1685,we take a sum of all the words.
cs-410_4_10_404,"00:28:07,010","00:28:09,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1687,We can call this the features
cs-410_4_10_405,"00:28:09,820","00:28:13,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1689,that we collected from the document
cs-410_4_10_406,"00:28:13,530","00:28:18,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1693,And then each feature has
cs-410_4_10_407,"00:28:19,660","00:28:24,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1699,does this feature support category one or
cs-410_4_10_408,"00:28:24,815","00:28:30,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1704,And this is estimated as the log of
cs-410_4_10_409,"00:28:32,315","00:28:35,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1712,And then finally we have
cs-410_4_10_410,"00:28:35,565","00:28:39,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1715,So that formula actually
cs-410_4_10_411,"00:28:39,555","00:28:43,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1719,be generalized to accommodate
cs-410_4_10_412,"00:28:43,862","00:28:48,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1723,that's why I have introduce
cs-410_4_10_413,"00:28:48,704","00:28:54,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1728,To introduce beta 0 to denote the Bayes
cs-410_4_10_414,"00:28:54,546","00:28:58,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1734,beta sub i to denote
cs-410_4_10_415,"00:28:58,269","00:29:03,154",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1738,"Now we do this generalisation,"
cs-410_4_10_416,"00:29:03,154","00:29:08,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1743,general we can represent
cs-410_4_10_417,"00:29:08,815","00:29:13,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1748,here of course in this case
cs-410_4_10_418,"00:29:13,960","00:29:17,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1753,"But in general, we can put any features"
cs-410_4_10_419,"00:29:17,700","00:29:18,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1757,categorization.
cs-410_4_10_420,"00:29:18,760","00:29:20,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1758,"For example, document length or"
cs-410_4_10_421,"00:29:20,650","00:29:25,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1760,font size or
cs-410_4_10_422,"00:29:27,310","00:29:35,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1767,And then our scoring function can be
cs-410_4_10_423,"00:29:35,120","00:29:40,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1775,the sum of the feature
cs-410_4_10_424,"00:29:42,156","00:29:46,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1782,So if each f sub i is a feature
cs-410_4_10_425,"00:29:46,890","00:29:51,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1786,"by the corresponding weight,"
cs-410_4_10_426,"00:29:51,140","00:29:54,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1791,And this is the aggregate of all evidence
cs-410_4_10_427,"00:29:54,860","00:29:56,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1794,features.
cs-410_4_10_428,"00:29:56,360","00:29:57,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1796,And of course there are parameters here.
cs-410_4_10_429,"00:29:57,960","00:29:59,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1797,So what are the parameters?
cs-410_4_10_430,"00:29:59,060","00:30:00,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1799,"Well, these are the betas."
cs-410_4_10_431,"00:30:00,510","00:30:02,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1800,These betas are weights.
cs-410_4_10_432,"00:30:02,690","00:30:07,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1802,"And with a proper setting of the weights,"
cs-410_4_10_433,"00:30:07,040","00:30:13,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1807,"to work well to classify documents,"
cs-410_4_10_434,"00:30:13,470","00:30:16,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1813,We can clearly see naive Bayes
cs-410_4_10_435,"00:30:16,770","00:30:18,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1816,this general classifier.
cs-410_4_10_436,"00:30:18,760","00:30:23,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1818,"Actually, this general form is very close"
cs-410_4_10_437,"00:30:23,940","00:30:28,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1823,"regression, and this is actually one"
cs-410_4_10_438,"00:30:28,800","00:30:31,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1828,discriminative approaches
cs-410_4_10_439,"00:30:32,340","00:30:36,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1832,And we're going to talk more
cs-410_4_10_440,"00:30:36,980","00:30:40,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1836,here I want you to note that
cs-410_4_10_441,"00:30:40,600","00:30:44,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1840,a close connection between
cs-410_4_10_442,"00:30:44,350","00:30:48,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1844,And this slide shows how naive Bayes
cs-410_4_10_443,"00:30:48,930","00:30:50,663",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1848,a logistic regression.
cs-410_4_10_444,"00:30:50,663","00:30:55,692",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1850,And you can also see that in
cs-410_4_10_445,"00:30:55,692","00:31:00,079",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1855,that tend to use more
cs-410_4_10_446,"00:31:00,079","00:31:05,116",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1860,we can accommodate more
cs-410_4_10_447,"00:31:05,116","00:31:15,116",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1865,[MUSIC]
cs-410_2_10_1,"00:00:00,069","00:00:07,429",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_10_2,"00:00:07,429","00:00:11,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,is a continuing discussion of Generative
cs-410_2_10_3,"00:00:13,450","00:00:17,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we are going to continue"
cs-410_2_10_4,"00:00:17,620","00:00:20,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"particularly, the Generative"
cs-410_2_10_5,"00:00:23,950","00:00:28,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So this is a slide that you have seen
cs-410_2_10_6,"00:00:28,320","00:00:32,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,the likelihood function for
cs-410_2_10_7,"00:00:32,735","00:00:38,049",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"distributions, being a two component"
cs-410_2_10_8,"00:00:39,800","00:00:47,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Now in this lecture, we're going to"
cs-410_2_10_9,"00:00:47,360","00:00:51,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,Now if you look at the formula and think
cs-410_2_10_10,"00:00:51,670","00:00:56,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,you'll realize that all we need is to add
cs-410_2_10_11,"00:00:57,960","00:01:04,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,So you can just add more thetas and
cs-410_2_10_12,"00:01:04,020","00:01:08,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,thetas and the probabilities of
cs-410_2_10_13,"00:01:08,890","00:01:13,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,So this is precisely what we
cs-410_2_10_14,"00:01:13,200","00:01:17,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,the general presentation of the mixture
cs-410_2_10_15,"00:01:19,810","00:01:24,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,So as more cases would follow these
cs-410_2_10_16,"00:01:24,820","00:01:27,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,think about our data.
cs-410_2_10_17,"00:01:27,430","00:01:30,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,And so in this case our data
cs-410_2_10_18,"00:01:30,360","00:01:33,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,"end documents denoted by d sub i,"
cs-410_2_10_19,"00:01:33,740","00:01:37,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"and then we talk about the other models,"
cs-410_2_10_20,"00:01:37,310","00:01:41,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"In this case, we design a mixture"
cs-410_2_10_21,"00:01:41,410","00:01:48,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,It's a little bit different from the topic
cs-410_2_10_22,"00:01:48,280","00:01:52,396",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We have a set of theta i's that
cs-410_2_10_23,"00:01:52,396","00:01:55,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,corresponding to the k
cs-410_2_10_24,"00:01:55,810","00:02:01,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,We have p of each theta i as
cs-410_2_10_25,"00:02:01,260","00:02:05,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,each of the k distributions
cs-410_2_10_26,"00:02:05,463","00:02:11,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,Now note that although our goal
cs-410_2_10_27,"00:02:11,090","00:02:16,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,we actually have used a more general
cs-410_2_10_28,"00:02:16,450","00:02:19,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"cluster and this as you will see later,"
cs-410_2_10_29,"00:02:19,560","00:02:25,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,will allow us to assign
cs-410_2_10_30,"00:02:25,610","00:02:29,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,that has the highest probability of
cs-410_2_10_31,"00:02:31,070","00:02:35,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"So as a result,"
cs-410_2_10_32,"00:02:36,880","00:02:40,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"properties, as you will see later."
cs-410_2_10_33,"00:02:42,390","00:02:46,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So the model basically would make
cs-410_2_10_34,"00:02:46,010","00:02:47,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,the generation of a document.
cs-410_2_10_35,"00:02:47,370","00:02:51,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,We first choose a theta i according
cs-410_2_10_36,"00:02:51,130","00:02:55,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,then generate all the words in
cs-410_2_10_37,"00:02:55,740","00:02:58,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,Note that it's important that we
cs-410_2_10_38,"00:02:58,500","00:03:02,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,use this distribution all
cs-410_2_10_39,"00:03:02,030","00:03:04,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,This is very different from topic model.
cs-410_2_10_40,"00:03:04,770","00:03:08,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,So the likelihood function would
cs-410_2_10_41,"00:03:10,060","00:03:16,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,So you can take a look
cs-410_2_10_42,"00:03:16,620","00:03:22,244",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,we have used the different notation
cs-410_2_10_43,"00:03:22,244","00:03:28,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,here in the second line of this equation.
cs-410_2_10_44,"00:03:28,810","00:03:33,837",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,You are going to see now
cs-410_2_10_45,"00:03:33,837","00:03:39,102",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"to use unique word in the vocabulary,"
cs-410_2_10_46,"00:03:39,102","00:03:45,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,instead of particular
cs-410_2_10_47,"00:03:45,130","00:03:50,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,"So from X subject to W,"
cs-410_2_10_48,"00:03:50,750","00:03:58,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,this change allows us to show
cs-410_2_10_49,"00:03:58,580","00:04:03,227",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,And you have seen this change also
cs-410_2_10_50,"00:04:03,227","00:04:08,191",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,it's basically still just a product of
cs-410_2_10_51,"00:04:10,010","00:04:10,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,And so
cs-410_2_10_52,"00:04:10,900","00:04:15,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"with the likelihood function, now we can"
cs-410_2_10_53,"00:04:15,100","00:04:19,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,Here we can simply use
cs-410_2_10_54,"00:04:19,090","00:04:22,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,So that's just a standard
cs-410_2_10_55,"00:04:22,960","00:04:25,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,So all should be familiar to you now.
cs-410_2_10_56,"00:04:25,880","00:04:27,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,It's just a different model.
cs-410_2_10_57,"00:04:27,890","00:04:30,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"So after we have estimated parameters,"
cs-410_2_10_58,"00:04:30,390","00:04:34,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,how can we then allocate
cs-410_2_10_59,"00:04:34,060","00:04:37,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"Well, let's take a look at"
cs-410_2_10_60,"00:04:37,740","00:04:41,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,So we just repeated the parameters
cs-410_2_10_61,"00:04:43,030","00:04:47,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,Now if you think about what we can
cs-410_2_10_62,"00:04:47,230","00:04:52,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,we can actually get more information than
cs-410_2_10_63,"00:04:52,640","00:04:57,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"So theta i for example,"
cs-410_2_10_64,"00:04:57,008","00:05:02,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"this is actually a by-product, it can help"
cs-410_2_10_65,"00:05:02,770","00:05:06,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,If you look at the top
cs-410_2_10_66,"00:05:06,020","00:05:09,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,in this word distribution and they will
cs-410_2_10_67,"00:05:11,130","00:05:16,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,p of theta i can be interpreted as
cs-410_2_10_68,"00:05:16,010","00:05:21,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,tells us how likely the cluster would
cs-410_2_10_69,"00:05:21,310","00:05:24,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,The more likely a cluster is
cs-410_2_10_70,"00:05:24,750","00:05:28,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,we can assume the larger
cs-410_2_10_71,"00:05:30,280","00:05:32,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,Note that unlike in PLSA and
cs-410_2_10_72,"00:05:32,880","00:05:36,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,this probability of theta
cs-410_2_10_73,"00:05:37,640","00:05:41,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,Now you may recall that the topic
cs-410_2_10_74,"00:05:41,520","00:05:42,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,actually depends on d.
cs-410_2_10_75,"00:05:42,750","00:05:48,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,That means each document can have
cs-410_2_10_76,"00:05:48,720","00:05:54,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,but here we have a generic choice
cs-410_2_10_77,"00:05:54,260","00:05:58,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"But of course, even a particular document"
cs-410_2_10_78,"00:05:58,950","00:06:01,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,topic is more likely to
cs-410_2_10_79,"00:06:01,840","00:06:02,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"So in that sense,"
cs-410_2_10_80,"00:06:02,770","00:06:08,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,we can still have a document
cs-410_2_10_81,"00:06:10,020","00:06:14,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,So now let's look at the key problem
cs-410_2_10_82,"00:06:14,890","00:06:16,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,assigning clusters to documents.
cs-410_2_10_83,"00:06:17,940","00:06:22,587",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,So that's the computer c sub d here and
cs-410_2_10_84,"00:06:22,587","00:06:27,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,the range of one through k to indicate
cs-410_2_10_85,"00:06:28,690","00:06:32,985",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,Now first you might think about
cs-410_2_10_86,"00:06:32,985","00:06:37,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,that is to assign d to the cluster
cs-410_2_10_87,"00:06:37,939","00:06:41,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,that most likely has
cs-410_2_10_88,"00:06:42,450","00:06:46,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So that means we're going to choose
cs-410_2_10_89,"00:06:46,530","00:06:49,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,gives d the highest probability.
cs-410_2_10_90,"00:06:49,500","00:06:50,734",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,"In other words,"
cs-410_2_10_91,"00:06:50,734","00:06:56,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,we see which distribution has the content
cs-410_2_10_92,"00:06:56,580","00:07:01,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"Intuitively that makes sense,"
cs-410_2_10_93,"00:07:01,870","00:07:06,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"does not consider the size of clusters,"
cs-410_2_10_94,"00:07:06,980","00:07:12,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,so a better way is to use
cs-410_2_10_95,"00:07:12,140","00:07:16,038",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,in this case the prior is p of theta i.
cs-410_2_10_96,"00:07:16,038","00:07:20,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"And together, that is, we're going to"
cs-410_2_10_97,"00:07:20,880","00:07:24,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"the posterior probability of theta,"
cs-410_2_10_98,"00:07:25,650","00:07:30,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,And if we choose theta .based
cs-410_2_10_99,"00:07:30,058","00:07:36,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,we would have the following formula that
cs-410_2_10_100,"00:07:36,010","00:07:42,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"And in this case, we're going to choose"
cs-410_2_10_101,"00:07:42,390","00:07:47,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,that means a large cluster and
cs-410_2_10_102,"00:07:47,610","00:07:51,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,So we're going to favor
cs-410_2_10_103,"00:07:51,690","00:07:54,982",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,also consistent with the document.
cs-410_2_10_104,"00:07:54,982","00:08:01,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,And that intuitively makes
cs-410_2_10_105,"00:08:01,090","00:08:05,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,a document being a large cluster is
cs-410_2_10_106,"00:08:07,640","00:08:13,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So this means once we can estimate
cs-410_2_10_107,"00:08:13,000","00:08:16,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,then we can easily solve
cs-410_2_10_108,"00:08:16,930","00:08:20,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"So next, we'll have to discuss how to"
cs-410_2_10_109,"00:08:20,850","00:08:25,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,actually compute
cs-410_2_10_110,"00:08:25,512","00:08:35,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,[MUSIC]
cs-410_3_10_1,"00:00:00,192","00:00:03,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_10_2,"00:00:06,614","00:00:09,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about text categorization.
cs-410_3_10_3,"00:00:11,360","00:00:15,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture, we're going to"
cs-410_3_10_4,"00:00:16,390","00:00:21,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,This is a very important technique for
cs-410_3_10_5,"00:00:22,470","00:00:27,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,It is relevant to discovery
cs-410_3_10_6,"00:00:27,035","00:00:29,134",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,knowledge as shown here.
cs-410_3_10_7,"00:00:29,134","00:00:33,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"First, it's related to topic mining and"
cs-410_3_10_8,"00:00:33,380","00:00:36,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"And, that's because it has to do with"
cs-410_3_10_9,"00:00:36,060","00:00:40,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,analyzing text to data based
cs-410_3_10_10,"00:00:40,970","00:00:46,239",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"Secondly, it's also related to"
cs-410_3_10_11,"00:00:46,239","00:00:51,941",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,which has to do with discovery knowledge
cs-410_3_10_12,"00:00:51,941","00:00:56,301",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"Because we can categorize the authors,"
cs-410_3_10_13,"00:00:56,301","00:01:01,813",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,based on the content of the articles
cs-410_3_10_14,"00:01:01,813","00:01:06,611",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"We can, in general,"
cs-410_3_10_15,"00:01:06,611","00:01:10,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,based on the content that they produce.
cs-410_3_10_16,"00:01:12,300","00:01:16,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"Finally, it's also related"
cs-410_3_10_17,"00:01:16,720","00:01:21,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"Because, we can often use text"
cs-410_3_10_18,"00:01:21,760","00:01:26,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,variables in the real world that
cs-410_3_10_19,"00:01:27,230","00:01:32,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"And so, this is a very important"
cs-410_3_10_20,"00:01:34,820","00:01:37,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,This is the overall plan for
cs-410_3_10_21,"00:01:37,860","00:01:40,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"First, we're going to talk about"
cs-410_3_10_22,"00:01:40,750","00:01:44,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,why we're interested in
cs-410_3_10_23,"00:01:44,510","00:01:47,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"And now, we're going to talk about"
cs-410_3_10_24,"00:01:47,920","00:01:50,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,how to evaluate
cs-410_3_10_25,"00:01:50,780","00:01:56,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"So, the problem of text"
cs-410_3_10_26,"00:01:56,140","00:02:03,461",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,We're given a set of predefined categories
cs-410_3_10_27,"00:02:03,461","00:02:07,462",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"And often,"
cs-410_3_10_28,"00:02:07,462","00:02:12,519",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,training set of labeled text
cs-410_3_10_29,"00:02:12,519","00:02:17,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,objects have already been
cs-410_3_10_30,"00:02:17,810","00:02:23,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"And then, the task is to classify"
cs-410_3_10_31,"00:02:23,040","00:02:26,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,more of these predefined categories.
cs-410_3_10_32,"00:02:26,320","00:02:29,139",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"So, the picture on this"
cs-410_3_10_33,"00:02:30,270","00:02:32,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,"When we do text categorization,"
cs-410_3_10_34,"00:02:32,120","00:02:37,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,we have a lot of text objects to be
cs-410_3_10_35,"00:02:37,630","00:02:43,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"the system will, in general,"
cs-410_3_10_36,"00:02:43,820","00:02:49,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,As shown on the right and
cs-410_3_10_37,"00:02:49,110","00:02:54,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,and we often assume the availability
cs-410_3_10_38,"00:02:54,280","00:02:59,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,these are the documents that
cs-410_3_10_39,"00:02:59,060","00:03:01,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,And these examples are very important for
cs-410_3_10_40,"00:03:01,660","00:03:06,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,helping the system to learn
cs-410_3_10_41,"00:03:06,110","00:03:10,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"And, this would further help"
cs-410_3_10_42,"00:03:11,280","00:03:16,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,the categories of new text
cs-410_3_10_43,"00:03:16,560","00:03:20,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"So, here are some specific"
cs-410_3_10_44,"00:03:20,950","00:03:26,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"And in fact, there are many examples,"
cs-410_3_10_45,"00:03:27,230","00:03:33,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"So first, text objects can vary,"
cs-410_3_10_46,"00:03:33,000","00:03:36,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"or a passage, or a sentence,"
cs-410_3_10_47,"00:03:36,730","00:03:41,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"As in the case of clustering, the units"
cs-410_3_10_48,"00:03:41,400","00:03:44,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,this creates a lot of possibilities.
cs-410_3_10_49,"00:03:44,090","00:03:46,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"Secondly, categories can also vary."
cs-410_3_10_50,"00:03:46,690","00:03:49,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,"Allocate in general,"
cs-410_3_10_51,"00:03:49,880","00:03:51,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,One is internal categories.
cs-410_3_10_52,"00:03:51,560","00:03:55,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,These are categories that
cs-410_3_10_53,"00:03:55,890","00:04:00,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"For example, topic categories or"
cs-410_3_10_54,"00:04:00,850","00:04:04,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,they generally have to do with
cs-410_3_10_55,"00:04:04,810","00:04:06,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,throughout the categorization
cs-410_3_10_56,"00:04:08,210","00:04:13,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,The other kind is external categories
cs-410_3_10_57,"00:04:13,430","00:04:16,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,associated with the text object.
cs-410_3_10_58,"00:04:16,120","00:04:17,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"For example,"
cs-410_3_10_59,"00:04:17,630","00:04:22,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,authors are entities associated
cs-410_3_10_60,"00:04:22,810","00:04:28,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"And so, we can use their content in"
cs-410_3_10_61,"00:04:28,340","00:04:31,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,"which part, for example, and"
cs-410_3_10_62,"00:04:33,540","00:04:38,048",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"Or, we can have any"
cs-410_3_10_63,"00:04:38,048","00:04:43,147",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,associate with text data
cs-410_3_10_64,"00:04:43,147","00:04:47,788",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,connection between the entity and
cs-410_3_10_65,"00:04:47,788","00:04:54,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"For example, we might collect a lot"
cs-410_3_10_66,"00:04:54,025","00:04:58,073",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"a lot of reviews about a product,"
cs-410_3_10_67,"00:04:58,073","00:05:04,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,this text data can help us infer
cs-410_3_10_68,"00:05:04,770","00:05:07,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"In that case, we can treat this"
cs-410_3_10_69,"00:05:07,770","00:05:09,921",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,We can categorize restaurants or
cs-410_3_10_70,"00:05:09,921","00:05:13,924",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,categorize products based on
cs-410_3_10_71,"00:05:13,924","00:05:17,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"So, this is an example for"
cs-410_3_10_72,"00:05:17,245","00:05:20,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,Here are some specific
cs-410_3_10_73,"00:05:20,400","00:05:25,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,News categorization is very
cs-410_3_10_74,"00:05:25,110","00:05:30,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,News agencies would like
cs-410_3_10_75,"00:05:30,009","00:05:35,672",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,categories to categorize
cs-410_3_10_76,"00:05:35,672","00:05:39,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"And, these virtual article"
cs-410_3_10_77,"00:05:39,824","00:05:43,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"For example, in the biomedical domain,"
cs-410_3_10_78,"00:05:43,650","00:05:47,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"MeSH stands for Medical Subject Heading,"
cs-410_3_10_79,"00:05:49,090","00:05:52,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,characterize content of
cs-410_3_10_80,"00:05:54,590","00:05:59,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Another example of application is spam
cs-410_3_10_81,"00:05:59,860","00:06:04,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"So, we often have a spam filter"
cs-410_3_10_82,"00:06:04,940","00:06:10,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,to help us distinguish spams
cs-410_3_10_83,"00:06:10,260","00:06:13,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,this is clearly a binary
cs-410_3_10_84,"00:06:14,500","00:06:18,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,Sentiment categorization of
cs-410_3_10_85,"00:06:18,460","00:06:23,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,another kind of applications where we
cs-410_3_10_86,"00:06:23,120","00:06:26,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,negative or positive and
cs-410_3_10_87,"00:06:27,460","00:06:32,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"So, you can have send them to categories,"
cs-410_3_10_88,"00:06:35,520","00:06:39,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,Another application is automatic
cs-410_3_10_89,"00:06:39,480","00:06:43,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,you might want to automatically sort your
cs-410_3_10_90,"00:06:43,750","00:06:47,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,one application of text categorization
cs-410_3_10_91,"00:06:48,370","00:06:52,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,The results are another important kind
cs-410_3_10_92,"00:06:52,580","00:06:55,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"to the right person to handle,"
cs-410_3_10_93,"00:06:55,910","00:07:01,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,email messaging is generally routed
cs-410_3_10_94,"00:07:01,890","00:07:05,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Different people tend to handle
cs-410_3_10_95,"00:07:05,820","00:07:11,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"And in many cases, a person would manually"
cs-410_3_10_96,"00:07:11,220","00:07:15,231",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,"But, if you can imagine,"
cs-410_3_10_97,"00:07:15,231","00:07:18,794",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,text categorization system
cs-410_3_10_98,"00:07:18,794","00:07:24,969",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"And, this is a class file, the incoming"
cs-410_3_10_99,"00:07:24,969","00:07:31,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,where each category actually corresponds
cs-410_3_10_100,"00:07:31,265","00:07:35,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"And finally, author attribution, as I just"
cs-410_3_10_101,"00:07:35,975","00:07:39,759",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,it's another example of using text
cs-410_3_10_102,"00:07:41,480","00:07:42,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,some other entities.
cs-410_3_10_103,"00:07:42,960","00:07:46,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,"And, there are also many variants"
cs-410_3_10_104,"00:07:46,890","00:07:50,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,"And so, first, we have the simplest case,"
cs-410_3_10_105,"00:07:50,980","00:07:52,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,where there are only two categories.
cs-410_3_10_106,"00:07:52,990","00:07:57,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"And, there are many examples like that,"
cs-410_3_10_107,"00:07:59,040","00:08:03,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,Applications with one distinguishing
cs-410_3_10_108,"00:08:03,600","00:08:04,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,documents for a particular query.
cs-410_3_10_109,"00:08:06,040","00:08:12,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,Spam filtering just distinguishing spams
cs-410_3_10_110,"00:08:12,330","00:08:16,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"Sometimes, classifications of"
cs-410_3_10_111,"00:08:16,800","00:08:17,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,positive and a negative.
cs-410_3_10_112,"00:08:19,120","00:08:22,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,A more general case would be K-category
cs-410_3_10_113,"00:08:22,650","00:08:26,755",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"many applications like that,"
cs-410_3_10_114,"00:08:26,755","00:08:30,155",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"So, topic categorization is often"
cs-410_3_10_115,"00:08:30,155","00:08:31,935",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,multiple topics.
cs-410_3_10_116,"00:08:31,935","00:08:36,205",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,Email routing would be another example
cs-410_3_10_117,"00:08:36,205","00:08:39,322",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,if you route the email to
cs-410_3_10_118,"00:08:39,322","00:08:44,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,then there are multiple
cs-410_3_10_119,"00:08:44,550","00:08:48,212",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"So, in all these cases, there are more"
cs-410_3_10_120,"00:08:49,272","00:08:52,382",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,Another variation is to have
cs-410_3_10_121,"00:08:52,382","00:08:54,442",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,where categories form a hierarchy.
cs-410_3_10_122,"00:08:54,442","00:08:56,602",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"Again, topical hierarchy is very common."
cs-410_3_10_123,"00:08:58,232","00:09:00,742",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Yet another variation is
cs-410_3_10_124,"00:09:00,742","00:09:04,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,That's when you have multiple
cs-410_3_10_125,"00:09:04,550","00:09:08,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,then you hope to kind of
cs-410_3_10_126,"00:09:08,150","00:09:13,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,Further leverage the dependency of
cs-410_3_10_127,"00:09:13,340","00:09:15,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,each individual task.
cs-410_3_10_128,"00:09:15,250","00:09:19,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Among all these binary categorizations
cs-410_3_10_129,"00:09:19,870","00:09:25,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,part of it also is because it's simple and
cs-410_3_10_130,"00:09:25,170","00:09:31,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,it can actually be used to perform
cs-410_3_10_131,"00:09:31,000","00:09:34,839",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"For example, a K-category"
cs-410_3_10_132,"00:09:34,839","00:09:38,665",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,performed by using binary categorization.
cs-410_3_10_133,"00:09:40,075","00:09:43,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,"Basically, we can look at"
cs-410_3_10_134,"00:09:43,405","00:09:49,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,then the binary categorization problem
cs-410_3_10_135,"00:09:49,385","00:09:52,005",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"not, meaning in other categories."
cs-410_3_10_136,"00:09:53,485","00:09:59,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,"And, the hierarchical categorization"
cs-410_3_10_137,"00:09:59,820","00:10:04,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,doing flat categorization at each level.
cs-410_3_10_138,"00:10:04,300","00:10:07,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"So, we have, first, we categorize"
cs-410_3_10_139,"00:10:07,000","00:10:09,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"a small number of high-level categories,"
cs-410_3_10_140,"00:10:09,140","00:10:13,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"and inside each category, we have further"
cs-410_3_10_141,"00:10:15,000","00:10:16,728",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,"So, why is text categorization important?"
cs-410_3_10_142,"00:10:16,728","00:10:21,464",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"Well, I already showed that you,"
cs-410_3_10_143,"00:10:21,464","00:10:23,244",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,there are several reasons.
cs-410_3_10_144,"00:10:23,244","00:10:28,891",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,One is text categorization helps enrich
cs-410_3_10_145,"00:10:28,891","00:10:34,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,more understanding of text data that's
cs-410_3_10_146,"00:10:34,970","00:10:38,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"So, now with categorization text can"
cs-410_3_10_147,"00:10:38,738","00:10:47,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,The keyword conditions that's often
cs-410_3_10_148,"00:10:47,310","00:10:52,455",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,But we can now also add categories and
cs-410_3_10_149,"00:10:55,485","00:11:00,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,Semantic categories assigned can also
cs-410_3_10_150,"00:11:00,085","00:11:01,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,application.
cs-410_3_10_151,"00:11:01,145","00:11:07,869",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,"So, for example, semantic categories"
cs-410_3_10_152,"00:11:07,869","00:11:12,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,other attribution might
cs-410_3_10_153,"00:11:12,248","00:11:18,118",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,Another example is when semantic
cs-410_3_10_154,"00:11:18,118","00:11:24,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,of text content and this is another case
cs-410_3_10_155,"00:11:25,950","00:11:30,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,"For example, if we want to know"
cs-410_3_10_156,"00:11:32,010","00:11:37,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,could first categorize the opinions
cs-410_3_10_157,"00:11:37,830","00:11:42,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,"as positive or negative and then, that"
cs-410_3_10_158,"00:11:42,730","00:11:47,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"the sentiment, and it would tell us about"
cs-410_3_10_159,"00:11:47,810","00:11:52,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,the 70% of the views are positive and
cs-410_3_10_160,"00:11:53,810","00:11:56,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,"So, without doing categorization,"
cs-410_3_10_161,"00:11:56,865","00:12:02,402",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,it will be much harder to aggregate
cs-410_3_10_162,"00:12:02,402","00:12:07,468",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,way of coding text in some sense
cs-410_3_10_163,"00:12:07,468","00:12:13,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"And, sometimes you may see in some"
cs-410_3_10_164,"00:12:13,640","00:12:18,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,"called a text coded,"
cs-410_3_10_165,"00:12:18,704","00:12:22,316",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,The second kind of reasons is to use text
cs-410_3_10_166,"00:12:22,316","00:12:27,024",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=742,categorization to infer
cs-410_3_10_167,"00:12:27,024","00:12:31,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,and text categories allows
cs-410_3_10_168,"00:12:31,950","00:12:36,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,of such entities that
cs-410_3_10_169,"00:12:36,950","00:12:41,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,"So, this means we can"
cs-410_3_10_170,"00:12:41,140","00:12:44,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,to discover knowledge about the world.
cs-410_3_10_171,"00:12:44,090","00:12:48,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"In general, as long as we can associate"
cs-410_3_10_172,"00:12:48,370","00:12:53,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,we can always the text of data to help
cs-410_3_10_173,"00:12:53,600","00:12:54,502",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,"So, it's used for"
cs-410_3_10_174,"00:12:54,502","00:12:59,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,single information network that will
cs-410_3_10_175,"00:12:59,380","00:13:03,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,The obvious entities that can be
cs-410_3_10_176,"00:13:03,750","00:13:08,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,"But, you can also imagine the author's"
cs-410_3_10_177,"00:13:08,340","00:13:14,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,other things can be actually
cs-410_3_10_178,"00:13:14,090","00:13:18,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"Once we have made the connection, then we"
cs-410_3_10_179,"00:13:18,860","00:13:23,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"So, this is a general way to allow"
cs-410_3_10_180,"00:13:23,520","00:13:26,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,the text categorization to discover
cs-410_3_10_181,"00:13:26,890","00:13:32,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,"Very useful, especially in big text"
cs-410_3_10_182,"00:13:32,330","00:13:38,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,just using text data as extra sets
cs-410_3_10_183,"00:13:38,150","00:13:43,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,to infer certain decision factors
cs-410_3_10_184,"00:13:43,930","00:13:45,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"Specifically with text, for example,"
cs-410_3_10_185,"00:13:45,710","00:13:49,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,we can also think of examples of
cs-410_3_10_186,"00:13:49,220","00:13:53,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,"For example, discovery of"
cs-410_3_10_187,"00:13:53,190","00:13:59,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"And, this can be done by categorizing"
cs-410_3_10_188,"00:14:00,680","00:14:05,566",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,Another example is to predict the party
cs-410_3_10_189,"00:14:05,566","00:14:07,314",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,on the political speech.
cs-410_3_10_190,"00:14:07,314","00:14:11,967",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,"And, this is again an example"
cs-410_3_10_191,"00:14:11,967","00:14:15,146",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,some knowledge about the real world.
cs-410_3_10_192,"00:14:15,146","00:14:19,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,"In nature,"
cs-410_3_10_193,"00:14:19,265","00:14:24,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,that's as we defined and
cs-410_3_10_194,"00:14:24,980","00:14:34,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=864,[MUSIC]
cs-410_1_10_1,"00:00:00,012","00:00:07,427",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_10_2,"00:00:07,427","00:00:10,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is the first one
cs-410_1_10_3,"00:00:14,165","00:00:17,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we are going to"
cs-410_1_10_4,"00:00:18,430","00:00:24,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,This is a very important technique for
cs-410_1_10_5,"00:00:24,590","00:00:25,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"In particular,"
cs-410_1_10_6,"00:00:25,100","00:00:30,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,in this lecture we're going to start with
cs-410_1_10_7,"00:00:31,650","00:00:37,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"And that is, what is text clustering and"
cs-410_1_10_8,"00:00:38,060","00:00:42,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"In the following lectures, we are going"
cs-410_1_10_9,"00:00:42,610","00:00:44,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,How to evaluate the clustering results?
cs-410_1_10_10,"00:00:47,060","00:00:48,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,So what is text clustering?
cs-410_1_10_11,"00:00:49,500","00:00:52,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,"Well, clustering actually is"
cs-410_1_10_12,"00:00:52,640","00:00:55,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,data mining as you might have
cs-410_1_10_13,"00:00:56,760","00:01:00,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,The idea is to discover natural
cs-410_1_10_14,"00:01:01,250","00:01:05,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"In another words,"
cs-410_1_10_15,"00:01:05,040","00:01:09,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"In our case, these objects are of course,"
cs-410_1_10_16,"00:01:09,170","00:01:14,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,"For example, they can be documents,"
cs-410_1_10_17,"00:01:14,510","00:01:21,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"sentences, or websites, and then I'll"
cs-410_1_10_18,"00:01:21,510","00:01:26,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"So let's see an example, well, here"
cs-410_1_10_19,"00:01:26,560","00:01:31,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,I just used some shapes to denote
cs-410_1_10_20,"00:01:33,450","00:01:39,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"Now if I ask you, what are some natural"
cs-410_1_10_21,"00:01:39,690","00:01:47,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,if you look at it and you might agree that
cs-410_1_10_22,"00:01:47,790","00:01:51,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,or their locations on this
cs-410_1_10_23,"00:01:53,240","00:01:55,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,So we got the three clusters in this case.
cs-410_1_10_24,"00:01:56,940","00:02:01,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And they may not be so
cs-410_1_10_25,"00:02:01,360","00:02:06,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,these three clusters but it really depends
cs-410_1_10_26,"00:02:07,650","00:02:11,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,Maybe some of you have also seen
cs-410_1_10_27,"00:02:11,450","00:02:14,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,we might get different clusters.
cs-410_1_10_28,"00:02:14,050","00:02:21,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,And you'll see another example
cs-410_1_10_29,"00:02:21,400","00:02:27,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"But the main point of here is, the problem"
cs-410_1_10_30,"00:02:29,200","00:02:34,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And the problem lies in
cs-410_1_10_31,"00:02:34,130","00:02:36,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,And what do you mean by similar objects?
cs-410_1_10_32,"00:02:38,160","00:02:40,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,Now this problem has to be
cs-410_1_10_33,"00:02:40,990","00:02:44,537",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,clearly defined in order to have
cs-410_1_10_34,"00:02:46,315","00:02:49,295",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And the problem is in general
cs-410_1_10_35,"00:02:49,295","00:02:53,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,that any two objects can be similar
cs-410_1_10_36,"00:02:53,445","00:02:59,119",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"So for example, this will kept"
cs-410_1_10_37,"00:03:00,300","00:03:02,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,So are the two words similar?
cs-410_1_10_38,"00:03:02,650","00:03:08,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,"Well, it depends on how if"
cs-410_1_10_39,"00:03:11,070","00:03:16,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,properties of car and
cs-410_1_10_40,"00:03:16,000","00:03:20,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"if you look at them functionally,"
cs-410_1_10_41,"00:03:20,630","00:03:23,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,can both be transportation tools.
cs-410_1_10_42,"00:03:23,650","00:03:26,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"So in that sense, they may be similar."
cs-410_1_10_43,"00:03:26,220","00:03:31,599",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"So as we can see, it really depends on"
cs-410_1_10_44,"00:03:32,740","00:03:37,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,And so it ought to make
cs-410_1_10_45,"00:03:37,700","00:03:43,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,A user must define the perspective for
cs-410_1_10_46,"00:03:44,310","00:03:47,599",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,And we call this perspective
cs-410_1_10_47,"00:03:49,270","00:03:54,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"And when you define a clustering problem,"
cs-410_1_10_48,"00:03:55,340","00:04:00,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,your perspective for
cs-410_1_10_49,"00:04:00,870","00:04:05,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,the similarity that will be
cs-410_1_10_50,"00:04:05,830","00:04:11,795",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"because otherwise,"
cs-410_1_10_51,"00:04:11,795","00:04:16,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,one can have different
cs-410_1_10_52,"00:04:16,870","00:04:19,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,So let's look at the example here.
cs-410_1_10_53,"00:04:19,910","00:04:24,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,"You are seeing some objects,"
cs-410_1_10_54,"00:04:24,210","00:04:29,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,that are very similar to what you
cs-410_1_10_55,"00:04:29,730","00:04:34,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"if I ask you to group these objects,"
cs-410_1_10_56,"00:04:38,040","00:04:42,052",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,feel there is more than here
cs-410_1_10_57,"00:04:42,052","00:04:47,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"For example, you might think, well, we"
cs-410_1_10_58,"00:04:47,810","00:04:53,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"can steer a group by ships, so that would"
cs-410_1_10_59,"00:04:53,510","00:04:57,558",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"However, you might also feel that,"
cs-410_1_10_60,"00:04:57,558","00:05:02,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"well, maybe the objects can be"
cs-410_1_10_61,"00:05:02,380","00:05:07,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,So that would give us a different way
cs-410_1_10_62,"00:05:07,020","00:05:11,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,the size and
cs-410_1_10_63,"00:05:12,910","00:05:16,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"So as you can see clearly here,"
cs-410_1_10_64,"00:05:16,440","00:05:18,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,we'll get different clustering result.
cs-410_1_10_65,"00:05:18,860","00:05:23,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,So that also clearly tells us that in
cs-410_1_10_66,"00:05:23,750","00:05:27,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,we must use perspective.
cs-410_1_10_67,"00:05:27,060","00:05:32,054",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Without perspective, it's very hard to"
cs-410_1_10_68,"00:05:36,152","00:05:40,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,So there are many examples
cs-410_1_10_69,"00:05:42,330","00:05:48,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,"And so for example, we can cluster"
cs-410_1_10_70,"00:05:48,380","00:05:51,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"So in this case,"
cs-410_1_10_71,"00:05:52,280","00:05:55,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,We may be able to cluster terms.
cs-410_1_10_72,"00:05:55,780","00:05:58,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,"In this case, terms are objects."
cs-410_1_10_73,"00:05:58,300","00:06:03,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,And a cluster of terms can be used to
cs-410_1_10_74,"00:06:03,480","00:06:08,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"In fact, there's a topic models that you"
cs-410_1_10_75,"00:06:08,530","00:06:13,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,give you cluster of terms in some
cs-410_1_10_76,"00:06:13,850","00:06:19,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,high probabilities from word distribution.
cs-410_1_10_77,"00:06:19,610","00:06:25,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,Another example is just to cluster any
cs-410_1_10_78,"00:06:25,330","00:06:30,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"sentences, or any segments that you can"
cs-410_1_10_79,"00:06:32,100","00:06:36,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"For example, we might extract the order"
cs-410_1_10_80,"00:06:36,300","00:06:39,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"let's say, by using a topic model."
cs-410_1_10_81,"00:06:39,030","00:06:43,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,Now once we've got those
cs-410_1_10_82,"00:06:45,120","00:06:50,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,cluster the segments that we've got to
cs-410_1_10_83,"00:06:50,850","00:06:56,908",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,discover interesting clusters that
cs-410_1_10_84,"00:06:56,908","00:07:00,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,So this is a case of combining text
cs-410_1_10_85,"00:07:00,860","00:07:03,762",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And in general you will
cs-410_1_10_86,"00:07:05,140","00:07:09,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,can be accurate combined in
cs-410_1_10_87,"00:07:09,670","00:07:14,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,the goal of doing more sophisticated
cs-410_1_10_88,"00:07:16,030","00:07:20,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,We can also cluster fairly
cs-410_1_10_89,"00:07:20,070","00:07:24,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,I just mean text objects may
cs-410_1_10_90,"00:07:24,600","00:07:27,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"So for example, we might cluster websites."
cs-410_1_10_91,"00:07:27,440","00:07:31,228",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Each website is actually
cs-410_1_10_92,"00:07:31,228","00:07:39,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"Similarly, we can also cluster articles"
cs-410_1_10_93,"00:07:39,065","00:07:44,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,So we can trigger all the articles
cs-410_1_10_94,"00:07:44,573","00:07:45,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,clustering.
cs-410_1_10_95,"00:07:45,785","00:07:50,652",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"In this way, we might group authors"
cs-410_1_10_96,"00:07:50,652","00:07:52,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,published papers or similar.
cs-410_1_10_97,"00:07:55,150","00:08:00,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,For the more text clusters will be for
cs-410_1_10_98,"00:08:00,290","00:08:06,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,That's because we can in general cluster
cs-410_1_10_99,"00:08:08,210","00:08:11,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,So more generally why is
cs-410_1_10_100,"00:08:11,750","00:08:16,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"Well, it's because it's a very"
cs-410_1_10_101,"00:08:16,100","00:08:18,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,particularly exploratory text analysis.
cs-410_1_10_102,"00:08:20,250","00:08:25,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,And so a typical scenario is that
cs-410_1_10_103,"00:08:25,690","00:08:30,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,let's say all the email messages
cs-410_1_10_104,"00:08:30,300","00:08:32,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"all the literature articles, etc."
cs-410_1_10_105,"00:08:32,070","00:08:35,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,And then you hope to get a sense
cs-410_1_10_106,"00:08:35,960","00:08:40,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,"the connection, so for example,"
cs-410_1_10_107,"00:08:40,970","00:08:45,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"a sense about major topics,"
cs-410_1_10_108,"00:08:45,910","00:08:49,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,representative documents
cs-410_1_10_109,"00:08:49,360","00:08:53,164",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,And clustering to help
cs-410_1_10_110,"00:08:53,164","00:08:59,949",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,We sometimes also want to link
cs-410_1_10_111,"00:08:59,949","00:09:03,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,And these objects might be
cs-410_1_10_112,"00:09:03,960","00:09:04,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"And in that case,"
cs-410_1_10_113,"00:09:04,830","00:09:09,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,such a technique can help us remove
cs-410_1_10_114,"00:09:10,910","00:09:13,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,Sometimes they are about
cs-410_1_10_115,"00:09:13,280","00:09:17,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,by linking them together we can have
cs-410_1_10_116,"00:09:19,660","00:09:24,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,We may also used text clustering to create
cs-410_1_10_117,"00:09:24,420","00:09:28,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,we can create a hierarchy of structures
cs-410_1_10_118,"00:09:31,270","00:09:36,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,We may also use text clustering to induce
cs-410_1_10_119,"00:09:36,140","00:09:40,206",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"data when we cluster documents together,"
cs-410_1_10_120,"00:09:40,206","00:09:44,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,And then we can say when
cs-410_1_10_121,"00:09:44,100","00:09:45,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,then the feature value would be one.
cs-410_1_10_122,"00:09:45,790","00:09:49,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"And if a document is not in this cluster,"
cs-410_1_10_123,"00:09:49,870","00:09:54,298",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,And this helps provide additional
cs-410_1_10_124,"00:09:54,298","00:09:57,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,text classification as
cs-410_1_10_125,"00:09:59,870","00:10:03,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"So there are, in general,"
cs-410_1_10_126,"00:10:03,320","00:10:06,218",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,And I just thought of
cs-410_1_10_127,"00:10:06,218","00:10:08,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"One is to cluster search results, for"
cs-410_1_10_128,"00:10:08,490","00:10:12,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"example, [INAUDIBLE] search engine"
cs-410_1_10_129,"00:10:12,360","00:10:19,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,that the user can see overall structure
cs-410_1_10_130,"00:10:19,020","00:10:22,454",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,And when the query's ambiguous this
cs-410_1_10_131,"00:10:22,454","00:10:26,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,the clusters likely represent
cs-410_1_10_132,"00:10:28,630","00:10:33,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,Another application is to understand the
cs-410_1_10_133,"00:10:33,535","00:10:34,943",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,"their emails, right."
cs-410_1_10_134,"00:10:34,943","00:10:40,238",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"So in this case,"
cs-410_1_10_135,"00:10:40,238","00:10:44,903",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,then find in the major
cs-410_1_10_136,"00:10:44,903","00:10:51,355",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,we can understand what are the major
cs-410_1_10_137,"00:10:51,355","00:10:57,897",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,[MUSIC]
cs-410_5_8_1,"00:00:00,025","00:00:06,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_8_2,"00:00:06,885","00:00:11,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,lecture is about topic mining and
cs-410_5_8_3,"00:00:11,190","00:00:14,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,We're going to talk about its
cs-410_5_8_4,"00:00:17,780","00:00:22,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,In this lecture we're going to talk
cs-410_5_8_5,"00:00:23,770","00:00:28,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"As you see on this road map,"
cs-410_5_8_6,"00:00:28,310","00:00:33,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,"mining knowledge about language,"
cs-410_5_8_7,"00:00:33,190","00:00:37,987",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,word associations such as paradigmatic and
cs-410_5_8_8,"00:00:39,190","00:00:43,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Now, starting from this lecture, we're"
cs-410_5_8_9,"00:00:43,100","00:00:47,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,"knowledge, which is content mining, and"
cs-410_5_8_10,"00:00:47,570","00:00:55,031",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,trying to discover knowledge about
cs-410_5_8_11,"00:00:56,140","00:00:58,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And we call that topic mining and
cs-410_5_8_12,"00:00:59,920","00:01:04,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,"In this lecture, we're going to talk about"
cs-410_5_8_13,"00:01:04,350","00:01:08,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"So first of all,"
cs-410_5_8_14,"00:01:08,260","00:01:12,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,So topic is something that we
cs-410_5_8_15,"00:01:12,600","00:01:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,it's actually not that
cs-410_5_8_16,"00:01:15,840","00:01:20,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Roughly speaking, topic is the main"
cs-410_5_8_17,"00:01:20,420","00:01:25,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,And you can think of this as a theme or
cs-410_5_8_18,"00:01:25,860","00:01:28,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,It can also have different granularities.
cs-410_5_8_19,"00:01:28,420","00:01:31,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"For example,"
cs-410_5_8_20,"00:01:31,240","00:01:34,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"A topic of article,"
cs-410_5_8_21,"00:01:34,800","00:01:40,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,the topic of all the research articles
cs-410_5_8_22,"00:01:40,540","00:01:45,629",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,so different grand narratives of topics
cs-410_5_8_23,"00:01:46,760","00:01:51,628",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Indeed, there are many applications that"
cs-410_5_8_24,"00:01:51,628","00:01:52,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,they're analyzed then.
cs-410_5_8_25,"00:01:52,980","00:01:54,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,Here are some examples.
cs-410_5_8_26,"00:01:54,300","00:01:58,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"For example, we might be interested"
cs-410_5_8_27,"00:01:58,280","00:02:00,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,users are talking about today?
cs-410_5_8_28,"00:02:00,470","00:02:03,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"Are they talking about NBA sports, or"
cs-410_5_8_29,"00:02:03,600","00:02:08,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,are they talking about some
cs-410_5_8_30,"00:02:08,540","00:02:12,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,Or we are interested in
cs-410_5_8_31,"00:02:12,970","00:02:17,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"For example, one might be interested in"
cs-410_5_8_32,"00:02:17,090","00:02:21,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"topics in data mining, and how are they"
cs-410_5_8_33,"00:02:21,840","00:02:26,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Now this involves discovery of topics
cs-410_5_8_34,"00:02:26,820","00:02:32,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,also we want to discover topics in
cs-410_5_8_35,"00:02:32,910","00:02:34,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,And then we can make a comparison.
cs-410_5_8_36,"00:02:34,690","00:02:38,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,We might also be also interested in
cs-410_5_8_37,"00:02:38,400","00:02:43,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"some products like the iPhone 6,"
cs-410_5_8_38,"00:02:43,710","00:02:48,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,And this involves discovering
cs-410_5_8_39,"00:02:48,360","00:02:52,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,iPhone 6 and
cs-410_5_8_40,"00:02:52,470","00:02:56,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Or perhaps we're interested in knowing
cs-410_5_8_41,"00:02:56,810","00:02:58,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,presidential election?
cs-410_5_8_42,"00:02:59,780","00:03:04,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,And all these have to do with discovering
cs-410_5_8_43,"00:03:04,800","00:03:08,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,and we're going to talk about a lot
cs-410_5_8_44,"00:03:08,680","00:03:12,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,In general we can view a topic as
cs-410_5_8_45,"00:03:12,920","00:03:17,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,So from text data we expect to
cs-410_5_8_46,"00:03:17,830","00:03:22,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,then these topics generally provide
cs-410_5_8_47,"00:03:22,650","00:03:25,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,And it tells us something about the world.
cs-410_5_8_48,"00:03:25,690","00:03:28,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,"About a product, about a person etc."
cs-410_5_8_49,"00:03:29,350","00:03:32,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Now when we have some non-text data,"
cs-410_5_8_50,"00:03:32,390","00:03:36,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,then we can have more context for
cs-410_5_8_51,"00:03:36,420","00:03:41,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"For example, we might know the time"
cs-410_5_8_52,"00:03:41,620","00:03:47,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,locations where the text
cs-410_5_8_53,"00:03:47,110","00:03:52,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"or the authors of the text, or"
cs-410_5_8_54,"00:03:52,945","00:03:54,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"All such meta data, or"
cs-410_5_8_55,"00:03:54,400","00:03:59,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,context variables can be associated
cs-410_5_8_56,"00:03:59,610","00:04:05,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,then we can use these context variables
cs-410_5_8_57,"00:04:05,340","00:04:09,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"For example, looking at topics over time,"
cs-410_5_8_58,"00:04:09,320","00:04:14,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"whether there's a trending topic, or"
cs-410_5_8_59,"00:04:15,620","00:04:18,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,Soon you are looking at topics
cs-410_5_8_60,"00:04:18,950","00:04:24,185",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,We might know some insights about
cs-410_5_8_61,"00:04:26,150","00:04:29,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,So that's why mining
cs-410_5_8_62,"00:04:29,900","00:04:34,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"Now, let's look at the tasks"
cs-410_5_8_63,"00:04:34,540","00:04:39,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"In general, it would involve first"
cs-410_5_8_64,"00:04:39,380","00:04:40,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,k topics.
cs-410_5_8_65,"00:04:40,810","00:04:45,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"And then we also would like to know, which"
cs-410_5_8_66,"00:04:45,430","00:04:46,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,to what extent.
cs-410_5_8_67,"00:04:46,600","00:04:52,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"So for example, in document one, we"
cs-410_5_8_68,"00:04:52,970","00:04:57,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,Topic 2 and
cs-410_5_8_69,"00:04:58,890","00:05:00,712",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"And other topics,"
cs-410_5_8_70,"00:05:00,712","00:05:06,778",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"Document two, on the other hand,"
cs-410_5_8_71,"00:05:06,778","00:05:10,553",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"but it did not cover Topic 1 at all, and"
cs-410_5_8_72,"00:05:10,553","00:05:15,873",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"it also covers Topic k to some extent,"
cs-410_5_8_73,"00:05:15,873","00:05:19,995",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,So now you can see there
cs-410_5_8_74,"00:05:19,995","00:05:25,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"sub-tasks, the first is to discover k"
cs-410_5_8_75,"00:05:25,760","00:05:27,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,What are these k topics?
cs-410_5_8_76,"00:05:27,140","00:05:28,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Okay, major topics in the text they are."
cs-410_5_8_77,"00:05:28,920","00:05:33,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,The second task is to figure out
cs-410_5_8_78,"00:05:33,180","00:05:34,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,to what extent.
cs-410_5_8_79,"00:05:34,430","00:05:37,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"So more formally,"
cs-410_5_8_80,"00:05:37,810","00:05:42,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"First, we have, as input,"
cs-410_5_8_81,"00:05:42,365","00:05:47,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Here we can denote the text
cs-410_5_8_82,"00:05:47,050","00:05:51,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,denote text article as d i.
cs-410_5_8_83,"00:05:51,740","00:05:56,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"And, we generally also need to have"
cs-410_5_8_84,"00:05:56,700","00:06:01,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,But there may be techniques that can
cs-410_5_8_85,"00:06:01,730","00:06:06,735",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,But in the techniques that we will
cs-410_5_8_86,"00:06:06,735","00:06:12,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"techniques, we often need to"
cs-410_5_8_87,"00:06:14,580","00:06:19,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,Now the output would then be the k
cs-410_5_8_88,"00:06:19,860","00:06:23,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,in order as theta sub
cs-410_5_8_89,"00:06:24,540","00:06:29,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,Also we want to generate the coverage of
cs-410_5_8_90,"00:06:29,820","00:06:32,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,this is denoted by pi sub i j.
cs-410_5_8_91,"00:06:33,562","00:06:38,073",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,And pi sub ij is the probability
cs-410_5_8_92,"00:06:38,073","00:06:41,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,covering topic theta sub j.
cs-410_5_8_93,"00:06:41,290","00:06:45,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,"So obviously for each document, we have"
cs-410_5_8_94,"00:06:45,450","00:06:47,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"what extent the document covers,"
cs-410_5_8_95,"00:06:48,930","00:06:53,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And we can assume that these
cs-410_5_8_96,"00:06:53,610","00:06:57,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,Because a document won't be able to cover
cs-410_5_8_97,"00:06:57,000","00:07:02,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,other topics outside of the topics
cs-410_5_8_98,"00:07:02,520","00:07:08,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"So now, the question is, how do we define"
cs-410_5_8_99,"00:07:08,170","00:07:11,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,Now this problem has not
cs-410_5_8_100,"00:07:11,500","00:07:15,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,until we define what is exactly theta.
cs-410_5_8_101,"00:07:16,970","00:07:19,381",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"So in the next few lectures,"
cs-410_5_8_102,"00:07:19,381","00:07:24,211",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,we're going to talk about
cs-410_5_8_103,"00:07:24,211","00:07:34,211",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,[MUSIC]
cs-410_2_8_1,"00:00:00,025","00:00:05,819",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is
cs-410_2_8_2,"00:00:05,819","00:00:12,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,relation discovery and
cs-410_2_8_3,"00:00:12,090","00:00:12,963",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture,"
cs-410_2_8_4,"00:00:12,963","00:00:16,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,we're going to continue the discussion
cs-410_2_8_5,"00:00:18,060","00:00:22,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,We're going to talk about the conditional
cs-410_2_8_6,"00:00:22,930","00:00:25,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,discovering syntagmatic relations.
cs-410_2_8_7,"00:00:25,700","00:00:29,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"Earlier, we talked about"
cs-410_2_8_8,"00:00:29,400","00:00:33,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,how easy it is to predict the presence or
cs-410_2_8_9,"00:00:34,180","00:00:37,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,"Now, we'll address"
cs-410_2_8_10,"00:00:37,700","00:00:41,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,we assume that we know something
cs-410_2_8_11,"00:00:41,320","00:00:48,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"So now the question is, suppose we know"
cs-410_2_8_12,"00:00:48,830","00:00:51,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,How would that help us
cs-410_2_8_13,"00:00:51,150","00:00:53,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"absence of water, like in meat?"
cs-410_2_8_14,"00:00:53,990","00:00:58,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"And in particular, we want to"
cs-410_2_8_15,"00:00:58,060","00:01:00,959",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,has helped us predict
cs-410_2_8_16,"00:01:02,020","00:01:05,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"And if we frame this using entrophy,"
cs-410_2_8_17,"00:01:05,040","00:01:10,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,that would mean we are interested
cs-410_2_8_18,"00:01:10,700","00:01:15,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,the presence of eats could reduce
cs-410_2_8_19,"00:01:15,100","00:01:18,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Or, reduce the entrophy"
cs-410_2_8_20,"00:01:18,800","00:01:23,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,corresponding to the presence or
cs-410_2_8_21,"00:01:23,430","00:01:27,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"We can also ask as a question,"
cs-410_2_8_22,"00:01:28,950","00:01:33,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Would that also help us predict
cs-410_2_8_23,"00:01:34,720","00:01:39,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,These questions can be
cs-410_2_8_24,"00:01:39,415","00:01:43,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,concept called a conditioning entropy.
cs-410_2_8_25,"00:01:43,120","00:01:48,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"So to explain this concept, let's first"
cs-410_2_8_26,"00:01:48,460","00:01:51,218",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,when we know nothing about the segment.
cs-410_2_8_27,"00:01:51,218","00:01:56,522",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,So we have these probabilities indicating
cs-410_2_8_28,"00:01:56,522","00:01:58,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,or it doesn't occur in the segment.
cs-410_2_8_29,"00:01:58,830","00:02:02,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,And we have an entropy function that
cs-410_2_8_30,"00:02:03,810","00:02:07,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"Now suppose we know eats is present, so"
cs-410_2_8_31,"00:02:07,410","00:02:11,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,now we know the value of another
cs-410_2_8_32,"00:02:12,730","00:02:15,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"Now, that would change all"
cs-410_2_8_33,"00:02:15,270","00:02:17,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,conditional probabilities.
cs-410_2_8_34,"00:02:17,550","00:02:20,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,Where we look at the presence or
cs-410_2_8_35,"00:02:21,800","00:02:25,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,given that we know eats
cs-410_2_8_36,"00:02:25,570","00:02:27,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"So as a result,"
cs-410_2_8_37,"00:02:27,480","00:02:31,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,if we replace these probabilities
cs-410_2_8_38,"00:02:31,820","00:02:36,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"probabilities in the entropy function,"
cs-410_2_8_39,"00:02:37,650","00:02:42,522",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,So this equation now here would be
cs-410_2_8_40,"00:02:42,522","00:02:46,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,the conditional entropy.
cs-410_2_8_41,"00:02:46,900","00:02:49,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,Conditional on the presence of eats.
cs-410_2_8_42,"00:02:52,180","00:02:57,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"So, you can see this is essentially"
cs-410_2_8_43,"00:02:57,070","00:03:01,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"seen before, except that all"
cs-410_2_8_44,"00:03:04,420","00:03:09,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And this then tells us
cs-410_2_8_45,"00:03:09,550","00:03:13,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,after we have known eats
cs-410_2_8_46,"00:03:14,380","00:03:17,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,"And of course, we can also define"
cs-410_2_8_47,"00:03:17,770","00:03:20,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,the scenario where we don't see eats.
cs-410_2_8_48,"00:03:20,540","00:03:25,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,So if we know it did not occur in
cs-410_2_8_49,"00:03:25,150","00:03:30,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,entropy would capture the instances
cs-410_2_8_50,"00:03:30,710","00:03:34,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"So now,"
cs-410_2_8_51,"00:03:34,110","00:03:37,609",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,we have the completed definition
cs-410_2_8_52,"00:03:39,250","00:03:48,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"Basically, we're going to consider both"
cs-410_2_8_53,"00:03:48,520","00:03:54,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,and this gives us a probability
cs-410_2_8_54,"00:03:54,280","00:03:58,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"Basically, whether eats is present or"
cs-410_2_8_55,"00:03:58,040","00:03:59,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,"And this of course,"
cs-410_2_8_56,"00:03:59,150","00:04:04,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,is the conditional entropy of
cs-410_2_8_57,"00:04:05,510","00:04:10,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"So if you expanded this entropy,"
cs-410_2_8_58,"00:04:10,110","00:04:14,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,then you have the following equation.
cs-410_2_8_59,"00:04:15,760","00:04:19,429",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,Where you see the involvement of
cs-410_2_8_60,"00:04:21,530","00:04:26,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"Now in general, for any discrete"
cs-410_2_8_61,"00:04:27,940","00:04:35,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,the conditional entropy is no larger
cs-410_2_8_62,"00:04:35,240","00:04:41,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"So basically, this is upper bound for"
cs-410_2_8_63,"00:04:41,950","00:04:46,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,That means by knowing more
cs-410_2_8_64,"00:04:46,380","00:04:49,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,we want to be able to
cs-410_2_8_65,"00:04:49,630","00:04:51,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,We can only reduce uncertainty.
cs-410_2_8_66,"00:04:51,570","00:04:56,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,And that intuitively makes sense
cs-410_2_8_67,"00:04:56,180","00:05:00,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,it should always help
cs-410_2_8_68,"00:05:00,180","00:05:04,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,And cannot hurt
cs-410_2_8_69,"00:05:05,420","00:05:08,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"Now, what's interesting here is also to"
cs-410_2_8_70,"00:05:08,880","00:05:11,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,value of this conditional entropy?
cs-410_2_8_71,"00:05:11,770","00:05:16,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"Now, we know that the maximum"
cs-410_2_8_72,"00:05:17,940","00:05:20,313",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,"But what about the minimum,"
cs-410_2_8_73,"00:05:22,883","00:05:28,552",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,I hope you can reach the conclusion that
cs-410_2_8_74,"00:05:28,552","00:05:33,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,And it will be interesting to think about
cs-410_2_8_75,"00:05:34,120","00:05:37,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"So, let's see how we can use conditional"
cs-410_2_8_76,"00:05:39,420","00:05:44,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"Now of course,"
cs-410_2_8_77,"00:05:44,250","00:05:48,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,one way to measure
cs-410_2_8_78,"00:05:48,300","00:05:53,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Because it tells us to what extent,"
cs-410_2_8_79,"00:05:53,750","00:05:58,995",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,word given that we know the presence or
cs-410_2_8_80,"00:05:58,995","00:06:03,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,Now before we look at the intuition
cs-410_2_8_81,"00:06:03,900","00:06:09,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"syntagmatic relations, it's useful to"
cs-410_2_8_82,"00:06:09,090","00:06:17,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"That is, the conditional entropy"
cs-410_2_8_83,"00:06:19,000","00:06:22,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"So here,"
cs-410_2_8_84,"00:06:22,980","00:06:28,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,we listed this conditional
cs-410_2_8_85,"00:06:28,420","00:06:31,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,"So, it's here."
cs-410_2_8_86,"00:06:33,550","00:06:35,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"So, what is the value of this?"
cs-410_2_8_87,"00:06:36,380","00:06:43,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Now, this means we know where"
cs-410_2_8_88,"00:06:43,370","00:06:47,717",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,And we hope to predict whether
cs-410_2_8_89,"00:06:47,717","00:06:52,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,"And of course, this is 0 because"
cs-410_2_8_90,"00:06:52,518","00:06:55,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,Once we know whether the word
cs-410_2_8_91,"00:06:55,862","00:06:59,132",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,we'll already know the answer
cs-410_2_8_92,"00:06:59,132","00:07:00,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,So this is zero.
cs-410_2_8_93,"00:07:00,410","00:07:03,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And that's also when this conditional
cs-410_2_8_94,"00:07:06,280","00:07:08,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"So now, let's look at some other cases."
cs-410_2_8_95,"00:07:09,530","00:07:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,So this is a case of knowing the and
cs-410_2_8_96,"00:07:15,840","00:07:20,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,And this is a case of knowing eats and
cs-410_2_8_97,"00:07:20,840","00:07:22,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,Which one do you think is smaller?
cs-410_2_8_98,"00:07:22,870","00:07:27,763",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,No doubt smaller entropy means easier for
cs-410_2_8_99,"00:07:31,511","00:07:33,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,Which one do you think is higher?
cs-410_2_8_100,"00:07:33,260","00:07:34,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,Which one is not smaller?
cs-410_2_8_101,"00:07:36,800","00:07:41,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"Well, if you at the uncertainty,"
cs-410_2_8_102,"00:07:41,732","00:07:45,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,the doesn't really tell
cs-410_2_8_103,"00:07:45,730","00:07:51,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,So knowing the occurrence of the doesn't
cs-410_2_8_104,"00:07:51,520","00:07:56,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,So it stays fairly close to
cs-410_2_8_105,"00:07:56,465","00:08:01,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,"Whereas in the case of eats,"
cs-410_2_8_106,"00:08:01,120","00:08:04,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So knowing presence of eats or
cs-410_2_8_107,"00:08:04,420","00:08:07,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,would help us predict whether meat occurs.
cs-410_2_8_108,"00:08:07,780","00:08:14,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So it can help us reduce entropy of meat.
cs-410_2_8_109,"00:08:14,290","00:08:20,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,"So we should expect the sigma term, namely"
cs-410_2_8_110,"00:08:21,630","00:08:25,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,And that means there is a stronger
cs-410_2_8_111,"00:08:29,070","00:08:36,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,So we now also know when
cs-410_2_8_112,"00:08:36,360","00:08:41,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"meat, then the conditional entropy"
cs-410_2_8_113,"00:08:41,400","00:08:45,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,And for what kind of words
cs-410_2_8_114,"00:08:45,300","00:08:49,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"Well, that's when this stuff"
cs-410_2_8_115,"00:08:49,885","00:08:55,339",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,"And like the for example,"
cs-410_2_8_116,"00:08:55,339","00:08:58,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,which is the entropy of meat itself.
cs-410_2_8_117,"00:08:59,970","00:09:03,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,So this suggests that when you
cs-410_2_8_118,"00:09:03,050","00:09:07,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"mining syntagmatic relations,"
cs-410_2_8_119,"00:09:10,140","00:09:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"For each word W1, we're going to"
cs-410_2_8_120,"00:09:14,780","00:09:21,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"And then, we can compute"
cs-410_2_8_121,"00:09:22,170","00:09:26,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,We thought all the candidate was in
cs-410_2_8_122,"00:09:26,630","00:09:30,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,"because we're out of favor,"
cs-410_2_8_123,"00:09:30,090","00:09:34,637",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,Meaning that it helps us predict
cs-410_2_8_124,"00:09:34,637","00:09:38,378",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"And then, we're going to take the top ring"
cs-410_2_8_125,"00:09:38,378","00:09:40,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,potential syntagmatic relations with W1.
cs-410_2_8_126,"00:09:41,910","00:09:47,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,Note that we need to use
cs-410_2_8_127,"00:09:47,700","00:09:51,474",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,The stresser can be the number
cs-410_2_8_128,"00:09:51,474","00:09:54,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,absolute value for
cs-410_2_8_129,"00:09:55,900","00:10:00,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,"Now, this would allow us to mine the most"
cs-410_2_8_130,"00:10:00,110","00:10:03,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,strongly correlated words with
cs-410_2_8_131,"00:10:06,380","00:10:10,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"But, this algorithm does not"
cs-410_2_8_132,"00:10:10,560","00:10:14,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,that K syntagmatical relations
cs-410_2_8_133,"00:10:14,800","00:10:19,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"Because in order to do that, we have to"
cs-410_2_8_134,"00:10:19,370","00:10:24,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,are comparable across different words.
cs-410_2_8_135,"00:10:24,010","00:10:28,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,In this case of discovering
cs-410_2_8_136,"00:10:28,470","00:10:33,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"a targeted word like W1, we only need"
cs-410_2_8_137,"00:10:34,980","00:10:38,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"for W1, given different words."
cs-410_2_8_138,"00:10:38,600","00:10:40,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"And in this case, they are comparable."
cs-410_2_8_139,"00:10:41,860","00:10:43,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,All right.
cs-410_2_8_140,"00:10:43,690","00:10:48,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,"So, the conditional entropy of W1, given"
cs-410_2_8_141,"00:10:48,040","00:10:49,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,given W3 are comparable.
cs-410_2_8_142,"00:10:51,100","00:10:55,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,They all measure how hard
cs-410_2_8_143,"00:10:55,490","00:11:00,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,"But, if we think about the two pairs,"
cs-410_2_8_144,"00:11:00,070","00:11:06,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"where we share W2 in the same condition,"
cs-410_2_8_145,"00:11:06,370","00:11:11,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,"Then, the conditional entropies"
cs-410_2_8_146,"00:11:11,296","00:11:15,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,You can think of about this question.
cs-410_2_8_147,"00:11:15,925","00:11:17,022",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Why?
cs-410_2_8_148,"00:11:17,022","00:11:19,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,So why are they not comfortable?
cs-410_2_8_149,"00:11:19,870","00:11:23,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"Well, that was because they"
cs-410_2_8_150,"00:11:23,210","00:11:25,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,Right?
cs-410_2_8_151,"00:11:25,690","00:11:29,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,the entropy of W1 and the entropy of W3.
cs-410_2_8_152,"00:11:29,230","00:11:31,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,And they have different upper bounds.
cs-410_2_8_153,"00:11:31,150","00:11:35,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,So we cannot really
cs-410_2_8_154,"00:11:35,000","00:11:36,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,So how do we address this problem?
cs-410_2_8_155,"00:11:38,000","00:11:45,219",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,"Well later, we'll discuss, we can use"
cs-410_2_8_156,"00:11:45,219","00:11:55,219",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,[MUSIC]
cs-410_6_8_1,"00:00:00,000","00:00:02,974",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_6_8_2,"00:00:07,749","00:00:11,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about topic mining and
cs-410_6_8_3,"00:00:12,760","00:00:17,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,We're going to talk about
cs-410_6_8_4,"00:00:17,130","00:00:20,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,This is a slide that you have
cs-410_6_8_5,"00:00:20,700","00:00:25,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,where we define the task of
cs-410_6_8_6,"00:00:25,120","00:00:30,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"We also raised the question, how do"
cs-410_6_8_7,"00:00:31,780","00:00:36,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"So in this lecture, we're going to"
cs-410_6_8_8,"00:00:36,020","00:00:37,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,that's our initial idea.
cs-410_6_8_9,"00:00:37,780","00:00:40,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,Our idea here is defining
cs-410_6_8_10,"00:00:42,020","00:00:44,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,A term can be a word or a phrase.
cs-410_6_8_11,"00:00:45,240","00:00:49,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,"And in general,"
cs-410_6_8_12,"00:00:49,500","00:00:54,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So our first thought is just
cs-410_6_8_13,"00:00:54,200","00:00:58,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"For example, we might have terms"
cs-410_6_8_14,"00:00:58,820","00:00:59,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,as you see here.
cs-410_6_8_15,"00:00:59,440","00:01:02,603",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,"Now if we define a topic in this way,"
cs-410_6_8_16,"00:01:02,603","00:01:09,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,we can then analyze the coverage
cs-410_6_8_17,"00:01:09,200","00:01:10,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,"Here for example,"
cs-410_6_8_18,"00:01:10,510","00:01:15,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,we might want to discover to what
cs-410_6_8_19,"00:01:15,560","00:01:21,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,And we found that 30% of the content
cs-410_6_8_20,"00:01:21,260","00:01:23,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"And 12% is about the travel, etc."
cs-410_6_8_21,"00:01:23,730","00:01:28,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,We might also discover document
cs-410_6_8_22,"00:01:28,880","00:01:31,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"So the coverage is zero, etc."
cs-410_6_8_23,"00:01:32,630","00:01:39,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"So now, of course,"
cs-410_6_8_24,"00:01:39,040","00:01:42,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"topic mining and analysis,"
cs-410_6_8_25,"00:01:42,900","00:01:44,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,One is to discover the topics.
cs-410_6_8_26,"00:01:44,960","00:01:48,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,And the second is to analyze coverage.
cs-410_6_8_27,"00:01:48,110","00:01:51,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,So let's first think
cs-410_6_8_28,"00:01:51,550","00:01:55,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,topics if we represent
cs-410_6_8_29,"00:01:55,080","00:01:59,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So that means we need to mine k
cs-410_6_8_30,"00:02:01,050","00:02:04,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"Now there are, of course,"
cs-410_6_8_31,"00:02:05,670","00:02:08,617",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,And we're going to talk about
cs-410_6_8_32,"00:02:08,617","00:02:10,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,which is also likely effective.
cs-410_6_8_33,"00:02:10,750","00:02:11,641",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"So first of all,"
cs-410_6_8_34,"00:02:11,641","00:02:16,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,we're going to parse the text data in
cs-410_6_8_35,"00:02:16,655","00:02:20,665",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,Here candidate terms can be words or
cs-410_6_8_36,"00:02:20,665","00:02:25,475",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,Let's say the simplest solution is
cs-410_6_8_37,"00:02:25,475","00:02:29,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,These words then become candidate topics.
cs-410_6_8_38,"00:02:29,145","00:02:32,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,Then we're going to design a scoring
cs-410_6_8_39,"00:02:32,790","00:02:33,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,is as a topic.
cs-410_6_8_40,"00:02:35,460","00:02:37,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So how can we design such a function?
cs-410_6_8_41,"00:02:37,150","00:02:40,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,Well there are many things
cs-410_6_8_42,"00:02:40,140","00:02:44,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"For example, we can use pure statistics"
cs-410_6_8_43,"00:02:45,550","00:02:48,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,"Intuitively, we would like to"
cs-410_6_8_44,"00:02:48,820","00:02:53,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,meaning terms that can represent
cs-410_6_8_45,"00:02:53,950","00:02:58,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,So that would mean we want
cs-410_6_8_46,"00:02:58,610","00:03:03,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"However, if we simply use the frequency"
cs-410_6_8_47,"00:03:03,990","00:03:07,982",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,then the highest scored terms
cs-410_6_8_48,"00:03:07,982","00:03:10,876",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"functional terms like the, etc."
cs-410_6_8_49,"00:03:10,876","00:03:13,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,Those terms occur very frequently English.
cs-410_6_8_50,"00:03:14,650","00:03:19,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,So we also want to avoid having
cs-410_6_8_51,"00:03:19,340","00:03:22,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,we want to penalize such words.
cs-410_6_8_52,"00:03:22,150","00:03:26,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"But in general, we would like to favor"
cs-410_6_8_53,"00:03:26,480","00:03:28,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,not so frequent.
cs-410_6_8_54,"00:03:28,020","00:03:34,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,So a particular approach could be based
cs-410_6_8_55,"00:03:35,140","00:03:37,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,And TF stands for term frequency.
cs-410_6_8_56,"00:03:37,230","00:03:40,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,IDF stands for inverse document frequency.
cs-410_6_8_57,"00:03:40,420","00:03:43,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,We talked about some of these
cs-410_6_8_58,"00:03:43,310","00:03:48,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,ideas in the lectures about
cs-410_6_8_59,"00:03:48,090","00:03:50,766",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"So these are statistical methods,"
cs-410_6_8_60,"00:03:50,766","00:03:56,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,meaning that the function is
cs-410_6_8_61,"00:03:56,280","00:03:59,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,So the scoring function
cs-410_6_8_62,"00:03:59,080","00:04:02,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"It can be applied to any language,"
cs-410_6_8_63,"00:04:02,890","00:04:06,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,But when we apply such a approach
cs-410_6_8_64,"00:04:06,650","00:04:12,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,we might also be able to leverage
cs-410_6_8_65,"00:04:12,020","00:04:16,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"For example, in news we might favor"
cs-410_6_8_66,"00:04:16,815","00:04:21,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,We might want to favor title
cs-410_6_8_67,"00:04:21,340","00:04:26,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,use the title to describe
cs-410_6_8_68,"00:04:27,750","00:04:32,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"If we're dealing with tweets,"
cs-410_6_8_69,"00:04:32,480","00:04:37,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,which are invented to denote topics.
cs-410_6_8_70,"00:04:37,430","00:04:43,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"So naturally, hashtags can be good"
cs-410_6_8_71,"00:04:44,780","00:04:50,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,"Anyway, after we have this design"
cs-410_6_8_72,"00:04:50,430","00:04:55,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,the k topical terms by simply picking
cs-410_6_8_73,"00:04:55,960","00:04:57,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"Now, of course,"
cs-410_6_8_74,"00:04:57,240","00:05:02,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we might encounter situation where the
cs-410_6_8_75,"00:05:02,040","00:05:06,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"They're semantically similar, or"
cs-410_6_8_76,"00:05:06,910","00:05:08,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,So that's not desirable.
cs-410_6_8_77,"00:05:08,860","00:05:12,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,So we also want to have coverage over
cs-410_6_8_78,"00:05:12,280","00:05:15,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So we would like to remove redundancy.
cs-410_6_8_79,"00:05:15,080","00:05:19,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,And one way to do that is
cs-410_6_8_80,"00:05:19,600","00:05:24,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,which is sometimes called a maximal
cs-410_6_8_81,"00:05:24,450","00:05:29,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"Basically, the idea is to go down"
cs-410_6_8_82,"00:05:29,330","00:05:34,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,function and gradually take terms
cs-410_6_8_83,"00:05:34,380","00:05:36,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"The first term, of course, will be picked."
cs-410_6_8_84,"00:05:36,840","00:05:40,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"When we pick the next term, we're"
cs-410_6_8_85,"00:05:40,500","00:05:45,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,been picked and try to avoid
cs-410_6_8_86,"00:05:45,120","00:05:50,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,So while we are considering
cs-410_6_8_87,"00:05:50,610","00:05:54,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,we are also considering
cs-410_6_8_88,"00:05:54,260","00:05:56,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,with respect to the terms
cs-410_6_8_89,"00:05:58,090","00:06:02,933",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"And with some thresholding,"
cs-410_6_8_90,"00:06:02,933","00:06:08,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,the redundancy removal and
cs-410_6_8_91,"00:06:08,330","00:06:11,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"Okay, so"
cs-410_6_8_92,"00:06:11,990","00:06:17,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,And those can be regarded as the topics
cs-410_6_8_93,"00:06:17,550","00:06:21,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"Next, let's think about how we're going"
cs-410_6_8_94,"00:06:23,430","00:06:26,971",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,"So looking at this picture,"
cs-410_6_8_95,"00:06:26,971","00:06:28,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,these topics.
cs-410_6_8_96,"00:06:28,130","00:06:31,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,And now suppose you are give a document.
cs-410_6_8_97,"00:06:31,190","00:06:35,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,How should we pick out coverage
cs-410_6_8_98,"00:06:36,660","00:06:42,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Well, one approach can be to simply"
cs-410_6_8_99,"00:06:42,690","00:06:46,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"So for example, sports might have occurred"
cs-410_6_8_100,"00:06:46,770","00:06:49,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,"travel occurred twice, etc."
cs-410_6_8_101,"00:06:49,570","00:06:54,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,And then we can just normalize these
cs-410_6_8_102,"00:06:54,420","00:06:56,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,probability for each topic.
cs-410_6_8_103,"00:06:56,570","00:07:01,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"So in general, the formula would"
cs-410_6_8_104,"00:07:01,780","00:07:05,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,all the terms that represent the topics.
cs-410_6_8_105,"00:07:05,240","00:07:10,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,And then simply normalize them so
cs-410_6_8_106,"00:07:10,220","00:07:13,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,topic in the document would add to one.
cs-410_6_8_107,"00:07:15,120","00:07:21,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,This forms a distribution of the topics
cs-410_6_8_108,"00:07:21,200","00:07:26,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,of different topics in the document.
cs-410_6_8_109,"00:07:26,560","00:07:30,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"Now, as always,"
cs-410_6_8_110,"00:07:30,100","00:07:34,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,"solving problem, we have to ask"
cs-410_6_8_111,"00:07:34,940","00:07:37,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,Or is this the best way
cs-410_6_8_112,"00:07:38,690","00:07:41,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,So now let's examine this approach.
cs-410_6_8_113,"00:07:41,110","00:07:44,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"In general,"
cs-410_6_8_114,"00:07:46,010","00:07:50,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,by using actual data sets and
cs-410_6_8_115,"00:07:52,360","00:07:57,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"Well, in this case let's take"
cs-410_6_8_116,"00:07:57,340","00:08:03,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,And we have a text document that's
cs-410_6_8_117,"00:08:04,800","00:08:07,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"So in terms of the content,"
cs-410_6_8_118,"00:08:08,950","00:08:14,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,But if we simply count these
cs-410_6_8_119,"00:08:14,600","00:08:19,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,we will find that the word sports
cs-410_6_8_120,"00:08:19,070","00:08:21,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,even though the content
cs-410_6_8_121,"00:08:22,520","00:08:25,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So the count of sports is zero.
cs-410_6_8_122,"00:08:25,750","00:08:31,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,That means the coverage of sports
cs-410_6_8_123,"00:08:31,939","00:08:36,723",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"Now of course,"
cs-410_6_8_124,"00:08:36,723","00:08:40,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,the document and
cs-410_6_8_125,"00:08:40,980","00:08:42,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,And that's okay.
cs-410_6_8_126,"00:08:42,230","00:08:47,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,But sports certainly is not okay because
cs-410_6_8_127,"00:08:47,257","00:08:49,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So this estimate has problem.
cs-410_6_8_128,"00:08:50,880","00:08:56,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"What's worse, the term travel"
cs-410_6_8_129,"00:08:56,050","00:08:59,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,So when we estimate the coverage
cs-410_6_8_130,"00:08:59,940","00:09:02,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,we have got a non-zero count.
cs-410_6_8_131,"00:09:02,140","00:09:05,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,So its estimated coverage
cs-410_6_8_132,"00:09:05,000","00:09:07,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,So this obviously is also not desirable.
cs-410_6_8_133,"00:09:08,800","00:09:13,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,So this simple example illustrates
cs-410_6_8_134,"00:09:13,910","00:09:17,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,"First, when we count what"
cs-410_6_8_135,"00:09:17,704","00:09:20,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,we also need to consider related words.
cs-410_6_8_136,"00:09:20,460","00:09:24,285",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,We can't simply just count
cs-410_6_8_137,"00:09:24,285","00:09:26,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"In this case, it did not occur at all."
cs-410_6_8_138,"00:09:26,440","00:09:31,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,But there are many related words
cs-410_6_8_139,"00:09:31,340","00:09:33,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,So we need to count
cs-410_6_8_140,"00:09:33,860","00:09:38,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,The second problem is that a word
cs-410_6_8_141,"00:09:38,910","00:09:42,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,So here it probably means
cs-410_6_8_142,"00:09:42,900","00:09:47,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,we can imagine it might also
cs-410_6_8_143,"00:09:47,600","00:09:53,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"So in that case, the star might actually"
cs-410_6_8_144,"00:09:54,210","00:09:56,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,So we need to deal with that as well.
cs-410_6_8_145,"00:09:56,360","00:10:02,325",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"Finally, a main restriction of this"
cs-410_6_8_146,"00:10:02,325","00:10:08,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,"term to describe the topic, so it cannot"
cs-410_6_8_147,"00:10:08,520","00:10:12,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"For example, a very specialized"
cs-410_6_8_148,"00:10:12,040","00:10:15,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,describe by using just a word or
cs-410_6_8_149,"00:10:15,210","00:10:17,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,We need to use more words.
cs-410_6_8_150,"00:10:17,150","00:10:20,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,So this example illustrates
cs-410_6_8_151,"00:10:20,760","00:10:23,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,this approach of treating a term as topic.
cs-410_6_8_152,"00:10:23,310","00:10:26,725",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"First, it lacks expressive power."
cs-410_6_8_153,"00:10:26,725","00:10:30,729",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,Meaning that it can only represent
cs-410_6_8_154,"00:10:30,729","00:10:36,035",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,it cannot represent the complicated topics
cs-410_6_8_155,"00:10:37,055","00:10:40,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,"Second, it's incomplete"
cs-410_6_8_156,"00:10:40,660","00:10:44,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,meaning that the topic itself
cs-410_6_8_157,"00:10:44,930","00:10:48,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,It does not suggest what other
cs-410_6_8_158,"00:10:48,820","00:10:52,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"Even if we're talking about sports,"
cs-410_6_8_159,"00:10:52,370","00:10:57,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,So it does not allow us to easily
cs-410_6_8_160,"00:10:57,060","00:10:59,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,conversion to coverage of this topic.
cs-410_6_8_161,"00:10:59,200","00:11:02,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"Finally, there is this problem"
cs-410_6_8_162,"00:11:02,410","00:11:05,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,A topical term or
cs-410_6_8_163,"00:11:05,862","00:11:08,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,"For example,"
cs-410_6_8_164,"00:11:10,570","00:11:14,125",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,"So in the next lecture,"
cs-410_6_8_165,"00:11:14,125","00:11:18,806",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,about how to solve
cs-410_6_8_166,"00:11:18,806","00:11:28,806",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,[MUSIC]
cs-410_3_8_1,"00:00:00,025","00:00:07,457",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_3_8_2,"00:00:07,457","00:00:11,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the syntagmatic
cs-410_3_8_3,"00:00:13,400","00:00:18,196",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,In this lecture we are going to continue
cs-410_3_8_4,"00:00:18,196","00:00:20,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In particular,"
cs-410_3_8_5,"00:00:20,850","00:00:24,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,"the concept in the information series,"
cs-410_3_8_6,"00:00:24,880","00:00:28,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,how it can be used to discover
cs-410_3_8_7,"00:00:28,760","00:00:32,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,Before we talked about the problem
cs-410_3_8_8,"00:00:32,880","00:00:38,014",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,that is the conditional entropy
cs-410_3_8_9,"00:00:38,014","00:00:42,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"It is not really comparable, so"
cs-410_3_8_10,"00:00:42,600","00:00:48,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,strong synagmatic relations
cs-410_3_8_11,"00:00:48,360","00:00:53,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,So now we are going to introduce mutual
cs-410_3_8_12,"00:00:53,050","00:00:57,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,in the information series
cs-410_3_8_13,"00:00:57,370","00:01:03,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,normalize the conditional entropy to make
cs-410_3_8_14,"00:01:04,930","00:01:10,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"In particular, mutual information"
cs-410_3_8_15,"00:01:10,090","00:01:17,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,matches the entropy reduction
cs-410_3_8_16,"00:01:17,380","00:01:22,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,More specifically the question we
cs-410_3_8_17,"00:01:22,270","00:01:25,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,of an entropy of X can
cs-410_3_8_18,"00:01:27,220","00:01:31,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So mathematically it can be
cs-410_3_8_19,"00:01:31,940","00:01:36,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"the original entropy of X, and"
cs-410_3_8_20,"00:01:37,970","00:01:42,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"And you might see,"
cs-410_3_8_21,"00:01:42,730","00:01:47,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,as reduction of entropy of
cs-410_3_8_22,"00:01:48,930","00:01:54,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,Now normally the two conditional
cs-410_3_8_23,"00:01:54,070","00:01:58,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"the entropy of Y given X are not equal,"
cs-410_3_8_24,"00:01:58,240","00:02:05,476",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,the reduction of entropy by knowing
cs-410_3_8_25,"00:02:05,476","00:02:12,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So, this quantity is called a Mutual"
cs-410_3_8_26,"00:02:12,805","00:02:17,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,And this function has some interesting
cs-410_3_8_27,"00:02:17,085","00:02:21,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,This is easy to understand because
cs-410_3_8_28,"00:02:22,782","00:02:29,132",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,not going to be lower than the possibility
cs-410_3_8_29,"00:02:29,132","00:02:33,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"In other words, the conditional entropy"
cs-410_3_8_30,"00:02:33,512","00:02:37,784",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,Knowing some information can
cs-410_3_8_31,"00:02:37,784","00:02:40,282",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,will not hurt us in predicting x.
cs-410_3_8_32,"00:02:41,510","00:02:46,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,The signal property is that it
cs-410_3_8_33,"00:02:46,375","00:02:51,142",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"entropy is not symmetrical,"
cs-410_3_8_34,"00:02:51,142","00:02:56,394",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,the third property is that It
cs-410_3_8_35,"00:02:56,394","00:03:01,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,only if the two random variables
cs-410_3_8_36,"00:03:01,580","00:03:07,949",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,That means knowing one of them does not
cs-410_3_8_37,"00:03:07,949","00:03:14,626",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,this last property can be verified by
cs-410_3_8_38,"00:03:14,626","00:03:19,144",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,it reaches 0 if and
cs-410_3_8_39,"00:03:19,144","00:03:24,102",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,[INAUDIBLE] Y is exactly the same
cs-410_3_8_40,"00:03:24,102","00:03:28,344",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,So that means knowing why it did not
cs-410_3_8_41,"00:03:28,344","00:03:30,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,a Y are completely independent.
cs-410_3_8_42,"00:03:32,120","00:03:37,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,Now when we fix X to rank different
cs-410_3_8_43,"00:03:37,880","00:03:44,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,would give the same order as
cs-410_3_8_44,"00:03:44,180","00:03:49,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"because in the function here,"
cs-410_3_8_45,"00:03:49,940","00:03:53,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,So ranking based on mutual entropy is
cs-410_3_8_46,"00:03:53,820","00:03:57,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"the conditional entropy of X given Y, but"
cs-410_3_8_47,"00:03:57,600","00:04:03,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,the mutual information allows us to
cs-410_3_8_48,"00:04:03,058","00:04:07,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"So, that is why mutual information is"
cs-410_3_8_49,"00:04:10,688","00:04:14,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"So, let us examine the intuition"
cs-410_3_8_50,"00:04:14,420","00:04:15,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,Syntagmatical Relation Mining.
cs-410_3_8_51,"00:04:17,150","00:04:20,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,"Now, the question we ask forcing"
cs-410_3_8_52,"00:04:20,430","00:04:24,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"whenever ""eats"" occurs,"
cs-410_3_8_53,"00:04:25,610","00:04:30,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,So this question can be framed as
cs-410_3_8_54,"00:04:30,710","00:04:33,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,which words have high mutual
cs-410_3_8_55,"00:04:33,055","00:04:37,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,so computer the missing information
cs-410_3_8_56,"00:04:39,050","00:04:44,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"And if we do that, and it is basically"
cs-410_3_8_57,"00:04:44,520","00:04:48,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,we will see that words that
cs-410_3_8_58,"00:04:48,990","00:04:50,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,will have a high point.
cs-410_3_8_59,"00:04:50,960","00:04:55,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,Whereas words that are not related
cs-410_3_8_60,"00:04:55,200","00:04:58,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"For this, I will give some example here."
cs-410_3_8_61,"00:04:58,530","00:05:01,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"The mutual information between ""eats"" and"
cs-410_3_8_62,"00:05:01,220","00:05:05,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"which is the same as between ""meats"" and"
cs-410_3_8_63,"00:05:05,650","00:05:10,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,symmetrical is expected to be higher than
cs-410_3_8_64,"00:05:10,960","00:05:14,638",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"the, because knowing the does not"
cs-410_3_8_65,"00:05:14,638","00:05:17,998",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"It is similar, and"
cs-410_3_8_66,"00:05:17,998","00:05:22,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,the as well.
cs-410_3_8_67,"00:05:22,280","00:05:26,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,And you also can easily
cs-410_3_8_68,"00:05:26,970","00:05:32,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,information between a word and
cs-410_3_8_69,"00:05:32,030","00:05:37,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,which is equal to
cs-410_3_8_70,"00:05:37,890","00:05:42,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"so, because in this case the reduction is"
cs-410_3_8_71,"00:05:42,740","00:05:48,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,maximum because knowing one allows
cs-410_3_8_72,"00:05:48,530","00:05:50,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"So the conditional entropy is zero,"
cs-410_3_8_73,"00:05:50,570","00:05:54,472",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,therefore the mutual information
cs-410_3_8_74,"00:05:54,472","00:06:02,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"It is going to be larger, then are equal"
cs-410_3_8_75,"00:06:02,520","00:06:05,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,In other words picking any other word and
cs-410_3_8_76,"00:06:05,420","00:06:08,588",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,the computer picking between eats and
cs-410_3_8_77,"00:06:08,588","00:06:13,511",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,You will not get any information larger
cs-410_3_8_78,"00:06:16,386","00:06:21,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,So now let us look at how to
cs-410_3_8_79,"00:06:21,390","00:06:23,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"Now in order to do that, we often"
cs-410_3_8_80,"00:06:25,110","00:06:29,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,use a different form of mutual
cs-410_3_8_81,"00:06:29,100","00:06:34,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,rewrite the mutual information
cs-410_3_8_82,"00:06:34,190","00:06:38,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,Where we essentially see
cs-410_3_8_83,"00:06:38,655","00:06:43,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,called a KL-divergence or divergence.
cs-410_3_8_84,"00:06:43,075","00:06:45,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,This is another term
cs-410_3_8_85,"00:06:45,615","00:06:48,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,It measures the divergence
cs-410_3_8_86,"00:06:50,615","00:06:54,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"Now, if you look at the formula,"
cs-410_3_8_87,"00:06:54,645","00:06:58,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,different values of the two random
cs-410_3_8_88,"00:06:58,190","00:07:04,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,mainly we are doing a comparison
cs-410_3_8_89,"00:07:04,110","00:07:06,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"The numerator has the joint,"
cs-410_3_8_90,"00:07:06,690","00:07:11,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,actual observed the joint distribution
cs-410_3_8_91,"00:07:12,690","00:07:15,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,The bottom part or the denominator can be
cs-410_3_8_92,"00:07:15,720","00:07:20,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,interpreted as the expected joint
cs-410_3_8_93,"00:07:20,695","00:07:26,782",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,if they were independent because when
cs-410_3_8_94,"00:07:26,782","00:07:32,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,they are joined distribution is equal to
cs-410_3_8_95,"00:07:35,300","00:07:39,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So this comparison will tell us whether
cs-410_3_8_96,"00:07:39,800","00:07:43,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,If they are indeed independent then we
cs-410_3_8_97,"00:07:44,390","00:07:49,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,but if the numerator is different
cs-410_3_8_98,"00:07:49,470","00:07:54,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,the two variables are not independent and
cs-410_3_8_99,"00:07:56,120","00:08:00,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,The sum is simply to take into
cs-410_3_8_100,"00:08:00,110","00:08:04,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,of the values of these
cs-410_3_8_101,"00:08:04,180","00:08:08,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"In our case, each random variable"
cs-410_3_8_102,"00:08:08,750","00:08:13,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"zero or one, so"
cs-410_3_8_103,"00:08:13,950","00:08:17,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,If we look at this form of mutual
cs-410_3_8_104,"00:08:17,330","00:08:21,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,information matches the divergence
cs-410_3_8_105,"00:08:21,230","00:08:25,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,from the expected distribution
cs-410_3_8_106,"00:08:25,800","00:08:30,144",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,"The larger this divergence is, the higher"
cs-410_3_8_107,"00:08:33,507","00:08:37,091",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,So now let us further look at what
cs-410_3_8_108,"00:08:37,091","00:08:39,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,involved in this formula
cs-410_3_8_109,"00:08:41,300","00:08:45,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,"And here, this is all the probabilities"
cs-410_3_8_110,"00:08:45,080","00:08:46,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,you to verify that.
cs-410_3_8_111,"00:08:46,500","00:08:51,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"Basically, we have first to"
cs-410_3_8_112,"00:08:51,610","00:08:56,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,corresponding to the presence or
cs-410_3_8_113,"00:08:56,380","00:08:59,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"So, for w1,"
cs-410_3_8_114,"00:09:02,600","00:09:07,995",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"They should sum to one, because a word"
cs-410_3_8_115,"00:09:07,995","00:09:13,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"In the segment, and similarly for"
cs-410_3_8_116,"00:09:13,260","00:09:18,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,"the second word, we also have two"
cs-410_3_8_117,"00:09:18,230","00:09:20,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"absences of this word, and"
cs-410_3_8_118,"00:09:21,920","00:09:26,162",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"And finally, we have a lot of"
cs-410_3_8_119,"00:09:26,162","00:09:31,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,the scenarios of co-occurrences of
cs-410_3_8_120,"00:09:34,513","00:09:39,107",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,And they sum to one because the two
cs-410_3_8_121,"00:09:39,107","00:09:41,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,possible scenarios.
cs-410_3_8_122,"00:09:41,420","00:09:43,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"Either they both occur, so"
cs-410_3_8_123,"00:09:43,730","00:09:49,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,in that case both variables will have
cs-410_3_8_124,"00:09:49,500","00:09:50,579",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,There are two scenarios.
cs-410_3_8_125,"00:09:51,660","00:09:55,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,In these two cases one of the random
cs-410_3_8_126,"00:09:55,910","00:10:03,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,the other will be zero and finally we have
cs-410_3_8_127,"00:10:03,560","00:10:06,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,This is when the two variables
cs-410_3_8_128,"00:10:07,620","00:10:12,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,So these are the probabilities involved
cs-410_3_8_129,"00:10:12,855","00:10:13,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,over here.
cs-410_3_8_130,"00:10:16,007","00:10:18,416",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,Once we know how to calculate
cs-410_3_8_131,"00:10:18,416","00:10:20,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,we can easily calculate
cs-410_3_8_132,"00:10:24,063","00:10:28,231",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,It is also interesting to know that
cs-410_3_8_133,"00:10:28,231","00:10:32,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"constraint among these probabilities,"
cs-410_3_8_134,"00:10:32,960","00:10:36,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,"So in the previous slide,"
cs-410_3_8_135,"00:10:36,400","00:10:41,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,that you have seen that
cs-410_3_8_136,"00:10:41,830","00:10:46,114",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,words sum to one and
cs-410_3_8_137,"00:10:46,114","00:10:53,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,that says the two words have these
cs-410_3_8_138,"00:10:53,190","00:10:57,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,but we also have some additional
cs-410_3_8_139,"00:10:58,600","00:11:03,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"For example, this one means if we add up"
cs-410_3_8_140,"00:11:03,670","00:11:07,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,the probabilities that we observe
cs-410_3_8_141,"00:11:07,890","00:11:12,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,the probabilities when the first word
cs-410_3_8_142,"00:11:12,500","00:11:16,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,We get exactly the probability
cs-410_3_8_143,"00:11:16,860","00:11:20,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"In other words, when the word is observed."
cs-410_3_8_144,"00:11:20,040","00:11:22,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,"When the first word is observed, and"
cs-410_3_8_145,"00:11:22,210","00:11:27,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,"there are only two scenarios, depending on"
cs-410_3_8_146,"00:11:27,640","00:11:31,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,"So, this probability captures the first"
cs-410_3_8_147,"00:11:31,750","00:11:33,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"actually is also observed, and"
cs-410_3_8_148,"00:11:33,860","00:11:38,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,this captures the second scenario
cs-410_3_8_149,"00:11:38,130","00:11:40,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,"So, we only see the first word, and"
cs-410_3_8_150,"00:11:40,145","00:11:45,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,it is easy to see the other equations
cs-410_3_8_151,"00:11:46,980","00:11:50,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,Now these equations allow us to
cs-410_3_8_152,"00:11:50,980","00:11:54,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,"other probabilities, and"
cs-410_3_8_153,"00:11:55,750","00:12:01,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,"So more specifically,"
cs-410_3_8_154,"00:12:01,010","00:12:06,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,"a word is present, like in this case,"
cs-410_3_8_155,"00:12:06,490","00:12:12,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,if we know the probability of
cs-410_3_8_156,"00:12:12,630","00:12:17,002",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,then we can easily compute
cs-410_3_8_157,"00:12:17,002","00:12:22,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,It is very easy to use this
cs-410_3_8_158,"00:12:22,770","00:12:27,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=742,we take care of the computation of
cs-410_3_8_159,"00:12:27,820","00:12:29,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,absence of each word.
cs-410_3_8_160,"00:12:29,950","00:12:33,146",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,Now let's look at
cs-410_3_8_161,"00:12:33,146","00:12:36,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Let us assume that we also have available
cs-410_3_8_162,"00:12:36,460","00:12:39,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,the probability that
cs-410_3_8_163,"00:12:39,548","00:12:44,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,Now it is easy to see that we can
cs-410_3_8_164,"00:12:44,220","00:12:45,829",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,probabilities based on these.
cs-410_3_8_165,"00:12:46,870","00:12:51,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,Specifically for
cs-410_3_8_166,"00:12:51,170","00:12:56,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,the probability that the first word
cs-410_3_8_167,"00:12:56,260","00:13:02,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,because we know these probabilities in
cs-410_3_8_168,"00:13:02,020","00:13:05,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=782,equation we can compute the probability
cs-410_3_8_169,"00:13:05,364","00:13:06,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,Word.
cs-410_3_8_170,"00:13:06,000","00:13:10,421",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"And then finally,"
cs-410_3_8_171,"00:13:10,421","00:13:14,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,by using this equation because
cs-410_3_8_172,"00:13:14,745","00:13:19,282",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"this is also known, and"
cs-410_3_8_173,"00:13:19,282","00:13:23,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,So this can be easier to calculate.
cs-410_3_8_174,"00:13:23,120","00:13:24,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,So now this can be calculated.
cs-410_3_8_175,"00:13:26,080","00:13:30,989",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,So this slide shows that we only
cs-410_3_8_176,"00:13:30,989","00:13:35,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,these three probabilities
cs-410_3_8_177,"00:13:35,800","00:13:43,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,naming the presence of each word and the
cs-410_3_8_178,"00:13:43,092","00:13:53,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,[MUSIC]
cs-410_1_8_1,"00:00:00,250","00:00:06,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_1_8_2,"00:00:06,380","00:00:13,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about the syntagmatic
cs-410_1_8_3,"00:00:13,220","00:00:17,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we're going to continue"
cs-410_1_8_4,"00:00:17,760","00:00:22,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"In particular, we're going to talk about"
cs-410_1_8_5,"00:00:22,420","00:00:25,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,And we're going to start with
cs-410_1_8_6,"00:00:25,770","00:00:29,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,which is the basis for designing some
cs-410_1_8_7,"00:00:32,480","00:00:33,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"By definition,"
cs-410_1_8_8,"00:00:33,110","00:00:39,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,syntagmatic relations hold between words
cs-410_1_8_9,"00:00:39,890","00:00:44,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"That means,"
cs-410_1_8_10,"00:00:44,190","00:00:47,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,we tend to see the occurrence
cs-410_1_8_11,"00:00:48,370","00:00:53,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"So, take a more specific example, here."
cs-410_1_8_12,"00:00:53,560","00:00:55,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"We can ask the question,"
cs-410_1_8_13,"00:00:55,470","00:00:59,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"whenever eats occurs,"
cs-410_1_8_14,"00:01:01,140","00:01:06,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Looking at the sentences on the left,"
cs-410_1_8_15,"00:01:06,000","00:01:11,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"together with eats, like cat,"
cs-410_1_8_16,"00:01:11,030","00:01:15,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,But if I take them out and
cs-410_1_8_17,"00:01:15,870","00:01:21,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"only show eats and some other words,"
cs-410_1_8_18,"00:01:21,550","00:01:27,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Can you predict what other words
cs-410_1_8_19,"00:01:28,315","00:01:31,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Right so
cs-410_1_8_20,"00:01:31,040","00:01:33,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,other words are associated with eats.
cs-410_1_8_21,"00:01:33,630","00:01:37,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"If they are associated with eats,"
cs-410_1_8_22,"00:01:38,625","00:01:43,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,More specifically our
cs-410_1_8_23,"00:01:43,060","00:01:47,072",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"any text segment which can be a sentence,"
cs-410_1_8_24,"00:01:47,072","00:01:51,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And then ask I the question,"
cs-410_1_8_25,"00:01:51,340","00:01:52,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,absent in this segment?
cs-410_1_8_26,"00:01:54,550","00:01:57,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,Right here we ask about the word W.
cs-410_1_8_27,"00:01:57,400","00:02:00,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,Is W present or absent in this segment?
cs-410_1_8_28,"00:02:02,400","00:02:05,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,Now what's interesting is that
cs-410_1_8_29,"00:02:05,100","00:02:08,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,some words are actually easier
cs-410_1_8_30,"00:02:10,150","00:02:14,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,If you take a look at the three
cs-410_1_8_31,"00:02:14,570","00:02:17,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"unicorn, which one do you"
cs-410_1_8_32,"00:02:20,630","00:02:23,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,Now if you think about it for
cs-410_1_8_33,"00:02:24,530","00:02:27,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,the is easier to predict because
cs-410_1_8_34,"00:02:27,910","00:02:30,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"So I can just say,"
cs-410_1_8_35,"00:02:31,940","00:02:37,946",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,Unicorn is also relatively easy
cs-410_1_8_36,"00:02:37,946","00:02:41,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,And I can bet that it doesn't
cs-410_1_8_37,"00:02:42,780","00:02:46,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,But meat is somewhere in
cs-410_1_8_38,"00:02:46,080","00:02:50,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And it makes it harder to predict because
cs-410_1_8_39,"00:02:50,580","00:02:52,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,"or the segment, more accurately."
cs-410_1_8_40,"00:02:53,842","00:02:58,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"But it may also not occur in the sentence,"
cs-410_1_8_41,"00:02:58,820","00:03:01,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,now let's study this
cs-410_1_8_42,"00:03:02,680","00:03:06,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,So the problem can be formally defined
cs-410_1_8_43,"00:03:06,090","00:03:10,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,as predicting the value of
cs-410_1_8_44,"00:03:10,030","00:03:14,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"Here we denote it by X sub w,"
cs-410_1_8_45,"00:03:14,080","00:03:17,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,this random variable is associated
cs-410_1_8_46,"00:03:18,380","00:03:23,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"When the value of the variable is 1,"
cs-410_1_8_47,"00:03:23,020","00:03:26,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"When it's 0, it means the word is absent."
cs-410_1_8_48,"00:03:26,110","00:03:31,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"And naturally, the probabilities for"
cs-410_1_8_49,"00:03:31,010","00:03:34,187",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,because a word is either present or
cs-410_1_8_50,"00:03:35,240","00:03:36,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,There's no other choice.
cs-410_1_8_51,"00:03:38,290","00:03:43,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,So the intuition with this concept earlier
cs-410_1_8_52,"00:03:43,610","00:03:48,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,"The more random this random variable is,"
cs-410_1_8_53,"00:03:49,710","00:03:53,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,Now the question is how does one
cs-410_1_8_54,"00:03:53,600","00:03:55,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,a random variable like X sub w?
cs-410_1_8_55,"00:03:56,940","00:04:01,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"How in general, can we quantify"
cs-410_1_8_56,"00:04:01,850","00:04:04,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,that's why we need a measure
cs-410_1_8_57,"00:04:04,690","00:04:10,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,this measure introduced in information
cs-410_1_8_58,"00:04:10,560","00:04:13,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,There is also some connection
cs-410_1_8_59,"00:04:13,790","00:04:15,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,that is beyond the scope of this course.
cs-410_1_8_60,"00:04:17,460","00:04:20,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,So for
cs-410_1_8_61,"00:04:20,750","00:04:22,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,as a function defined
cs-410_1_8_62,"00:04:22,910","00:04:27,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"In this case, it is a binary random"
cs-410_1_8_63,"00:04:27,000","00:04:30,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,be easily generalized for
cs-410_1_8_64,"00:04:32,070","00:04:34,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,"Now the function form looks like this,"
cs-410_1_8_65,"00:04:34,950","00:04:39,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,there's the sum of all the possible
cs-410_1_8_66,"00:04:39,410","00:04:44,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,Inside the sum for each value we
cs-410_1_8_67,"00:04:45,210","00:04:52,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,that the random variable equals this
cs-410_1_8_68,"00:04:53,380","00:04:55,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,And note that there is also
cs-410_1_8_69,"00:04:56,270","00:04:59,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,Now entropy in general is non-negative.
cs-410_1_8_70,"00:04:59,900","00:05:01,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,And that can be mathematically proved.
cs-410_1_8_71,"00:05:02,620","00:05:10,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"So if we expand this sum, we'll see that"
cs-410_1_8_72,"00:05:10,320","00:05:14,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,Where I explicitly plugged
cs-410_1_8_73,"00:05:14,130","00:05:18,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"And sometimes when we have 0 log of 0,"
cs-410_1_8_74,"00:05:18,370","00:05:25,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"we would generally define that as 0,"
cs-410_1_8_75,"00:05:28,480","00:05:30,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,So this is the entropy function.
cs-410_1_8_76,"00:05:30,330","00:05:33,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,And this function will
cs-410_1_8_77,"00:05:33,020","00:05:35,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,different distributions
cs-410_1_8_78,"00:05:37,260","00:05:40,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,And it clearly depends on the probability
cs-410_1_8_79,"00:05:40,650","00:05:43,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,that the random variable
cs-410_1_8_80,"00:05:43,850","00:05:49,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,If we plot this function against
cs-410_1_8_81,"00:05:49,780","00:05:55,114",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,the probability that the random
cs-410_1_8_82,"00:05:56,990","00:05:59,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,And then the function looks like this.
cs-410_1_8_83,"00:06:01,310","00:06:06,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"At the two ends,"
cs-410_1_8_84,"00:06:07,950","00:06:13,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"equals 1 is very small or very large,"
cs-410_1_8_85,"00:06:13,698","00:06:18,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,When it's 0.5 in the middle
cs-410_1_8_86,"00:06:20,180","00:06:24,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,Now if we plot the function
cs-410_1_8_87,"00:06:25,950","00:06:31,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,is taking a value of 0 and the function
cs-410_1_8_88,"00:06:31,090","00:06:37,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"would show exactly the same curve here,"
cs-410_1_8_89,"00:06:37,810","00:06:40,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,And so that's because
cs-410_1_8_90,"00:06:42,340","00:06:46,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"the two probabilities are symmetric,"
cs-410_1_8_91,"00:06:48,740","00:06:52,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,So an interesting question you
cs-410_1_8_92,"00:06:52,850","00:06:59,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,what kind of X does entropy
cs-410_1_8_93,"00:06:59,390","00:07:02,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,And we can in particular think
cs-410_1_8_94,"00:07:02,960","00:07:07,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"For example, in one case,"
cs-410_1_8_95,"00:07:08,840","00:07:10,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,always takes a value of 1.
cs-410_1_8_96,"00:07:10,600","00:07:14,304",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,The probability is 1.
cs-410_1_8_97,"00:07:16,390","00:07:18,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,Or there's a random variable that
cs-410_1_8_98,"00:07:19,890","00:07:24,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,is equally likely taking a value of one or
cs-410_1_8_99,"00:07:24,320","00:07:28,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,So in this case the probability
cs-410_1_8_100,"00:07:30,700","00:07:32,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,Now which one has a higher entropy?
cs-410_1_8_101,"00:07:34,650","00:07:38,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,It's easier to look at the problem
cs-410_1_8_102,"00:07:40,800","00:07:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,using coin tossing.
cs-410_1_8_103,"00:07:43,420","00:07:47,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,So when we think about random
cs-410_1_8_104,"00:07:48,770","00:07:55,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"it gives us a random variable,"
cs-410_1_8_105,"00:07:55,740","00:07:57,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,It can be head or tail.
cs-410_1_8_106,"00:07:57,860","00:08:03,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So we can define a random variable
cs-410_1_8_107,"00:08:03,040","00:08:08,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,"when the coin shows up as head,"
cs-410_1_8_108,"00:08:09,800","00:08:15,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,So now we can compute the entropy
cs-410_1_8_109,"00:08:15,390","00:08:20,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,And this entropy indicates how
cs-410_1_8_110,"00:08:22,050","00:08:22,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,of a coin toss.
cs-410_1_8_111,"00:08:25,440","00:08:27,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,So we can think about the two cases.
cs-410_1_8_112,"00:08:27,530","00:08:29,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"One is a fair coin, it's completely fair."
cs-410_1_8_113,"00:08:29,590","00:08:34,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,The coin shows up as head or
cs-410_1_8_114,"00:08:34,160","00:08:39,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,So the two probabilities would be a half.
cs-410_1_8_115,"00:08:39,160","00:08:42,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,Right?
cs-410_1_8_116,"00:08:44,680","00:08:47,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,Another extreme case is
cs-410_1_8_117,"00:08:47,620","00:08:50,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,where the coin always shows up as heads.
cs-410_1_8_118,"00:08:50,420","00:08:52,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,So it's a completely biased coin.
cs-410_1_8_119,"00:08:54,670","00:08:57,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,Now let's think about
cs-410_1_8_120,"00:08:57,910","00:09:04,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,And if you plug in these values you can
cs-410_1_8_121,"00:09:04,850","00:09:09,524",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,For a fair coin we see the entropy
cs-410_1_8_122,"00:09:11,270","00:09:14,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"For the completely biased coin,"
cs-410_1_8_123,"00:09:14,460","00:09:17,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,And that intuitively makes a lot of sense.
cs-410_1_8_124,"00:09:17,360","00:09:20,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,Because a fair coin is
cs-410_1_8_125,"00:09:22,080","00:09:24,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,Whereas a completely biased
cs-410_1_8_126,"00:09:24,950","00:09:26,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"We can always say, well, it's a head."
cs-410_1_8_127,"00:09:26,860","00:09:29,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,Because it is a head all the time.
cs-410_1_8_128,"00:09:29,190","00:09:34,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,So they can be shown on
cs-410_1_8_129,"00:09:34,400","00:09:40,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,So the fair coin corresponds to the middle
cs-410_1_8_130,"00:09:40,300","00:09:45,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,The completely biased coin
cs-410_1_8_131,"00:09:45,410","00:09:48,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,point where we have a probability
cs-410_1_8_132,"00:09:48,058","00:09:54,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,"So, now let's see how we can use"
cs-410_1_8_133,"00:09:54,870","00:09:59,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,Let's think about our problem is
cs-410_1_8_134,"00:09:59,670","00:10:01,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,absent in this segment.
cs-410_1_8_135,"00:10:01,650","00:10:05,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"Again, think about the three words,"
cs-410_1_8_136,"00:10:06,540","00:10:10,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,Now we can assume high entropy
cs-410_1_8_137,"00:10:11,910","00:10:18,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,And so we now have a quantitative way to
cs-410_1_8_138,"00:10:20,890","00:10:25,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,"Now if you look at the three words meat,"
cs-410_1_8_139,"00:10:25,810","00:10:33,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,we clearly would expect meat to have
cs-410_1_8_140,"00:10:33,310","00:10:39,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,"In fact if you look at the entropy of the,"
cs-410_1_8_141,"00:10:39,180","00:10:41,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,Because it occurs everywhere.
cs-410_1_8_142,"00:10:41,570","00:10:43,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,So it's like a completely biased coin.
cs-410_1_8_143,"00:10:44,610","00:10:46,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,Therefore the entropy is zero.
cs-410_1_8_144,"00:10:48,710","00:10:58,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,[MUSIC]
cs-410_4_8_1,"00:00:00,000","00:00:04,714",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_8_2,"00:00:06,455","00:00:09,677",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,"In general, we can use the empirical count"
cs-410_4_8_3,"00:00:09,677","00:00:15,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,of events in the observed data
cs-410_4_8_4,"00:00:15,340","00:00:19,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,And a commonly used technique is
cs-410_4_8_5,"00:00:19,080","00:00:22,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,where we simply normalize
cs-410_4_8_6,"00:00:22,600","00:00:30,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,"So if we do that, we can see, we can"
cs-410_4_8_7,"00:00:30,330","00:00:36,811",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,For estimating the probability that
cs-410_4_8_8,"00:00:36,811","00:00:42,773",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,we simply normalize the count of
cs-410_4_8_9,"00:00:42,773","00:00:47,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,So let's first take
cs-410_4_8_10,"00:00:47,278","00:00:52,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"On the right side, you see a list of some,"
cs-410_4_8_11,"00:00:52,970","00:00:55,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,These are segments.
cs-410_4_8_12,"00:00:55,010","00:00:59,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,And in some segments you see both words
cs-410_4_8_13,"00:00:59,860","00:01:01,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,both columns.
cs-410_4_8_14,"00:01:01,630","00:01:05,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"In some other cases only one will occur,"
cs-410_4_8_15,"00:01:05,830","00:01:07,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,the other column has zero.
cs-410_4_8_16,"00:01:07,590","00:01:11,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"And in all, of course, in some other"
cs-410_4_8_17,"00:01:11,130","00:01:13,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,so they are both zeros.
cs-410_4_8_18,"00:01:13,930","00:01:19,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"And for estimating these probabilities, we"
cs-410_4_8_19,"00:01:20,340","00:01:23,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,"So the three counts are first,"
cs-410_4_8_20,"00:01:23,560","00:01:27,337",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And that's the total number of
cs-410_4_8_21,"00:01:27,337","00:01:30,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,It's just as the ones in the column of W1.
cs-410_4_8_22,"00:01:30,950","00:01:34,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,We can count how many
cs-410_4_8_23,"00:01:34,470","00:01:40,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"The segment count is for word 2, and we"
cs-410_4_8_24,"00:01:40,460","00:01:45,425",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,And these will give us the total
cs-410_4_8_25,"00:01:45,425","00:01:49,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,The third count is when both words occur.
cs-410_4_8_26,"00:01:49,650","00:01:55,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"So this time, we're going to count"
cs-410_4_8_27,"00:01:56,580","00:02:00,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"And then, so this would give us"
cs-410_4_8_28,"00:02:00,060","00:02:03,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,where we have seen both W1 and W2.
cs-410_4_8_29,"00:02:03,510","00:02:08,112",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"Once we have these counts,"
cs-410_4_8_30,"00:02:08,112","00:02:11,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"which is the total number of segments, and"
cs-410_4_8_31,"00:02:11,019","00:02:16,706",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,this will give us the probabilities that
cs-410_4_8_32,"00:02:16,706","00:02:22,301",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"Now, there is a small problem,"
cs-410_4_8_33,"00:02:22,301","00:02:27,458",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"And in this case, we don't want a zero"
cs-410_4_8_34,"00:02:27,458","00:02:33,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"a small sample and in general, we would"
cs-410_4_8_35,"00:02:33,365","00:02:35,806",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,a [INAUDIBLE] to avoid any context.
cs-410_4_8_36,"00:02:35,806","00:02:39,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"So, to address this problem,"
cs-410_4_8_37,"00:02:39,630","00:02:43,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,And that's basically to add some
cs-410_4_8_38,"00:02:43,780","00:02:48,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,and so that we don't get
cs-410_4_8_39,"00:02:48,410","00:02:54,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"Now, the best way to understand smoothing"
cs-410_4_8_40,"00:02:54,250","00:03:00,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"data than we actually have, because we'll"
cs-410_4_8_41,"00:03:00,310","00:03:04,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"I illustrated on the top,"
cs-410_4_8_42,"00:03:04,650","00:03:10,095",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And these pseudo-segments would
cs-410_4_8_43,"00:03:10,095","00:03:15,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,of these words so
cs-410_4_8_44,"00:03:15,047","00:03:18,169",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"Now, in particular we introduce"
cs-410_4_8_45,"00:03:18,169","00:03:20,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,Each is weighted at one quarter.
cs-410_4_8_46,"00:03:20,990","00:03:25,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,And these represent the four different
cs-410_4_8_47,"00:03:25,930","00:03:30,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,"So now each event,"
cs-410_4_8_48,"00:03:30,490","00:03:35,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,at least one count or at least a non-zero
cs-410_4_8_49,"00:03:35,390","00:03:39,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"So, in the actual segments"
cs-410_4_8_50,"00:03:39,380","00:03:44,231",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,it's okay if we haven't observed
cs-410_4_8_51,"00:03:44,231","00:03:49,671",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"So more specifically, you can see"
cs-410_4_8_52,"00:03:49,671","00:03:55,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"ones in the two pseudo-segments,"
cs-410_4_8_53,"00:03:55,560","00:03:59,315",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"We add them up, we get 0.5."
cs-410_4_8_54,"00:03:59,315","00:04:03,319",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"And similar to this,"
cs-410_4_8_55,"00:04:03,319","00:04:08,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,pseudo-segment that indicates
cs-410_4_8_56,"00:04:09,450","00:04:14,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,And of course in the denominator we add
cs-410_4_8_57,"00:04:14,000","00:04:17,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"we add, in this case,"
cs-410_4_8_58,"00:04:17,520","00:04:21,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,Each is weighed at one quarter so
cs-410_4_8_59,"00:04:21,780","00:04:24,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"So, that's why in the denominator"
cs-410_4_8_60,"00:04:25,990","00:04:31,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,"So, this basically concludes"
cs-410_4_8_61,"00:04:31,460","00:04:33,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,four syntagmatic relation discoveries.
cs-410_4_8_62,"00:04:36,090","00:04:42,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,"Now, so to summarize,"
cs-410_4_8_63,"00:04:42,050","00:04:46,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,be discovered by measuring correlations
cs-410_4_8_64,"00:04:46,240","00:04:49,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,We've introduced the three
cs-410_4_8_65,"00:04:49,580","00:04:53,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"Entropy, which measures the uncertainty"
cs-410_4_8_66,"00:04:53,230","00:04:59,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"Conditional entropy, which measures"
cs-410_4_8_67,"00:04:59,060","00:05:04,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"And mutual information of X and Y,"
cs-410_4_8_68,"00:05:04,530","00:05:11,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"due to knowing Y, or"
cs-410_4_8_69,"00:05:11,240","00:05:12,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,They are the same.
cs-410_4_8_70,"00:05:12,660","00:05:17,111",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So these three concepts are actually very
cs-410_4_8_71,"00:05:17,111","00:05:20,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,That's why we spent some time
cs-410_4_8_72,"00:05:20,340","00:05:23,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"But in particular,"
cs-410_4_8_73,"00:05:23,150","00:05:25,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,discovering syntagmatic relations.
cs-410_4_8_74,"00:05:25,960","00:05:30,142",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"In particular,"
cs-410_4_8_75,"00:05:30,142","00:05:32,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,discovering such a relation.
cs-410_4_8_76,"00:05:32,370","00:05:37,241",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,It allows us to have values
cs-410_4_8_77,"00:05:37,241","00:05:42,211",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,words that are comparable and
cs-410_4_8_78,"00:05:42,211","00:05:48,208",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,discover the strongest syntagmatic
cs-410_4_8_79,"00:05:48,208","00:05:53,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Now, note that there is some relation"
cs-410_4_8_80,"00:05:53,700","00:05:55,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,[INAUDIBLE] relation discovery.
cs-410_4_8_81,"00:05:55,910","00:06:01,835",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,So we already discussed the possibility
cs-410_4_8_82,"00:06:01,835","00:06:06,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,terms in the context to potentially
cs-410_4_8_83,"00:06:06,683","00:06:11,187",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,that have syntagmatic relations
cs-410_4_8_84,"00:06:11,187","00:06:17,958",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"But here, once we use mutual information"
cs-410_4_8_85,"00:06:17,958","00:06:24,436",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,we can also represent the context with
cs-410_4_8_86,"00:06:24,436","00:06:29,567",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,So this would give us
cs-410_4_8_87,"00:06:29,567","00:06:33,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"the context of a word, like a cat."
cs-410_4_8_88,"00:06:33,490","00:06:37,394",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"And if we do the same for all the words,"
cs-410_4_8_89,"00:06:37,394","00:06:42,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,compare the similarity between these
cs-410_4_8_90,"00:06:42,320","00:06:45,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So this provides yet
cs-410_4_8_91,"00:06:45,850","00:06:48,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,paradigmatic relation discovery.
cs-410_4_8_92,"00:06:48,800","00:06:55,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And so to summarize this whole part
cs-410_4_8_93,"00:06:55,770","00:06:59,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"We introduce two basic associations,"
cs-410_4_8_94,"00:06:59,190","00:07:01,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,a syntagmatic relations.
cs-410_4_8_95,"00:07:01,000","00:07:05,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"These are fairly general, they apply"
cs-410_4_8_96,"00:07:05,710","00:07:10,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"the units don't have to be words,"
cs-410_4_8_97,"00:07:11,120","00:07:16,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,We introduced multiple statistical
cs-410_4_8_98,"00:07:16,235","00:07:20,762",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,mainly showing that pure
cs-410_4_8_99,"00:07:20,762","00:07:24,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,are variable for
cs-410_4_8_100,"00:07:24,840","00:07:28,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,And they can be combined to
cs-410_4_8_101,"00:07:28,800","00:07:35,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,These approaches can be applied
cs-410_4_8_102,"00:07:35,040","00:07:39,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,mostly because they are based
cs-410_4_8_103,"00:07:39,940","00:07:42,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,they can actually discover
cs-410_4_8_104,"00:07:44,360","00:07:47,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,We can also use different ways with
cs-410_4_8_105,"00:07:47,880","00:07:51,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,this would lead us to some interesting
cs-410_4_8_106,"00:07:51,360","00:07:56,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"For example, the context can be very"
cs-410_4_8_107,"00:07:56,190","00:08:00,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,"a sentence, or maybe paragraphs,"
cs-410_4_8_108,"00:08:00,760","00:08:05,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,allows to discover different flavors
cs-410_4_8_109,"00:08:05,330","00:08:09,362",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"And similarly,"
cs-410_4_8_110,"00:08:09,362","00:08:13,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,visual information to discover
cs-410_4_8_111,"00:08:13,380","00:08:19,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"We also have to define the segment, and"
cs-410_4_8_112,"00:08:19,110","00:08:22,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,text window or a longer text article.
cs-410_4_8_113,"00:08:22,560","00:08:26,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And this would give us different
cs-410_4_8_114,"00:08:26,508","00:08:32,677",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,These discovery associations can
cs-410_4_8_115,"00:08:32,677","00:08:37,701",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,in both information retrieval and
cs-410_4_8_116,"00:08:37,701","00:08:44,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,"So here are some recommended readings,"
cs-410_4_8_117,"00:08:44,100","00:08:46,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,The first is a book with
cs-410_4_8_118,"00:08:46,880","00:08:50,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,which is quite relevant to
cs-410_4_8_119,"00:08:50,810","00:08:55,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,The second is an article
cs-410_4_8_120,"00:08:55,120","00:08:58,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,statistical measures to
cs-410_4_8_121,"00:08:58,160","00:09:03,764",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Those are phrases that
cs-410_4_8_122,"00:09:03,764","00:09:07,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"For example,"
cs-410_4_8_123,"00:09:08,610","00:09:11,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,blue chip is not a chip that's blue.
cs-410_4_8_124,"00:09:11,550","00:09:16,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And the paper has a discussion about some
cs-410_4_8_125,"00:09:17,400","00:09:23,227",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,The third one is a new paper on a unified
cs-410_4_8_126,"00:09:23,227","00:09:29,441",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"relations and a syntagmatical relations,"
cs-410_4_8_127,"00:09:29,441","00:09:39,441",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,[SOUND]
cs-410_7_8_1,"00:00:00,025","00:00:05,683",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is a continued
cs-410_7_8_2,"00:00:05,683","00:00:13,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,discussion of probabilistic topic models.
cs-410_7_8_3,"00:00:13,370","00:00:19,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we're going to continue"
cs-410_7_8_4,"00:00:19,990","00:00:24,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We're going to talk about
cs-410_7_8_5,"00:00:24,970","00:00:28,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,are interested in just mining
cs-410_7_8_6,"00:00:30,880","00:00:35,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"So in this simple setup,"
cs-410_7_8_7,"00:00:35,910","00:00:41,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,one document and
cs-410_7_8_8,"00:00:41,060","00:00:44,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,So this is the simplest
cs-410_7_8_9,"00:00:44,810","00:00:49,921",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"The input now no longer has k,"
cs-410_7_8_10,"00:00:49,921","00:00:55,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,know there is only one topic and the
cs-410_7_8_11,"00:00:55,670","00:01:00,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"In the output,"
cs-410_7_8_12,"00:01:00,738","00:01:06,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,we assumed that the document
cs-410_7_8_13,"00:01:06,150","00:01:10,532",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,So the main goal is just to discover
cs-410_7_8_14,"00:01:10,532","00:01:12,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"this single topic, as shown here."
cs-410_7_8_15,"00:01:14,770","00:01:19,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"As always, when we think about using a"
cs-410_7_8_16,"00:01:19,275","00:01:24,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,we start with thinking about what
cs-410_7_8_17,"00:01:24,280","00:01:28,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,from what perspective we're going to
cs-410_7_8_18,"00:01:28,880","00:01:32,268",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,And then we're going to
cs-410_7_8_19,"00:01:32,268","00:01:36,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"the generating of the data,"
cs-410_7_8_20,"00:01:36,520","00:01:41,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,Where our perspective just means we want
cs-410_7_8_21,"00:01:41,310","00:01:45,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,"the data, so that the model will"
cs-410_7_8_22,"00:01:45,700","00:01:48,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,discovering the knowledge that we want.
cs-410_7_8_23,"00:01:48,770","00:01:54,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,And then we'll be thinking
cs-410_7_8_24,"00:01:54,210","00:02:00,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,write down the microfunction to
cs-410_7_8_25,"00:02:00,480","00:02:04,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,a data point will be
cs-410_7_8_26,"00:02:05,900","00:02:10,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,And the likelihood function will have
cs-410_7_8_27,"00:02:10,370","00:02:15,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,And then we argue our interest in
cs-410_7_8_28,"00:02:15,780","00:02:21,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,by maximizing the likelihood which will
cs-410_7_8_29,"00:02:21,680","00:02:26,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,These estimator parameters
cs-410_7_8_30,"00:02:26,710","00:02:31,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"of the mining hours,"
cs-410_7_8_31,"00:02:31,640","00:02:35,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,parameters as the knowledge
cs-410_7_8_32,"00:02:35,320","00:02:39,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So let's look at these steps for
cs-410_7_8_33,"00:02:39,690","00:02:45,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,Later we'll look at this procedure for
cs-410_7_8_34,"00:02:45,970","00:02:50,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,"So our data, in this case is, just"
cs-410_7_8_35,"00:02:50,170","00:02:52,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,Each word here is denoted by x sub i.
cs-410_7_8_36,"00:02:52,520","00:02:56,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Our model is a Unigram language model.
cs-410_7_8_37,"00:02:56,800","00:03:03,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,A word distribution that we hope to
cs-410_7_8_38,"00:03:03,420","00:03:08,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,So we will have as many parameters as many
cs-410_7_8_39,"00:03:09,950","00:03:14,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,And for convenience we're
cs-410_7_8_40,"00:03:14,580","00:03:18,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,denote the probability of word w sub i.
cs-410_7_8_41,"00:03:20,450","00:03:23,384",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,And obviously these theta
cs-410_7_8_42,"00:03:24,480","00:03:27,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,Now what does a likelihood
cs-410_7_8_43,"00:03:27,110","00:03:30,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"Well, this is just the probability"
cs-410_7_8_44,"00:03:30,970","00:03:31,948",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,that given such a model.
cs-410_7_8_45,"00:03:31,948","00:03:36,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,Because we assume the independence in
cs-410_7_8_46,"00:03:36,920","00:03:41,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,the document will be just a product
cs-410_7_8_47,"00:03:42,790","00:03:46,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,And since some word might
cs-410_7_8_48,"00:03:46,900","00:03:51,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,So we can also rewrite this
cs-410_7_8_49,"00:03:52,580","00:03:58,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"So in this line, we have rewritten"
cs-410_7_8_50,"00:03:58,550","00:04:05,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,over all the unique words in
cs-410_7_8_51,"00:04:05,360","00:04:09,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,Now this is different
cs-410_7_8_52,"00:04:09,170","00:04:13,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"Well, the product is over different"
cs-410_7_8_53,"00:04:15,040","00:04:19,694",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"Now when we do this transformation,"
cs-410_7_8_54,"00:04:19,694","00:04:24,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,introduce a counter function here.
cs-410_7_8_55,"00:04:24,120","00:04:29,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,This denotes the count of
cs-410_7_8_56,"00:04:29,395","00:04:33,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,similarly this is the count
cs-410_7_8_57,"00:04:33,390","00:04:37,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,because these words might
cs-410_7_8_58,"00:04:37,890","00:04:40,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,You can also see if a word did
cs-410_7_8_59,"00:04:41,810","00:04:46,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"It will have a zero count, therefore"
cs-410_7_8_60,"00:04:46,790","00:04:50,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,So this is a very useful form of
cs-410_7_8_61,"00:04:50,410","00:04:55,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,writing down the likelihood function
cs-410_7_8_62,"00:04:55,060","00:05:01,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"So I want you to pay attention to this,"
cs-410_7_8_63,"00:05:01,230","00:05:07,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,It's just to change the product over all
cs-410_7_8_64,"00:05:07,120","00:05:12,013",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,"So in the end, of course, we'll use"
cs-410_7_8_65,"00:05:12,013","00:05:14,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,function and it would look like this.
cs-410_7_8_66,"00:05:14,512","00:05:19,468",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"Next, we're going to find"
cs-410_7_8_67,"00:05:19,468","00:05:24,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,of these words that would maximize
cs-410_7_8_68,"00:05:24,530","00:05:30,539",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,So now lets take a look at the maximum
cs-410_7_8_69,"00:05:32,520","00:05:35,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,This line is copied from
cs-410_7_8_70,"00:05:35,870","00:05:37,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,It's just our likelihood function.
cs-410_7_8_71,"00:05:38,590","00:05:43,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,So our goal is to maximize
cs-410_7_8_72,"00:05:43,950","00:05:46,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,We will find it often easy to
cs-410_7_8_73,"00:05:47,310","00:05:51,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,maximize the local likelihood
cs-410_7_8_74,"00:05:51,110","00:05:56,531",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,And this is purely for
cs-410_7_8_75,"00:05:56,531","00:06:03,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,the logarithm transformation our function
cs-410_7_8_76,"00:06:03,698","00:06:10,704",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,And we also have constraints
cs-410_7_8_77,"00:06:10,704","00:06:16,743",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,The sum makes it easier to take
cs-410_7_8_78,"00:06:16,743","00:06:21,022",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,finding the optimal
cs-410_7_8_79,"00:06:21,022","00:06:27,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"So please take a look at this sum again,"
cs-410_7_8_80,"00:06:27,349","00:06:32,434",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,And this is a form of
cs-410_7_8_81,"00:06:32,434","00:06:38,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"see later also,"
cs-410_7_8_82,"00:06:38,430","00:06:42,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,So it's a sum over all
cs-410_7_8_83,"00:06:42,340","00:06:48,105",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,And inside the sum there is
cs-410_7_8_84,"00:06:48,105","00:06:54,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And this is macroed by
cs-410_7_8_85,"00:06:55,990","00:06:57,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,So let's see how we can
cs-410_7_8_86,"00:06:58,920","00:07:04,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,Now at this point the problem is purely a
cs-410_7_8_87,"00:07:04,030","00:07:11,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,to just the find the optimal solution
cs-410_7_8_88,"00:07:11,360","00:07:14,694",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,The objective function is
cs-410_7_8_89,"00:07:14,694","00:07:18,621",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,the constraint is that all these
cs-410_7_8_90,"00:07:18,621","00:07:23,234",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"So, one way to solve the problem is"
cs-410_7_8_91,"00:07:24,520","00:07:29,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,Now this command is beyond
cs-410_7_8_92,"00:07:29,040","00:07:33,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,since Lagrange multiplier is a very
cs-410_7_8_93,"00:07:33,670","00:07:37,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"to just give a brief introduction to this,"
cs-410_7_8_94,"00:07:39,720","00:07:43,857",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,So in this approach we will
cs-410_7_8_95,"00:07:43,857","00:07:49,887",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,And this function will combine
cs-410_7_8_96,"00:07:49,887","00:07:55,392",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,with another term that
cs-410_7_8_97,"00:07:55,392","00:07:59,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"we introduce Lagrange multiplier here,"
cs-410_7_8_98,"00:07:59,980","00:08:04,978",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"lambda, so it's an additional parameter."
cs-410_7_8_99,"00:08:04,978","00:08:10,432",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"Now, the idea of this approach is just to"
cs-410_7_8_100,"00:08:10,432","00:08:14,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"in some sense,"
cs-410_7_8_101,"00:08:14,800","00:08:18,318",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,Now we are just interested in
cs-410_7_8_102,"00:08:19,460","00:08:24,022",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"As you may recall from calculus,"
cs-410_7_8_103,"00:08:24,022","00:08:29,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,would be achieved when
cs-410_7_8_104,"00:08:29,910","00:08:31,673",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,This is a necessary condition.
cs-410_7_8_105,"00:08:31,673","00:08:33,182",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"It's not sufficient, though."
cs-410_7_8_106,"00:08:33,182","00:08:38,205",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,So if we do that you will
cs-410_7_8_107,"00:08:38,205","00:08:42,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,with respect to theta i
cs-410_7_8_108,"00:08:42,785","00:08:50,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,And this part comes from the derivative
cs-410_7_8_109,"00:08:50,815","00:08:55,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,this lambda is simply taken from here.
cs-410_7_8_110,"00:08:55,390","00:09:00,178",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,And when we set it to zero we can
cs-410_7_8_111,"00:09:00,178","00:09:05,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,easily see theta sub i is
cs-410_7_8_112,"00:09:06,820","00:09:09,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,Since we know all the theta
cs-410_7_8_113,"00:09:09,900","00:09:12,423",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"we can plug this into this constraint,"
cs-410_7_8_114,"00:09:12,423","00:09:15,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,And this will allow us to solve for
cs-410_7_8_115,"00:09:16,630","00:09:20,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,And this is just a net
cs-410_7_8_116,"00:09:20,840","00:09:27,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,And this further allows us to then
cs-410_7_8_117,"00:09:27,350","00:09:31,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"eventually, to find the optimal"
cs-410_7_8_118,"00:09:31,380","00:09:37,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,And if you look at this formula it turns
cs-410_7_8_119,"00:09:37,280","00:09:43,089",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,because this is just the normalized
cs-410_7_8_120,"00:09:43,089","00:09:47,751",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,which is also a sum of all
cs-410_7_8_121,"00:09:47,751","00:09:52,157",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"So, after all this mess, after all,"
cs-410_7_8_122,"00:09:52,157","00:09:59,044",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,we have just obtained something
cs-410_7_8_123,"00:09:59,044","00:10:04,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,this will be just our
cs-410_7_8_124,"00:10:04,415","00:10:10,338",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,maximize the data by
cs-410_7_8_125,"00:10:10,338","00:10:16,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,mass as possible to all
cs-410_7_8_126,"00:10:16,419","00:10:21,408",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,And you might also notice that this is
cs-410_7_8_127,"00:10:21,408","00:10:23,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,raised estimator.
cs-410_7_8_128,"00:10:23,450","00:10:29,333",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"In general, the estimator would be to"
cs-410_7_8_129,"00:10:29,333","00:10:35,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,the counts have to be done in a particular
cs-410_7_8_130,"00:10:35,050","00:10:41,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,So this is basically an analytical
cs-410_7_8_131,"00:10:41,730","00:10:46,303",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,"In general though, when the likelihood"
cs-410_7_8_132,"00:10:46,303","00:10:50,919",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,going to be able to solve the optimization
cs-410_7_8_133,"00:10:50,919","00:10:55,134",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,Instead we have to use some
cs-410_7_8_134,"00:10:55,134","00:10:58,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,"we're going to see such cases later, also."
cs-410_7_8_135,"00:10:58,787","00:11:02,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,So if you imagine what would we
cs-410_7_8_136,"00:11:02,385","00:11:07,146",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,likelihood estimator to estimate one
cs-410_7_8_137,"00:11:07,146","00:11:09,903",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,Let's imagine this document
cs-410_7_8_138,"00:11:09,903","00:11:16,277",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,"Now, what you might see is"
cs-410_7_8_139,"00:11:16,277","00:11:20,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"On the top, you will see the high"
cs-410_7_8_140,"00:11:20,555","00:11:23,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,"common words,"
cs-410_7_8_141,"00:11:23,710","00:11:27,742",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,And this will be followed by
cs-410_7_8_142,"00:11:27,742","00:11:31,622",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,"characterize the topic well like text,"
cs-410_7_8_143,"00:11:31,622","00:11:36,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"And then in the end,"
cs-410_7_8_144,"00:11:36,275","00:11:40,017",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,words that are not really
cs-410_7_8_145,"00:11:40,017","00:11:44,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,they might be extraneously
cs-410_7_8_146,"00:11:44,320","00:11:49,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"As a topic representation,"
cs-410_7_8_147,"00:11:49,590","00:11:52,452",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,That because the high probability
cs-410_7_8_148,"00:11:52,452","00:11:55,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,they are not really
cs-410_7_8_149,"00:11:55,310","00:11:58,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,So my question is how can we
cs-410_7_8_150,"00:11:59,720","00:12:02,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,Now this is the topic of the next module.
cs-410_7_8_151,"00:12:02,680","00:12:06,913",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,We're going to talk about how to use
cs-410_7_8_152,"00:12:06,913","00:12:08,077",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,these common words.
cs-410_7_8_153,"00:12:08,077","00:12:18,077",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,[MUSIC]
cs-410_1_11_1,"00:00:00,012","00:00:07,093",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_11_2,"00:00:07,093","00:00:11,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the discriminative
cs-410_1_11_3,"00:00:13,000","00:00:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,In this lecture we're going to
cs-410_1_11_4,"00:00:15,840","00:00:20,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,do text categorization and
cs-410_1_11_5,"00:00:20,220","00:00:24,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,This is a slide that you have seen from
cs-410_1_11_6,"00:00:24,760","00:00:29,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,where we have shown that although
cs-410_1_11_7,"00:00:29,120","00:00:34,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"the generation of text data, from each"
cs-410_1_11_8,"00:00:34,090","00:00:40,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,rule to eventually rewrite the scoring
cs-410_1_11_9,"00:00:40,900","00:00:45,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,And this scoring function is basically
cs-410_1_11_10,"00:00:45,520","00:00:50,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,"of word features, where the feature values"
cs-410_1_11_11,"00:00:50,530","00:00:55,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,are the log of probability ratios of
cs-410_1_11_12,"00:00:57,280","00:01:02,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,Now this kind of scoring function
cs-410_1_11_13,"00:01:02,670","00:01:08,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,function where we can in general
cs-410_1_11_14,"00:01:08,570","00:01:12,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,Of course the features don't
cs-410_1_11_15,"00:01:12,340","00:01:16,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,Their features can be other
cs-410_1_11_16,"00:01:16,280","00:01:22,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,And we mentioned that this is precisely
cs-410_1_11_17,"00:01:22,880","00:01:27,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"So, in this lecture we're going to"
cs-410_1_11_18,"00:01:27,570","00:01:31,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,They try to model
cs-410_1_11_19,"00:01:31,450","00:01:36,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,labels given the data directly
cs-410_1_11_20,"00:01:36,990","00:01:41,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,to compute that interactively
cs-410_1_11_21,"00:01:41,550","00:01:47,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,So the general idea of logistic
cs-410_1_11_22,"00:01:47,150","00:01:52,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,the dependency of a binary
cs-410_1_11_23,"00:01:52,350","00:01:56,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,on some predictors that are denoted as X.
cs-410_1_11_24,"00:01:56,360","00:02:01,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,So here we have also changed the notation
cs-410_1_11_25,"00:02:01,720","00:02:06,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,to X for future values.
cs-410_1_11_26,"00:02:07,140","00:02:10,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,You may recall in the previous
cs-410_1_11_27,"00:02:10,120","00:02:12,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,FI to represent the future values.
cs-410_1_11_28,"00:02:13,910","00:02:18,762",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,"And here we use the notation of X factor,"
cs-410_1_11_29,"00:02:18,762","00:02:23,331",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,which is more common when we introduce
cs-410_1_11_30,"00:02:23,331","00:02:27,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,such discriminative algorithms.
cs-410_1_11_31,"00:02:27,640","00:02:29,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"So, X is our input."
cs-410_1_11_32,"00:02:29,690","00:02:37,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,It's a vector with n features and
cs-410_1_11_33,"00:02:37,930","00:02:42,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,And I will go with a model that dependency
cs-410_1_11_34,"00:02:42,920","00:02:44,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,these features.
cs-410_1_11_35,"00:02:44,360","00:02:49,897",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,So in our categorization problem when
cs-410_1_11_36,"00:02:49,897","00:02:55,183",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"theta 2, and we can use the Y value to"
cs-410_1_11_37,"00:02:55,183","00:03:00,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"it means the category of the document,"
cs-410_1_11_38,"00:03:00,080","00:03:07,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"Now, the goal here is the model, the"
cs-410_1_11_39,"00:03:07,225","00:03:13,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,as opposed to model of the generation of
cs-410_1_11_40,"00:03:13,465","00:03:15,985",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,And another advantage of this
cs-410_1_11_41,"00:03:15,985","00:03:19,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,it would allow many other features
cs-410_1_11_42,"00:03:19,880","00:03:23,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,since we're not modeling
cs-410_1_11_43,"00:03:23,490","00:03:25,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,And we can plug in any
cs-410_1_11_44,"00:03:25,830","00:03:31,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So this is potentially advantageous for
cs-410_1_11_45,"00:03:31,410","00:03:34,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"So more specifically,"
cs-410_1_11_46,"00:03:34,510","00:03:40,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,assume the functional form of Y
cs-410_1_11_47,"00:03:40,760","00:03:46,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,And this is very closely
cs-410_1_11_48,"00:03:46,610","00:03:51,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,odds that I introduced in the Naive Bayes
cs-410_1_11_49,"00:03:51,290","00:03:56,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,of the two categories that you
cs-410_1_11_50,"00:03:57,900","00:04:00,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,So this is what I meant.
cs-410_1_11_51,"00:04:00,230","00:04:05,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"So in the case of Naive Bayes,"
cs-410_1_11_52,"00:04:05,430","00:04:11,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,eventually we have reached
cs-410_1_11_53,"00:04:12,990","00:04:18,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,But here we actually
cs-410_1_11_54,"00:04:18,290","00:04:23,001",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,that we with the model our
cs-410_1_11_55,"00:04:23,001","00:04:27,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,probability of Y given X
cs-410_1_11_56,"00:04:29,840","00:04:36,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,directly as a function of these features.
cs-410_1_11_57,"00:04:37,580","00:04:46,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"So, most specifically we assume that the"
cs-410_1_11_58,"00:04:46,260","00:04:52,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,the probability of Y equals
cs-410_1_11_59,"00:04:54,460","00:04:56,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"All right, so it's a function of x and"
cs-410_1_11_60,"00:04:56,580","00:05:00,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,it's a linear combination of these feature
cs-410_1_11_61,"00:05:02,390","00:05:06,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,And it seems we know that
cs-410_1_11_62,"00:05:06,790","00:05:11,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,is one minus probability
cs-410_1_11_63,"00:05:11,100","00:05:16,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,this can be also written in this way.
cs-410_1_11_64,"00:05:16,030","00:05:20,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,So this is a log out ratio here.
cs-410_1_11_65,"00:05:22,040","00:05:23,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"And so in logistic regression,"
cs-410_1_11_66,"00:05:23,250","00:05:27,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,we're basically assuming that
cs-410_1_11_67,"00:05:27,490","00:05:34,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,Okay my X is dependent on this linear
cs-410_1_11_68,"00:05:34,570","00:05:39,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So it's just one of the many possible
cs-410_1_11_69,"00:05:39,960","00:05:42,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,But this particular form
cs-410_1_11_70,"00:05:42,880","00:05:45,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,it also has some nice properties.
cs-410_1_11_71,"00:05:47,760","00:05:53,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,So if we rewrite this equation to actually
cs-410_1_11_72,"00:05:53,690","00:05:58,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,In terms of X by getting rid of
cs-410_1_11_73,"00:05:58,770","00:06:01,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,and this is called a logistical function.
cs-410_1_11_74,"00:06:01,980","00:06:07,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"It's a transformation of X into Y,"
cs-410_1_11_75,"00:06:08,120","00:06:14,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"on the right side here, so"
cs-410_1_11_76,"00:06:14,090","00:06:19,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"into a range of values from 0 to 1.0,"
cs-410_1_11_77,"00:06:19,310","00:06:23,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,And that's precisely what we want
cs-410_1_11_78,"00:06:24,350","00:06:26,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,And the function form looks like this.
cs-410_1_11_79,"00:06:28,170","00:06:31,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,So this is the basic idea
cs-410_1_11_80,"00:06:31,790","00:06:34,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,And it's a very useful classifier that
cs-410_1_11_81,"00:06:34,570","00:06:39,231",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,can be used to do a lot of classification
cs-410_1_11_82,"00:06:41,750","00:06:47,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,So as in all cases of model we would be
cs-410_1_11_83,"00:06:47,100","00:06:50,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,And in fact in all of the machine running
cs-410_1_11_84,"00:06:50,780","00:06:54,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,set up object and
cs-410_1_11_85,"00:06:54,980","00:07:00,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,then the next step is to
cs-410_1_11_86,"00:07:00,120","00:07:02,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"In general, we're going to adjust"
cs-410_1_11_87,"00:07:02,680","00:07:06,641",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,Optimize the performance of
cs-410_1_11_88,"00:07:06,641","00:07:13,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,So in our case just assume we have
cs-410_1_11_89,"00:07:13,410","00:07:20,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,each pair is basically a future vector
cs-410_1_11_90,"00:07:20,810","00:07:23,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,Y is either 1 or 0.
cs-410_1_11_91,"00:07:23,530","00:07:29,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,So in our case we are interested
cs-410_1_11_92,"00:07:31,310","00:07:36,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,The conditional likelihood here is
cs-410_1_11_93,"00:07:36,020","00:07:41,829",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,basically to model why
cs-410_1_11_94,"00:07:41,829","00:07:46,382",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"so it's not like a moderate x, but"
cs-410_1_11_95,"00:07:46,382","00:07:50,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,rather we're going to model this.
cs-410_1_11_96,"00:07:50,787","00:07:55,589",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,Note that this is a conditional
cs-410_1_11_97,"00:07:55,589","00:08:00,494",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,this is also precisely what we wanted For
cs-410_1_11_98,"00:08:00,494","00:08:06,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,Now so the likelihood function would be
cs-410_1_11_99,"00:08:06,266","00:08:07,383",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"And in each case,"
cs-410_1_11_100,"00:08:07,383","00:08:12,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,this is the model of the probability of
cs-410_1_11_101,"00:08:12,990","00:08:19,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"So given a particular Xi, how likely"
cs-410_1_11_102,"00:08:19,960","00:08:23,228",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"Of course, Yi could be 1 or"
cs-410_1_11_103,"00:08:23,228","00:08:28,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,the function found here would vary
cs-410_1_11_104,"00:08:28,310","00:08:33,374",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"If it's a 1, we'll be taking this form."
cs-410_1_11_105,"00:08:33,374","00:08:36,276",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,And that's basically the logistic
cs-410_1_11_106,"00:08:36,276","00:08:38,723",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"But what about this, if it's 0?"
cs-410_1_11_107,"00:08:38,723","00:08:45,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,"Well, if it's 0, then we have to use"
cs-410_1_11_108,"00:08:48,299","00:08:50,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"Now, how do we get this one?"
cs-410_1_11_109,"00:08:50,310","00:08:54,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"Well, that's just a 1 minus"
cs-410_1_11_110,"00:08:55,990","00:08:58,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,And you can easily see this.
cs-410_1_11_111,"00:08:58,200","00:09:04,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Now the key point in here is that the
cs-410_1_11_112,"00:09:04,220","00:09:09,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"Yi, if it's a 1,"
cs-410_1_11_113,"00:09:09,340","00:09:13,852",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,And if you think about when we
cs-410_1_11_114,"00:09:13,852","00:09:19,033",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,we're basically going to want this
cs-410_1_11_115,"00:09:19,033","00:09:26,519",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,"When the label is 1, that means"
cs-410_1_11_116,"00:09:26,519","00:09:31,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,"But if the document is not,"
cs-410_1_11_117,"00:09:31,925","00:09:36,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,and what's going to happen is
cs-410_1_11_118,"00:09:36,821","00:09:40,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,small as possible because this sum's 1.
cs-410_1_11_119,"00:09:40,500","00:09:45,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,"When I maximize this one,"
cs-410_1_11_120,"00:09:48,070","00:09:53,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,"So you can see basically, if we maximize"
cs-410_1_11_121,"00:09:53,275","00:09:58,568",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,to basically try to make the prediction on
cs-410_1_11_122,"00:10:00,957","00:10:04,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,"So as another occasion, when you"
cs-410_1_11_123,"00:10:04,970","00:10:07,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"basically you'll find a beta value,"
cs-410_1_11_124,"00:10:07,075","00:10:11,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,a set of beta values that would
cs-410_1_11_125,"00:10:12,190","00:10:15,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"And this, again, then gives us"
cs-410_1_11_126,"00:10:15,930","00:10:20,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,"In this case,"
cs-410_1_11_127,"00:10:20,130","00:10:22,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,Newton's method is a popular
cs-410_1_11_128,"00:10:22,870","00:10:25,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,there are other methods as well.
cs-410_1_11_129,"00:10:25,050","00:10:29,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"But in the end,"
cs-410_1_11_130,"00:10:29,270","00:10:34,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,"Once we have the beta values,"
cs-410_1_11_131,"00:10:34,590","00:10:38,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,function to help us classify a document.
cs-410_1_11_132,"00:10:39,620","00:10:40,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,So what's the function?
cs-410_1_11_133,"00:10:40,580","00:10:42,399",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,"Well, it's this one."
cs-410_1_11_134,"00:10:42,399","00:10:47,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"See, if we have all the beta values,"
cs-410_1_11_135,"00:10:47,330","00:10:52,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,All we need is to compute the Xi for that
cs-410_1_11_136,"00:10:52,810","00:10:58,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,That will give us an estimated probability
cs-410_1_11_137,"00:10:59,170","00:11:02,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"Okay so, so much for"
cs-410_1_11_138,"00:11:02,930","00:11:06,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,Let's also introduce another
cs-410_1_11_139,"00:11:06,710","00:11:08,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,called K-Nearest Neighbors.
cs-410_1_11_140,"00:11:08,230","00:11:12,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"Now in general, I should say there"
cs-410_1_11_141,"00:11:12,340","00:11:17,517",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,a thorough introduction to all of them is
cs-410_1_11_142,"00:11:17,517","00:11:20,169",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,And you should take
cs-410_1_11_143,"00:11:20,169","00:11:23,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,read more about machine
cs-410_1_11_144,"00:11:23,500","00:11:27,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"Here, I just want to include the basic"
cs-410_1_11_145,"00:11:27,950","00:11:32,345",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,"used classifiers, since you might"
cs-410_1_11_146,"00:11:32,345","00:11:36,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,So the second classifier is
cs-410_1_11_147,"00:11:36,610","00:11:40,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"In this approach,"
cs-410_1_11_148,"00:11:40,830","00:11:45,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,the conditional probability of label
cs-410_1_11_149,"00:11:45,615","00:11:49,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,So the idea is to keep all
cs-410_1_11_150,"00:11:49,360","00:11:53,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,then once we see a text object that we
cs-410_1_11_151,"00:11:53,900","00:11:59,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,the K examples in the training set and
cs-410_1_11_152,"00:11:59,290","00:12:03,981",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,"Basically, this is to find"
cs-410_1_11_153,"00:12:03,981","00:12:05,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,the training data set.
cs-410_1_11_154,"00:12:05,700","00:12:08,314",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,So once we found the neighborhood and
cs-410_1_11_155,"00:12:08,314","00:12:14,132",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,we found the object that are close to the
cs-410_1_11_156,"00:12:14,132","00:12:18,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,and let's say we have found
cs-410_1_11_157,"00:12:18,620","00:12:21,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,That's why this method is
cs-410_1_11_158,"00:12:21,460","00:12:26,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,Then we're going to assign the category
cs-410_1_11_159,"00:12:26,230","00:12:28,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,Basically we're going to allow
cs-410_1_11_160,"00:12:28,870","00:12:32,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,the category of the objective that
cs-410_1_11_161,"00:12:33,560","00:12:38,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Now that means if most of them have
cs-410_1_11_162,"00:12:38,240","00:12:41,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,"one, they're going to say this"
cs-410_1_11_163,"00:12:43,100","00:12:47,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,This approach can also be improved by
cs-410_1_11_164,"00:12:47,820","00:12:49,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,of a current object.
cs-410_1_11_165,"00:12:49,240","00:12:53,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"Basically, we can assume a closed"
cs-410_1_11_166,"00:12:53,560","00:12:55,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,about the category of the subject.
cs-410_1_11_167,"00:12:55,110","00:13:00,626",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So, we can give such a neighbor"
cs-410_1_11_168,"00:13:00,626","00:13:04,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,And we can take away some of
cs-410_1_11_169,"00:13:06,120","00:13:08,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,But the general idea is look
cs-410_1_11_170,"00:13:08,520","00:13:13,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,then try to assess the category based
cs-410_1_11_171,"00:13:13,270","00:13:15,745",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,"Intuitively, this makes a lot of sense."
cs-410_1_11_172,"00:13:15,745","00:13:21,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,"But mathematically, this can also be"
cs-410_1_11_173,"00:13:21,170","00:13:26,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=801,there's a conditional probability of
cs-410_1_11_174,"00:13:28,190","00:13:33,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=808,Now I'm going to explain this intuition in
cs-410_1_11_175,"00:13:33,640","00:13:40,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,emphasize that we do need a similarity
cs-410_1_11_176,"00:13:40,530","00:13:43,874",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,"Note that in naive base class five,"
cs-410_1_11_177,"00:13:43,874","00:13:48,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"And in logistical regression, we did not"
cs-410_1_11_178,"00:13:48,160","00:13:52,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,"either, but here we explicitly"
cs-410_1_11_179,"00:13:52,570","00:13:57,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,Now this similarity function
cs-410_1_11_180,"00:13:57,500","00:14:02,288",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,us to inject any of our
cs-410_1_11_181,"00:14:02,288","00:14:07,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,Basically effective features
cs-410_1_11_182,"00:14:07,420","00:14:12,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,make the objects that are on the same
cs-410_1_11_183,"00:14:12,770","00:14:16,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,distinguishing objects
cs-410_1_11_184,"00:14:16,600","00:14:21,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,So the design of this similarity function
cs-410_1_11_185,"00:14:21,100","00:14:25,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,of the features in logistical
cs-410_1_11_186,"00:14:25,340","00:14:28,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,So let's illustrate how K-NN works.
cs-410_1_11_187,"00:14:28,350","00:14:32,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=868,Now suppose we have a lot
cs-410_1_11_188,"00:14:32,360","00:14:38,612",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,And I've colored them differently and
cs-410_1_11_189,"00:14:38,612","00:14:43,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,Now suppose we have a new object in
cs-410_1_11_190,"00:14:43,690","00:14:46,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,"So according to this approach,"
cs-410_1_11_191,"00:14:46,530","00:14:50,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,"Now, let's first think of a special"
cs-410_1_11_192,"00:14:50,730","00:14:51,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,the closest neighbor.
cs-410_1_11_193,"00:14:53,100","00:14:59,264",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,"Now in this case, let's assume the closest"
cs-410_1_11_194,"00:14:59,264","00:15:04,191",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,"And so then we're going to say,"
cs-410_1_11_195,"00:15:04,191","00:15:09,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,"object that is in category of diamonds,"
cs-410_1_11_196,"00:15:09,250","00:15:11,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,"Then we're going to say, well,"
cs-410_1_11_197,"00:15:11,945","00:15:17,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,we're going to assign the same
cs-410_1_11_198,"00:15:17,250","00:15:22,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,But let's also look at another possibility
cs-410_1_11_199,"00:15:22,346","00:15:24,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,so let's think about the four neighbors.
cs-410_1_11_200,"00:15:26,060","00:15:31,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,"In this case, we're going to include a lot"
cs-410_1_11_201,"00:15:31,090","00:15:32,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,"pink, right?"
cs-410_1_11_202,"00:15:32,970","00:15:38,182",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=932,"So in this case now, we're going to"
cs-410_1_11_203,"00:15:38,182","00:15:41,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=938,there are three neighbors
cs-410_1_11_204,"00:15:41,590","00:15:43,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,"So if we take a vote,"
cs-410_1_11_205,"00:15:43,020","00:15:48,001",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,then we'll conclude the object is
cs-410_1_11_206,"00:15:48,001","00:15:52,252",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,So this both illustrates how
cs-410_1_11_207,"00:15:52,252","00:15:57,021",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,also it illustrates some potential
cs-410_1_11_208,"00:15:57,021","00:16:00,867",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,"Basically, the results might"
cs-410_1_11_209,"00:16:00,867","00:16:03,703",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,k's an important parameter to optimize.
cs-410_1_11_210,"00:16:03,703","00:16:07,871",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,"Now, you can intuitively imagine"
cs-410_1_11_211,"00:16:07,871","00:16:11,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"around this object, and"
cs-410_1_11_212,"00:16:11,800","00:16:16,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,a lot of neighbors who will
cs-410_1_11_213,"00:16:16,360","00:16:21,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=976,"But if we have only a few,"
cs-410_1_11_214,"00:16:21,140","00:16:25,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=981,"So on the one hand,"
cs-410_1_11_215,"00:16:25,220","00:16:26,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,And then we have more votes.
cs-410_1_11_216,"00:16:26,850","00:16:31,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,"But on the other hand, as we try to find"
cs-410_1_11_217,"00:16:31,770","00:16:36,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,on getting neighbors that are not
cs-410_1_11_218,"00:16:36,990","00:16:40,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=996,They might actually be far away
cs-410_1_11_219,"00:16:40,210","00:16:44,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,So although you get more neighbors but
cs-410_1_11_220,"00:16:44,520","00:16:47,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1004,helpful because they are not
cs-410_1_11_221,"00:16:47,650","00:16:51,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,So the parameter still has
cs-410_1_11_222,"00:16:51,150","00:16:55,996",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,"And typically, you can optimize such"
cs-410_1_11_223,"00:16:55,996","00:17:01,378",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,"Basically, you're going to separate"
cs-410_1_11_224,"00:17:01,378","00:17:05,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,then you're going to use one
cs-410_1_11_225,"00:17:05,803","00:17:10,778",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,the parameter k here or some other
cs-410_1_11_226,"00:17:10,778","00:17:15,913",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,And then you're going to assume
cs-410_1_11_227,"00:17:15,913","00:17:21,063",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,training that will be actually be
cs-410_1_11_228,"00:17:23,103","00:17:24,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,"So as I mentioned,"
cs-410_1_11_229,"00:17:24,257","00:17:29,234",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,K-NN can be actually regarded as estimate
cs-410_1_11_230,"00:17:29,234","00:17:34,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,an that's why we put this in the category
cs-410_1_11_231,"00:17:34,600","00:17:39,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,So the key assumption that we made in
cs-410_1_11_232,"00:17:39,470","00:17:44,027",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1059,of the label given the document
cs-410_1_11_233,"00:17:44,027","00:17:51,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,example probability of theta i
cs-410_1_11_234,"00:17:51,620","00:17:56,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,And that just means we're going to assume
cs-410_1_11_235,"00:17:56,890","00:18:01,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1076,all the documents in these region R here.
cs-410_1_11_236,"00:18:01,570","00:18:05,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1081,And suppose we draw a neighborhood and
cs-410_1_11_237,"00:18:05,260","00:18:10,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,since the data instances are very
cs-410_1_11_238,"00:18:10,320","00:18:15,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1090,the conditional distribution of the label
cs-410_1_11_239,"00:18:15,530","00:18:19,408",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,If these are very different
cs-410_1_11_240,"00:18:19,408","00:18:23,136",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,the probability of c doc given
cs-410_1_11_241,"00:18:23,136","00:18:24,976",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1103,So that's a very key assumption.
cs-410_1_11_242,"00:18:24,976","00:18:29,481",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1104,And that's actually important assumption
cs-410_1_11_243,"00:18:29,481","00:18:34,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1109,that would allow us to
cs-410_1_11_244,"00:18:34,820","00:18:35,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1114,"But in reality,"
cs-410_1_11_245,"00:18:35,730","00:18:39,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1115,"whether this is true of course,"
cs-410_1_11_246,"00:18:39,560","00:18:43,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,Because neighborhood is largely
cs-410_1_11_247,"00:18:43,610","00:18:48,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,If our similarity function captures
cs-410_1_11_248,"00:18:48,180","00:18:51,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,distributions then these
cs-410_1_11_249,"00:18:51,290","00:18:55,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1131,if our similarity function could
cs-410_1_11_250,"00:18:55,240","00:18:58,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1135,assumption would be a problem and
cs-410_1_11_251,"00:18:59,320","00:19:01,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,"Okay, let's proceed with these assumption."
cs-410_1_11_252,"00:19:01,680","00:19:03,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,"Then what we are saying is that,"
cs-410_1_11_253,"00:19:03,310","00:19:07,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1143,in order to estimate the probability
cs-410_1_11_254,"00:19:07,570","00:19:14,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1147,We can try to estimate the probability of
cs-410_1_11_255,"00:19:14,230","00:19:16,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1154,"Now, this has a benefit, of course,"
cs-410_1_11_256,"00:19:16,730","00:19:20,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,of bringing additional data points to
cs-410_1_11_257,"00:19:22,660","00:19:25,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1162,And so this is precisely the idea of K-NN.
cs-410_1_11_258,"00:19:25,410","00:19:29,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1165,Basically now we can use
cs-410_1_11_259,"00:19:29,910","00:19:33,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1169,all the documents in this region
cs-410_1_11_260,"00:19:33,870","00:19:40,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1173,And I have even given a formula here where
cs-410_1_11_261,"00:19:40,340","00:19:44,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1180,this region and then normalize that by the
cs-410_1_11_262,"00:19:44,910","00:19:49,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,"So the numerator that you see here,"
cs-410_1_11_263,"00:19:49,510","00:19:55,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,is a counter of the documents in
cs-410_1_11_264,"00:19:55,025","00:19:57,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,Since these are training document and
cs-410_1_11_265,"00:19:57,910","00:20:01,394",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1197,We can simply count how many
cs-410_1_11_266,"00:20:01,394","00:20:03,491",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1201,"How many times we have the same signs,"
cs-410_1_11_267,"00:20:03,491","00:20:07,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1203,And then the denominator is just
cs-410_1_11_268,"00:20:07,269","00:20:08,981",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1207,documents in this region.
cs-410_1_11_269,"00:20:08,981","00:20:12,781",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1208,So this gives us a rough estimate of
cs-410_1_11_270,"00:20:12,781","00:20:13,661",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1212,neighborhood.
cs-410_1_11_271,"00:20:13,661","00:20:17,539",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1213,And we are going to assign
cs-410_1_11_272,"00:20:17,539","00:20:21,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1217,to our data object since
cs-410_1_11_273,"00:20:21,821","00:20:31,821",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1221,[MUSIC]
cs-410_4_11_1,"00:00:00,025","00:00:05,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[NOISE] This lecture is about the ordinal
cs-410_4_11_2,"00:00:05,824","00:00:12,859",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,logistic regression for
cs-410_4_11_3,"00:00:12,859","00:00:18,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"So, this is our problem set up for a"
cs-410_4_11_4,"00:00:18,730","00:00:21,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,Or more specifically a rating prediction.
cs-410_4_11_5,"00:00:21,460","00:00:27,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,We have an opinionated text document d as
cs-410_4_11_6,"00:00:27,430","00:00:30,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,a rating in the range of 1 through k so
cs-410_4_11_7,"00:00:30,770","00:00:37,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"it's a discrete rating, and"
cs-410_4_11_8,"00:00:37,110","00:00:38,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,We have k categories here.
cs-410_4_11_9,"00:00:38,960","00:00:40,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,Now we could use a regular text for
cs-410_4_11_10,"00:00:40,330","00:00:42,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,categorization technique
cs-410_4_11_11,"00:00:42,950","00:00:48,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,But such a solution would not consider the
cs-410_4_11_12,"00:00:48,890","00:00:53,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"Intuitively, the features that can"
cs-410_4_11_13,"00:00:53,400","00:00:58,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"or rather rating 2 from 1,"
cs-410_4_11_14,"00:00:58,380","00:01:02,749",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,those that can distinguish k from k-1.
cs-410_4_11_15,"00:01:02,749","00:01:08,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"For example, positive words"
cs-410_4_11_16,"00:01:08,610","00:01:11,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,When we train categorization
cs-410_4_11_17,"00:01:11,950","00:01:16,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,problem by treating these categories as
cs-410_4_11_18,"00:01:17,700","00:01:18,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,So what's the solution?
cs-410_4_11_19,"00:01:18,760","00:01:23,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,Well in general we can order to classify
cs-410_4_11_20,"00:01:23,830","00:01:26,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And here we're going to
cs-410_4_11_21,"00:01:26,330","00:01:29,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,called ordinal logistic regression.
cs-410_4_11_22,"00:01:29,030","00:01:33,218",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"Now, let's first think about how"
cs-410_4_11_23,"00:01:33,218","00:01:34,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,a binary sentiment.
cs-410_4_11_24,"00:01:34,410","00:01:36,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,A categorization problem.
cs-410_4_11_25,"00:01:36,460","00:01:40,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,So suppose we just wanted to distinguish
cs-410_4_11_26,"00:01:40,700","00:01:44,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,that is just a two category
cs-410_4_11_27,"00:01:44,070","00:01:47,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,So the predictors are represented as X and
cs-410_4_11_28,"00:01:47,660","00:01:50,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,And there are M features all together.
cs-410_4_11_29,"00:01:50,080","00:01:52,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,The feature value is a real number.
cs-410_4_11_30,"00:01:52,390","00:01:55,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,And this can be representation
cs-410_4_11_31,"00:01:56,790","00:02:02,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"And why it has two values,"
cs-410_4_11_32,"00:02:02,150","00:02:04,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"1 means X is positive,"
cs-410_4_11_33,"00:02:04,940","00:02:09,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,And then of course this is a standard
cs-410_4_11_34,"00:02:09,940","00:02:11,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,We can apply logistical regression.
cs-410_4_11_35,"00:02:11,990","00:02:17,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,You may recall that in logistical
cs-410_4_11_36,"00:02:17,620","00:02:22,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"of probability that the Y is equal to one,"
cs-410_4_11_37,"00:02:22,820","00:02:28,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,assumed to be a linear function
cs-410_4_11_38,"00:02:28,490","00:02:35,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,So this would allow us to also write
cs-410_4_11_39,"00:02:36,030","00:02:41,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,in this equation that you
cs-410_4_11_40,"00:02:43,020","00:02:47,306",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,So that's a logistical function and
cs-410_4_11_41,"00:02:47,306","00:02:52,421",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,you can see it relates
cs-410_4_11_42,"00:02:52,421","00:02:57,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,probability that y=1
cs-410_4_11_43,"00:02:57,970","00:03:02,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,And of course beta i's
cs-410_4_11_44,"00:03:02,960","00:03:07,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,just a direct application of logistical
cs-410_4_11_45,"00:03:08,790","00:03:11,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"What if we have multiple categories,"
cs-410_4_11_46,"00:03:11,730","00:03:16,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,Well we have to use such a binary
cs-410_4_11_47,"00:03:16,600","00:03:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,to solve this multi
cs-410_4_11_48,"00:03:21,170","00:03:26,215",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,And the idea is we can introduce
cs-410_4_11_49,"00:03:26,215","00:03:29,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,In each case we asked
cs-410_4_11_50,"00:03:29,790","00:03:35,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"whether the rating is j or above,"
cs-410_4_11_51,"00:03:35,210","00:03:41,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"So when Yj is equal to 1,"
cs-410_4_11_52,"00:03:41,550","00:03:44,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"When it's 0,"
cs-410_4_11_53,"00:03:45,360","00:03:51,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So basically if we want to predict
cs-410_4_11_54,"00:03:51,520","00:03:57,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,we first have one classifier to
cs-410_4_11_55,"00:03:57,070","00:03:59,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,And that's our classifier one.
cs-410_4_11_56,"00:03:59,220","00:04:02,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,And then we're going to have another
cs-410_4_11_57,"00:04:02,275","00:04:05,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,At k-1 from the rest.
cs-410_4_11_58,"00:04:05,850","00:04:06,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,That's Classifier 2.
cs-410_4_11_59,"00:04:06,700","00:04:11,989",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"And in the end, we need a Classifier"
cs-410_4_11_60,"00:04:11,989","00:04:16,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So altogether we'll have k-1 classifiers.
cs-410_4_11_61,"00:04:17,830","00:04:23,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,Now if we do that of course then
cs-410_4_11_62,"00:04:23,580","00:04:27,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,and the logistical regression program
cs-410_4_11_63,"00:04:27,750","00:04:30,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,as you have just seen
cs-410_4_11_64,"00:04:30,910","00:04:33,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,Only that here we have more parameters.
cs-410_4_11_65,"00:04:33,760","00:04:37,566",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"Because for each classifier,"
cs-410_4_11_66,"00:04:37,566","00:04:41,889",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,So now the logistical regression
cs-410_4_11_67,"00:04:41,889","00:04:44,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,which corresponds to a rating level.
cs-410_4_11_68,"00:04:46,190","00:04:51,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,And I have also used of
cs-410_4_11_69,"00:04:51,910","00:04:54,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,And this is to.
cs-410_4_11_70,"00:04:54,390","00:04:57,451",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"Make the notation more consistent,"
cs-410_4_11_71,"00:04:57,451","00:05:02,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,than was what we can show in
cs-410_4_11_72,"00:05:02,800","00:05:09,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,So here we now have basically k minus one
cs-410_4_11_73,"00:05:09,000","00:05:12,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,Each has it's own set of parameters.
cs-410_4_11_74,"00:05:12,380","00:05:18,349",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"So now with this approach,"
cs-410_4_11_75,"00:05:19,350","00:05:23,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,After we have trained these k-1
cs-410_4_11_76,"00:05:23,760","00:05:30,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,"separately of course,"
cs-410_4_11_77,"00:05:30,160","00:05:37,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,then invoke a classifier
cs-410_4_11_78,"00:05:37,085","00:05:43,955",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,So first let look at the classifier
cs-410_4_11_79,"00:05:43,955","00:05:49,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,So this classifier will tell
cs-410_4_11_80,"00:05:49,010","00:05:54,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,have a rating of K or about.
cs-410_4_11_81,"00:05:54,230","00:05:58,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,If probability according to this
cs-410_4_11_82,"00:05:58,360","00:06:00,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"larger than point five,"
cs-410_4_11_83,"00:06:00,650","00:06:01,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,The rating is K.
cs-410_4_11_84,"00:06:02,540","00:06:06,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"Now, what if it's not as"
cs-410_4_11_85,"00:06:06,750","00:06:10,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"Well, that means the rating's below K,"
cs-410_4_11_86,"00:06:11,050","00:06:13,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"So now,"
cs-410_4_11_87,"00:06:13,750","00:06:17,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,which tells us whether
cs-410_4_11_88,"00:06:18,690","00:06:20,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,It's at least K minus one.
cs-410_4_11_89,"00:06:20,660","00:06:23,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,And if the probability is
cs-410_4_11_90,"00:06:23,140","00:06:26,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,"then we'll say, well, then it's k-1."
cs-410_4_11_91,"00:06:26,400","00:06:27,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,What if it says no?
cs-410_4_11_92,"00:06:27,960","00:06:30,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"Well, that means the rating"
cs-410_4_11_93,"00:06:30,280","00:06:34,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,And so we're going to just keep
cs-410_4_11_94,"00:06:34,990","00:06:41,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,And here we hit the end when we need
cs-410_4_11_95,"00:06:41,340","00:06:43,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,So this would help us solve the problem.
cs-410_4_11_96,"00:06:43,510","00:06:44,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,Right?
cs-410_4_11_97,"00:06:44,350","00:06:49,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,So we can have a classifier that would
cs-410_4_11_98,"00:06:49,320","00:06:51,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,in the range of 1 through k.
cs-410_4_11_99,"00:06:51,120","00:06:55,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Now unfortunately such a strategy is not
cs-410_4_11_100,"00:06:55,510","00:07:01,661",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,And specifically there are two
cs-410_4_11_101,"00:07:01,661","00:07:03,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,So these equations are the same as.
cs-410_4_11_102,"00:07:03,850","00:07:05,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,You have seen before.
cs-410_4_11_103,"00:07:06,250","00:07:10,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,Now the first problem is that there
cs-410_4_11_104,"00:07:10,100","00:07:11,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,There are many parameters.
cs-410_4_11_105,"00:07:11,630","00:07:15,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,"Now, can you count how many"
cs-410_4_11_106,"00:07:15,540","00:07:18,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,Now this may be a interesting exercise.
cs-410_4_11_107,"00:07:18,680","00:07:19,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,To do.
cs-410_4_11_108,"00:07:19,440","00:07:24,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,you might want to just pause the video and
cs-410_4_11_109,"00:07:24,250","00:07:27,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,How many parameters do I have for
cs-410_4_11_110,"00:07:28,580","00:07:30,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,And how many classifiers do we have?
cs-410_4_11_111,"00:07:31,840","00:07:37,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"Well you can see the, and so"
cs-410_4_11_112,"00:07:37,310","00:07:42,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"n plus one parameters, and we have k"
cs-410_4_11_113,"00:07:42,680","00:07:49,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,so the total number of parameters is
cs-410_4_11_114,"00:07:49,030","00:07:49,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,That's a lot.
cs-410_4_11_115,"00:07:49,820","00:07:54,096",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"A lot of parameters, so when"
cs-410_4_11_116,"00:07:54,096","00:07:58,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,we would in general need a lot of data
cs-410_4_11_117,"00:07:58,530","00:08:03,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,to help us decide the optimal
cs-410_4_11_118,"00:08:04,450","00:08:05,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So that's not ideal.
cs-410_4_11_119,"00:08:07,225","00:08:10,751",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,Now the second problems
cs-410_4_11_120,"00:08:10,751","00:08:15,595",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"these k minus 1 plus fives,"
cs-410_4_11_121,"00:08:15,595","00:08:17,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,These problems are actually dependent.
cs-410_4_11_122,"00:08:18,372","00:08:23,172",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"In general, words that are positive"
cs-410_4_11_123,"00:08:25,042","00:08:27,082",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,for any of these classifiers.
cs-410_4_11_124,"00:08:27,082","00:08:28,752",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,For all these classifiers.
cs-410_4_11_125,"00:08:28,752","00:08:31,896",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,So we should be able to take
cs-410_4_11_126,"00:08:33,016","00:08:37,846",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,Now the idea of ordinal logistical
cs-410_4_11_127,"00:08:37,846","00:08:42,007",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,The key idea is just
cs-410_4_11_128,"00:08:42,007","00:08:46,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,independent logistical
cs-410_4_11_129,"00:08:46,390","00:08:51,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,And that idea is to tie
cs-410_4_11_130,"00:08:51,590","00:08:59,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,And that means we are going to
cs-410_4_11_131,"00:08:59,070","00:09:05,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,These are the parameters that indicated
cs-410_4_11_132,"00:09:05,290","00:09:09,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,And we're going to assume these
cs-410_4_11_133,"00:09:09,490","00:09:10,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,all the K- 1 parameters.
cs-410_4_11_134,"00:09:10,920","00:09:13,678",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"And this just encodes our intuition that,"
cs-410_4_11_135,"00:09:13,678","00:09:17,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,positive words in general would
cs-410_4_11_136,"00:09:19,550","00:09:25,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,"So this is intuitively assumptions,"
cs-410_4_11_137,"00:09:25,220","00:09:27,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,And we have this order
cs-410_4_11_138,"00:09:28,630","00:09:34,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"Now in fact, this would allow us"
cs-410_4_11_139,"00:09:34,370","00:09:37,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,One is it's going to reduce
cs-410_4_11_140,"00:09:38,750","00:09:42,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,And the other is to allow us
cs-410_4_11_141,"00:09:42,880","00:09:45,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,Because all these parameters
cs-410_4_11_142,"00:09:45,860","00:09:51,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"So these training data, for"
cs-410_4_11_143,"00:09:51,200","00:09:55,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,shared to help us set
cs-410_4_11_144,"00:09:56,280","00:10:00,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,So we have more data to help
cs-410_4_11_145,"00:10:01,790","00:10:02,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"So what's the consequence,"
cs-410_4_11_146,"00:10:02,840","00:10:08,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,well the formula would look very similar
cs-410_4_11_147,"00:10:08,010","00:10:13,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,now the beta parameter has just one
cs-410_4_11_148,"00:10:13,440","00:10:17,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,It no longer has the other index that
cs-410_4_11_149,"00:10:19,260","00:10:21,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,So that means we tie them together.
cs-410_4_11_150,"00:10:21,340","00:10:26,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,And there's only one set of better
cs-410_4_11_151,"00:10:26,340","00:10:31,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,"However, each classifier still"
cs-410_4_11_152,"00:10:31,180","00:10:33,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,The R for parameter.
cs-410_4_11_153,"00:10:33,060","00:10:35,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,Except it's different.
cs-410_4_11_154,"00:10:35,290","00:10:39,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,And this is of course needed to predict
cs-410_4_11_155,"00:10:39,950","00:10:43,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,So R for sub j is different it
cs-410_4_11_156,"00:10:43,840","00:10:46,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,has a different R value.
cs-410_4_11_157,"00:10:46,020","00:10:48,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,"But the rest of the parameters,"
cs-410_4_11_158,"00:10:48,890","00:10:53,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"So now you can also ask the question,"
cs-410_4_11_159,"00:10:53,940","00:10:57,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,"Again, that's an interesting"
cs-410_4_11_160,"00:10:57,140","00:11:00,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,"So if you think about it for a moment, and"
cs-410_4_11_161,"00:11:00,910","00:11:05,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"you will see now, the param,"
cs-410_4_11_162,"00:11:05,415","00:11:08,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,Specifically we have M plus K minus one.
cs-410_4_11_163,"00:11:08,320","00:11:13,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"Because we have M, beta values, and"
cs-410_4_11_164,"00:11:15,550","00:11:17,575",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,"So let's just look basically,"
cs-410_4_11_165,"00:11:17,575","00:11:21,931",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,that's basically the main idea of
cs-410_4_11_166,"00:11:24,695","00:11:31,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,"So, now, let's see how we can use such"
cs-410_4_11_167,"00:11:31,290","00:11:39,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"It turns out that with this, this idea of"
cs-410_4_11_168,"00:11:39,730","00:11:44,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,We also end up by having
cs-410_4_11_169,"00:11:44,710","00:11:50,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"And more specifically now, the criteria"
cs-410_4_11_170,"00:11:50,220","00:11:55,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,"are at least 0.5 above,"
cs-410_4_11_171,"00:11:55,440","00:12:00,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,whether the score of
cs-410_4_11_172,"00:12:00,810","00:12:06,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,"equal to negative authors of j,"
cs-410_4_11_173,"00:12:06,390","00:12:11,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,"Now, the scoring function is just"
cs-410_4_11_174,"00:12:11,130","00:12:14,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,all the features with
cs-410_4_11_175,"00:12:15,790","00:12:21,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,"So, this means now we can simply make"
cs-410_4_11_176,"00:12:21,820","00:12:27,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,"the value of this scoring function,"
cs-410_4_11_177,"00:12:27,900","00:12:33,121",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,Now you can see the general
cs-410_4_11_178,"00:12:33,121","00:12:39,584",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,when the score is in the particular
cs-410_4_11_179,"00:12:39,584","00:12:46,569",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,then we will assign the corresponding
cs-410_4_11_180,"00:12:49,960","00:12:53,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"So in this approach,"
cs-410_4_11_181,"00:12:55,140","00:12:59,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,by using the features and
cs-410_4_11_182,"00:13:00,150","00:13:04,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,This score will then be
cs-410_4_11_183,"00:13:04,490","00:13:09,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,alpha values to see which
cs-410_4_11_184,"00:13:09,020","00:13:09,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"And then,"
cs-410_4_11_185,"00:13:09,540","00:13:14,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"using the range, we can then decide which"
cs-410_4_11_186,"00:13:14,220","00:13:19,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"Because, these ranges of alpha"
cs-410_4_11_187,"00:13:19,750","00:13:24,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,"levels of ratings, and that's from"
cs-410_4_11_188,"00:13:24,840","00:13:30,909",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,Each is tied to some level of rating.
cs-410_4_11_189,"00:13:30,909","00:13:40,909",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,[MUSIC]
cs-410_3_11_1,"00:00:00,025","00:00:06,573",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is
cs-410_3_11_2,"00:00:06,573","00:00:12,439",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,of evaluation of text categorization.
cs-410_3_11_3,"00:00:12,439","00:00:18,302",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,Earlier we have introduced measures that
cs-410_3_11_4,"00:00:18,302","00:00:19,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,recall.
cs-410_3_11_5,"00:00:19,920","00:00:26,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,For each category and each document
cs-410_3_11_6,"00:00:27,680","00:00:32,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,further examine how to combine the
cs-410_3_11_7,"00:00:32,530","00:00:36,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"different documents how to aggregate them,"
cs-410_3_11_8,"00:00:36,980","00:00:41,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,You see on the title here I indicated
cs-410_3_11_9,"00:00:41,220","00:00:46,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,this is in contrast to micro average
cs-410_3_11_10,"00:00:47,750","00:00:53,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"So, again, for each category we're going"
cs-410_3_11_11,"00:00:53,710","00:00:59,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,for example category c1 we have
cs-410_3_11_12,"00:00:59,880","00:01:06,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,And similarly we can do that for category
cs-410_3_11_13,"00:01:06,380","00:01:11,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,Now once we compute that and
cs-410_3_11_14,"00:01:11,050","00:01:13,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,example we can aggregate
cs-410_3_11_15,"00:01:13,840","00:01:17,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"For all the categories, for"
cs-410_3_11_16,"00:01:17,610","00:01:24,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,And this is often very useful to summarize
cs-410_3_11_17,"00:01:24,160","00:01:26,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,And aggregation can be
cs-410_3_11_18,"00:01:26,780","00:01:32,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,"Again as I said, in a case when you"
cs-410_3_11_19,"00:01:32,550","00:01:36,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,it's always good to think about what's
cs-410_3_11_20,"00:01:36,630","00:01:41,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"For example, we can consider arithmetic"
cs-410_3_11_21,"00:01:41,750","00:01:46,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,"you can use geometric mean,"
cs-410_3_11_22,"00:01:46,180","00:01:50,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Depending on the way you aggregate,"
cs-410_3_11_23,"00:01:50,540","00:01:54,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"in terms of which method works better,"
cs-410_3_11_24,"00:01:54,370","00:02:00,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,differences and choosing the right one or
cs-410_3_11_25,"00:02:00,860","00:02:03,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,So the difference fore example
cs-410_3_11_26,"00:02:03,770","00:02:08,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,geometrically is that the arithmetically
cs-410_3_11_27,"00:02:08,360","00:02:12,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,values whereas geometrically would
cs-410_3_11_28,"00:02:12,170","00:02:16,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,Base and so whether you are want
cs-410_3_11_29,"00:02:16,940","00:02:22,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,high values would be a question
cs-410_3_11_30,"00:02:22,040","00:02:24,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,similar we can do that for
cs-410_3_11_31,"00:02:24,720","00:02:29,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,So that's how we can generate the overall
cs-410_3_11_32,"00:02:31,660","00:02:36,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,Now we can do the same for aggregation
cs-410_3_11_33,"00:02:36,990","00:02:40,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,So it's exactly the same situation for
cs-410_3_11_34,"00:02:40,300","00:02:42,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"Precision, recall, and F."
cs-410_3_11_35,"00:02:42,340","00:02:47,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,And then after we have completed
cs-410_3_11_36,"00:02:47,130","00:02:51,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,we're going to aggregate them to generate
cs-410_3_11_37,"00:02:51,590","00:02:52,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,overall F score.
cs-410_3_11_38,"00:02:53,510","00:02:57,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"These are, again, examining"
cs-410_3_11_39,"00:02:57,380","00:03:00,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,Which one's more useful will
cs-410_3_11_40,"00:03:00,390","00:03:06,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"In general, it's beneficial to look at"
cs-410_3_11_41,"00:03:06,180","00:03:10,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,And especially if you compare different
cs-410_3_11_42,"00:03:10,850","00:03:16,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,it might reveal which method
cs-410_3_11_43,"00:03:16,370","00:03:19,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,in what situations and
cs-410_3_11_44,"00:03:19,830","00:03:23,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,Understanding the strands of a method or
cs-410_3_11_45,"00:03:23,070","00:03:25,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,this provides further insight for
cs-410_3_11_46,"00:03:28,260","00:03:32,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"So as I mentioned,"
cs-410_3_11_47,"00:03:32,180","00:03:35,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,in contrast to the macro average
cs-410_3_11_48,"00:03:35,890","00:03:41,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"In this case, what we do is you"
cs-410_3_11_49,"00:03:41,110","00:03:44,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,and then compute the precision and recall.
cs-410_3_11_50,"00:03:45,460","00:03:50,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So we can compute the overall
cs-410_3_11_51,"00:03:50,480","00:03:55,832",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"how many cases are in true positive,"
cs-410_3_11_52,"00:03:55,832","00:04:01,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"etc, it's computing the values"
cs-410_3_11_53,"00:04:01,660","00:04:04,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,and then we can compute the precision and
cs-410_3_11_54,"00:04:06,060","00:04:10,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"In contrast, in macro-averaging, we're"
cs-410_3_11_55,"00:04:10,296","00:04:16,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,And then aggregate over these categories
cs-410_3_11_56,"00:04:16,070","00:04:19,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,then aggregate all the documents but
cs-410_3_11_57,"00:04:21,130","00:04:24,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,Now this would be very similar to
cs-410_3_11_58,"00:04:24,660","00:04:26,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"used earlier, and"
cs-410_3_11_59,"00:04:26,390","00:04:31,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,one problem here of course to treat all
cs-410_3_11_60,"00:04:32,400","00:04:34,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And this may not be desirable.
cs-410_3_11_61,"00:04:36,310","00:04:39,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,But it may be a property for
cs-410_3_11_62,"00:04:39,160","00:04:45,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"especially if we associate the, for"
cs-410_3_11_63,"00:04:45,570","00:04:50,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"Then we can actually compute for example,"
cs-410_3_11_64,"00:04:50,090","00:04:55,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,Where you associate the different cost or
cs-410_3_11_65,"00:04:56,210","00:04:59,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,so there could be variations of these
cs-410_3_11_66,"00:04:59,620","00:05:06,398",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,But in general macro average tends to
cs-410_3_11_67,"00:05:06,398","00:05:13,889",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,just because it might reflect the need for
cs-410_3_11_68,"00:05:14,890","00:05:20,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,on each category or performance on each
cs-410_3_11_69,"00:05:20,620","00:05:27,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"But macro averaging and micro averaging,"
cs-410_3_11_70,"00:05:27,210","00:05:32,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,and you might see both reported in
cs-410_3_11_71,"00:05:32,780","00:05:36,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,Also sometimes categorization
cs-410_3_11_72,"00:05:36,750","00:05:39,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,be evaluated from ranking prospective.
cs-410_3_11_73,"00:05:40,400","00:05:43,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,And this is because categorization
cs-410_3_11_74,"00:05:43,990","00:05:49,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,often indeed passed it to a human for
cs-410_3_11_75,"00:05:49,610","00:05:53,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,"For example, it might be passed"
cs-410_3_11_76,"00:05:53,300","00:05:58,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"For example, news articles can be tempted"
cs-410_3_11_77,"00:05:58,810","00:06:01,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,then human editors would
cs-410_3_11_78,"00:06:02,680","00:06:07,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,And all the email messages might be
cs-410_3_11_79,"00:06:07,500","00:06:09,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,handling in the help desk.
cs-410_3_11_80,"00:06:09,890","00:06:14,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And in such a case the categorizations
cs-410_3_11_81,"00:06:14,090","00:06:18,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,the task for
cs-410_3_11_82,"00:06:19,690","00:06:25,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"So, in this case the results"
cs-410_3_11_83,"00:06:26,370","00:06:32,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,and if the system can't give a score
cs-410_3_11_84,"00:06:32,450","00:06:39,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,confidence then we can use the scores
cs-410_3_11_85,"00:06:39,830","00:06:44,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"then evaluate the results as a rank list,"
cs-410_3_11_86,"00:06:44,660","00:06:47,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,Evaluation where you rank
cs-410_3_11_87,"00:06:49,040","00:06:53,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,So for example a discovery of
cs-410_3_11_88,"00:06:55,790","00:07:00,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,based on ranking emails for
cs-410_3_11_89,"00:07:00,140","00:07:04,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And this is useful if you want people
cs-410_3_11_90,"00:07:04,660","00:07:05,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"spam, right?"
cs-410_3_11_91,"00:07:05,770","00:07:10,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,The person would then take
cs-410_3_11_92,"00:07:10,170","00:07:14,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,then verify whether this is indeed a spam.
cs-410_3_11_93,"00:07:14,770","00:07:19,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,So to reflect the utility for
cs-410_3_11_94,"00:07:19,180","00:07:23,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,better to evaluate Ranking Chris and this
cs-410_3_11_95,"00:07:25,020","00:07:27,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,And in such a case often
cs-410_3_11_96,"00:07:27,650","00:07:31,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,better formulated as a ranking problem
cs-410_3_11_97,"00:07:31,810","00:07:35,545",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"So for example, ranking documents in"
cs-410_3_11_98,"00:07:35,545","00:07:39,255",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"as a binary categorization problem,"
cs-410_3_11_99,"00:07:39,255","00:07:43,505",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,are useful to users from those that
cs-410_3_11_100,"00:07:43,505","00:07:47,045",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"frame this as a ranking problem,"
cs-410_3_11_101,"00:07:47,045","00:07:50,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,That's because people tend
cs-410_3_11_102,"00:07:52,160","00:07:56,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,ranking evaluation more reflects
cs-410_3_11_103,"00:07:58,180","00:08:02,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"So to summarize categorization evaluation,"
cs-410_3_11_104,"00:08:02,230","00:08:05,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,first evaluation is always very
cs-410_3_11_105,"00:08:05,220","00:08:06,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,So get it right.
cs-410_3_11_106,"00:08:07,200","00:08:10,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"If you don't get it right,"
cs-410_3_11_107,"00:08:10,120","00:08:14,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,And you might be misled to believe
cs-410_3_11_108,"00:08:14,160","00:08:15,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,which is in fact not true.
cs-410_3_11_109,"00:08:15,810","00:08:17,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,So it's very important to get it right.
cs-410_3_11_110,"00:08:18,880","00:08:22,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Measures must also reflect
cs-410_3_11_111,"00:08:22,270","00:08:24,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,a particular application.
cs-410_3_11_112,"00:08:24,100","00:08:25,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,"For example, in spam filtering and"
cs-410_3_11_113,"00:08:25,760","00:08:29,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,news categorization the results
cs-410_3_11_114,"00:08:30,680","00:08:33,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,So then we would need to
cs-410_3_11_115,"00:08:33,760","00:08:35,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,design measures appropriately.
cs-410_3_11_116,"00:08:36,650","00:08:41,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,We generally need to consider how will the
cs-410_3_11_117,"00:08:41,660","00:08:43,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,and think from a user's perspective.
cs-410_3_11_118,"00:08:43,630","00:08:46,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,What quality is important?
cs-410_3_11_119,"00:08:46,220","00:08:47,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,What aspect of quality is important?
cs-410_3_11_120,"00:08:49,240","00:08:52,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,Sometimes there are trade offs between
cs-410_3_11_121,"00:08:52,440","00:08:57,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,recall and so we need to know for this
cs-410_3_11_122,"00:08:57,610","00:08:58,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,or high precision is more important.
cs-410_3_11_123,"00:08:59,910","00:09:03,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,Ideally we associate the different cost
cs-410_3_11_124,"00:09:03,570","00:09:06,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,And this of course has to be designed
cs-410_3_11_125,"00:09:08,140","00:09:12,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,Some commonly used measures for relative
cs-410_3_11_126,"00:09:12,950","00:09:17,268",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,"Classification accuracy, it's very"
cs-410_3_11_127,"00:09:17,268","00:09:22,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,[INAUDIBLE] preceding [INAUDIBLE]
cs-410_3_11_128,"00:09:22,230","00:09:27,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,"report characterizing performances,"
cs-410_3_11_129,"00:09:27,266","00:09:32,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,[INAUDIBLE] like a [INAUDIBLE] Per
cs-410_3_11_130,"00:09:32,440","00:09:37,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"take a average of all of them, different"
cs-410_3_11_131,"00:09:37,790","00:09:42,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"In general, you want to look at the"
cs-410_3_11_132,"00:09:42,910","00:09:46,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,particular applications some perspectives
cs-410_3_11_133,"00:09:46,970","00:09:50,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,diagnoses and
cs-410_3_11_134,"00:09:50,120","00:09:54,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,It's generally useful to look at
cs-410_3_11_135,"00:09:54,920","00:10:00,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,to see subtle differences between methods
cs-410_3_11_136,"00:10:00,220","00:10:03,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,from which you can obtain sight for
cs-410_3_11_137,"00:10:04,670","00:10:07,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,Finally sometimes ranking
cs-410_3_11_138,"00:10:07,340","00:10:11,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,be careful sometimes categorization has
cs-410_3_11_139,"00:10:11,590","00:10:16,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,and there're machine running methods for
cs-410_3_11_140,"00:10:17,480","00:10:19,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,So here are two suggested readings.
cs-410_3_11_141,"00:10:19,990","00:10:25,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,One is some chapters of this book where
cs-410_3_11_142,"00:10:25,120","00:10:27,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,evaluation measures.
cs-410_3_11_143,"00:10:27,090","00:10:31,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,The second is a paper about
cs-410_3_11_144,"00:10:31,916","00:10:33,759",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,text categorization and
cs-410_3_11_145,"00:10:33,759","00:10:39,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,it also has an excellent discussion of
cs-410_3_11_146,"00:10:39,738","00:10:49,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,[MUSIC]
cs-410_2_11_1,"00:00:07,290","00:00:11,068",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,[SOUND] This lecture is
cs-410_2_11_2,"00:00:11,068","00:00:15,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,Discriminative Classifiers for
cs-410_2_11_3,"00:00:15,610","00:00:18,096",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"So, in this lecture,"
cs-410_2_11_4,"00:00:18,096","00:00:22,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,another Discriminative Classifier called
cs-410_2_11_5,"00:00:22,450","00:00:25,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,Which is a very popular
cs-410_2_11_6,"00:00:25,050","00:00:28,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,it has been also shown to be effective for
cs-410_2_11_7,"00:00:31,350","00:00:34,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"So to introduce this classifier,"
cs-410_2_11_8,"00:00:34,380","00:00:38,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,let's also think about the simple
cs-410_2_11_9,"00:00:38,060","00:00:43,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"We have two topic categories,"
cs-410_2_11_10,"00:00:43,300","00:00:47,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,And we want to classify documents
cs-410_2_11_11,"00:00:47,760","00:00:51,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,we're going to represent again
cs-410_2_11_12,"00:00:53,200","00:00:58,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"Now, the idea of this classifier is"
cs-410_2_11_13,"00:00:59,150","00:01:01,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,here that you'll see and
cs-410_2_11_14,"00:01:01,360","00:01:05,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,it's very similar to what you have
cs-410_2_11_15,"00:01:05,820","00:01:11,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,And we're going to do also say
cs-410_2_11_16,"00:01:11,240","00:01:16,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,value is positive then we're going to
cs-410_2_11_17,"00:01:16,690","00:01:20,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"Otherwise, we're going to"
cs-410_2_11_18,"00:01:20,470","00:01:27,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,So that makes 0 that is the decision
cs-410_2_11_19,"00:01:28,830","00:01:33,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"So, in generally hiding"
cs-410_2_11_20,"00:01:33,990","00:01:37,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,corresponds to a hyper plain.
cs-410_2_11_21,"00:01:38,210","00:01:43,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,Now I've shown you a simple case of two
cs-410_2_11_22,"00:01:43,180","00:01:49,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,X2 and this case this corresponds
cs-410_2_11_23,"00:01:51,220","00:01:55,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"So, this is a line defined by"
cs-410_2_11_24,"00:01:55,980","00:02:00,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"just three parameters here,"
cs-410_2_11_25,"00:02:02,390","00:02:07,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"Now, this line is heading"
cs-410_2_11_26,"00:02:07,320","00:02:13,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"it shows that as we increase X1,"
cs-410_2_11_27,"00:02:13,450","00:02:17,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,So we know that beta one and beta two have
cs-410_2_11_28,"00:02:17,780","00:02:18,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,the other is positive.
cs-410_2_11_29,"00:02:20,800","00:02:26,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,So let's just assume that beta one is
cs-410_2_11_30,"00:02:28,810","00:02:31,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"Now, it's interesting to examine, then,"
cs-410_2_11_31,"00:02:31,250","00:02:34,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,the data instances on
cs-410_2_11_32,"00:02:34,800","00:02:39,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"So, here, the data instance are visualized"
cs-410_2_11_33,"00:02:39,690","00:02:41,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,diamonds for the other class.
cs-410_2_11_34,"00:02:43,140","00:02:49,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,"Now, one question is to take a point"
cs-410_2_11_35,"00:02:49,090","00:02:54,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"what's the value of this expression, or"
cs-410_2_11_36,"00:02:55,350","00:02:57,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So what do you think?
cs-410_2_11_37,"00:02:57,000","00:03:00,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"Basically, we're going to evaluate"
cs-410_2_11_38,"00:03:01,740","00:03:06,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,"And as we said, if this value's positive"
cs-410_2_11_39,"00:03:06,190","00:03:09,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"one, and if it's negative,"
cs-410_2_11_40,"00:03:09,610","00:03:15,343",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"Intuitively, this line separates these two"
cs-410_2_11_41,"00:03:15,343","00:03:19,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,one side would be positive and the points
cs-410_2_11_42,"00:03:19,870","00:03:23,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,Our question is under the assumption
cs-410_2_11_43,"00:03:23,200","00:03:25,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,let's examine a particular
cs-410_2_11_44,"00:03:27,590","00:03:30,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,So what do you think is
cs-410_2_11_45,"00:03:31,610","00:03:37,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"Well, to examine the sine we can"
cs-410_2_11_46,"00:03:37,830","00:03:40,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"And we can compare this with let's say,"
cs-410_2_11_47,"00:03:42,050","00:03:46,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"value on the line, let's see,"
cs-410_2_11_48,"00:03:48,440","00:03:53,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"While they have identical X1, but"
cs-410_2_11_49,"00:03:54,740","00:03:59,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"Now, let's look at the sin"
cs-410_2_11_50,"00:03:59,790","00:04:01,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Well, we know this is a positive."
cs-410_2_11_51,"00:04:02,850","00:04:06,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"So, what that means is"
cs-410_2_11_52,"00:04:06,260","00:04:10,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,this point should be higher
cs-410_2_11_53,"00:04:10,400","00:04:14,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,this point on the line that means
cs-410_2_11_54,"00:04:16,190","00:04:19,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,So we know in general of
cs-410_2_11_55,"00:04:20,960","00:04:25,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,the function's value will be positive and
cs-410_2_11_56,"00:04:25,380","00:04:29,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,you can also verify all the points
cs-410_2_11_57,"00:04:29,380","00:04:31,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,And so this is how this kind
cs-410_2_11_58,"00:04:31,750","00:04:35,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,linear separator can then separate
cs-410_2_11_59,"00:04:37,810","00:04:42,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"So, now the natural question is,"
cs-410_2_11_60,"00:04:42,830","00:04:47,687",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"Now, I've get you one line here"
cs-410_2_11_61,"00:04:47,687","00:04:53,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"And this line, of course, is determined"
cs-410_2_11_62,"00:04:53,190","00:04:55,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,Different coefficients will
cs-410_2_11_63,"00:04:55,210","00:04:58,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"So, we could imagine there are other"
cs-410_2_11_64,"00:04:58,770","00:05:00,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"Gamma, for example,"
cs-410_2_11_65,"00:05:00,630","00:05:04,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,could give us another line that counts
cs-410_2_11_66,"00:05:06,010","00:05:09,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"Of course, there are also lines that won't"
cs-410_2_11_67,"00:05:09,710","00:05:12,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"But, the question is,"
cs-410_2_11_68,"00:05:12,310","00:05:15,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"separate both clauses,"
cs-410_2_11_69,"00:05:15,950","00:05:21,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"In fact, you can imagine, there are many"
cs-410_2_11_70,"00:05:21,740","00:05:27,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,"So, the logistical regression classifier"
cs-410_2_11_71,"00:05:27,310","00:05:33,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,some criteria to determine where this line
cs-410_2_11_72,"00:05:33,060","00:05:36,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,And uses a conditional likelihood
cs-410_2_11_73,"00:05:36,610","00:05:38,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,which line is the best.
cs-410_2_11_74,"00:05:38,310","00:05:41,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,But in SVM we're going to
cs-410_2_11_75,"00:05:41,130","00:05:43,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,determining which line is the best.
cs-410_2_11_76,"00:05:43,500","00:05:44,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"And this time,"
cs-410_2_11_77,"00:05:44,230","00:05:48,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,the criteria is more tied to
cs-410_2_11_78,"00:05:49,460","00:05:56,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,"So, the basic idea is to choose"
cs-410_2_11_79,"00:05:56,120","00:05:57,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,So what is a margin?
cs-410_2_11_80,"00:05:57,180","00:06:03,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"So, I choose some dotted"
cs-410_2_11_81,"00:06:03,540","00:06:09,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,the boundaries of those
cs-410_2_11_82,"00:06:09,020","00:06:13,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And the margin is simply
cs-410_2_11_83,"00:06:13,890","00:06:17,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"the separator, and"
cs-410_2_11_84,"00:06:18,490","00:06:23,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,So you can see the margin of this
cs-410_2_11_85,"00:06:23,830","00:06:25,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,you can also define
cs-410_2_11_86,"00:06:27,020","00:06:31,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,In order for
cs-410_2_11_87,"00:06:31,190","00:06:35,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,it has to be kind of in the middle
cs-410_2_11_88,"00:06:35,700","00:06:40,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,you don't want this separator to
cs-410_2_11_89,"00:06:40,050","00:06:42,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,that in intuition makes a lot of sense.
cs-410_2_11_90,"00:06:44,460","00:06:47,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,So this is basic idea of SVM.
cs-410_2_11_91,"00:06:47,050","00:06:50,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,We're going to choose a linear
cs-410_2_11_92,"00:06:52,130","00:06:55,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Now on this slide,"
cs-410_2_11_93,"00:06:55,450","00:06:58,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,that I'm not going to use beta
cs-410_2_11_94,"00:06:58,460","00:07:03,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"But instead, I'm going to use w although"
cs-410_2_11_95,"00:07:03,740","00:07:05,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,don't be confused here.
cs-410_2_11_96,"00:07:05,370","00:07:09,618",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"W here is actually a width,"
cs-410_2_11_97,"00:07:12,734","00:07:19,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,So I'm also using lowercase b to
cs-410_2_11_98,"00:07:20,030","00:07:24,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,And there are instances do
cs-410_2_11_99,"00:07:24,100","00:07:28,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,I also use the vector form
cs-410_2_11_100,"00:07:28,790","00:07:34,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So we see a transpose of w vector
cs-410_2_11_101,"00:07:35,290","00:07:42,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So b is a bias constant and w is a set of
cs-410_2_11_102,"00:07:42,080","00:07:45,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,We have m features and
cs-410_2_11_103,"00:07:45,260","00:07:46,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,that will represent as a vector.
cs-410_2_11_104,"00:07:47,640","00:07:51,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"And similarly, the data instance here,"
cs-410_2_11_105,"00:07:51,260","00:07:55,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,is represented by also a feature
cs-410_2_11_106,"00:07:55,940","00:07:59,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Xi is a feature value.
cs-410_2_11_107,"00:07:59,100","00:08:04,418",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"For example, word count and"
cs-410_2_11_108,"00:08:04,418","00:08:08,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"Multiply these two vectors together,"
cs-410_2_11_109,"00:08:08,960","00:08:14,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,we get the same form of the linear
cs-410_2_11_110,"00:08:14,335","00:08:16,713",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,It's just a different way
cs-410_2_11_111,"00:08:16,713","00:08:21,267",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,Now I use this way so that it's
cs-410_2_11_112,"00:08:21,267","00:08:24,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,people usually use when
cs-410_2_11_113,"00:08:24,750","00:08:29,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,This way you can better connect the slides
cs-410_2_11_114,"00:08:31,190","00:08:39,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"Okay, so when we maximize"
cs-410_2_11_115,"00:08:39,780","00:08:44,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,it just means the boundary of
cs-410_2_11_116,"00:08:44,730","00:08:49,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"a few data points, and these are the data"
cs-410_2_11_117,"00:08:49,800","00:08:54,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,So here illustrated are two support
cs-410_2_11_118,"00:08:54,600","00:08:56,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,the other class.
cs-410_2_11_119,"00:08:56,220","00:09:00,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,And these quotas define
cs-410_2_11_120,"00:09:00,900","00:09:05,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,you can imagine once we know which
cs-410_2_11_121,"00:09:06,430","00:09:09,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,center separator line will
cs-410_2_11_122,"00:09:09,750","00:09:16,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,So the other data points actually
cs-410_2_11_123,"00:09:16,320","00:09:20,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,And you can see if you change the other
cs-410_2_11_124,"00:09:20,420","00:09:22,905",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"the margin, so"
cs-410_2_11_125,"00:09:22,905","00:09:26,514",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,Mainly affected by
cs-410_2_11_126,"00:09:26,514","00:09:29,705",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,"Sorry, it's mainly affected"
cs-410_2_11_127,"00:09:29,705","00:09:32,639",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,that's why it's called
cs-410_2_11_128,"00:09:32,639","00:09:37,968",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"Okay, so now the next question is,"
cs-410_2_11_129,"00:09:37,968","00:09:42,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,how can we set it up to optimize the line?
cs-410_2_11_130,"00:09:42,730","00:09:47,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,How can we actually find the line or
cs-410_2_11_131,"00:09:47,430","00:09:51,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,Now this is equivalent to
cs-410_2_11_132,"00:09:51,390","00:09:55,779",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"b, because they will determine"
cs-410_2_11_133,"00:09:58,010","00:10:04,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,"So in the simplest case, the linear SVM"
cs-410_2_11_134,"00:10:04,700","00:10:10,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"So again, let's recall that our classifier"
cs-410_2_11_135,"00:10:10,230","00:10:15,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,"have weights for all the features, and the"
cs-410_2_11_136,"00:10:15,980","00:10:21,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,And the classifier will say X is in
cs-410_2_11_137,"00:10:21,040","00:10:23,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,"Otherwise, it's going to say"
cs-410_2_11_138,"00:10:23,950","00:10:27,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"So this is our assumption, our setup."
cs-410_2_11_139,"00:10:27,220","00:10:32,406",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,"So in the linear SVM,"
cs-410_2_11_140,"00:10:32,406","00:10:37,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,values to optimize the margins and
cs-410_2_11_141,"00:10:38,800","00:10:41,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,The training data would be basically
cs-410_2_11_142,"00:10:41,920","00:10:45,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,We have a set of training points
cs-410_2_11_143,"00:10:45,940","00:10:50,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,"then we also know the corresponding label,"
cs-410_2_11_144,"00:10:50,290","00:10:54,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"And here we define y i as two values, but"
cs-410_2_11_145,"00:10:54,310","00:10:58,358",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"these values are not 0, 1 as you"
cs-410_2_11_146,"00:10:58,358","00:11:03,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"positive 1, and they're corresponding to"
cs-410_2_11_147,"00:11:03,990","00:11:08,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,Now you might wonder why we
cs-410_2_11_148,"00:11:08,330","00:11:11,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"1 instead of having -1, 1."
cs-410_2_11_149,"00:11:11,770","00:11:15,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,And this is purely for mathematical
cs-410_2_11_150,"00:11:16,700","00:11:19,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,So the goal of optimization first is
cs-410_2_11_151,"00:11:19,450","00:11:23,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,to make sure the labeling of
cs-410_2_11_152,"00:11:23,700","00:11:28,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"So that just means if y i,"
cs-410_2_11_153,"00:11:28,240","00:11:33,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,"is 1, we would like this"
cs-410_2_11_154,"00:11:33,610","00:11:36,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,And here we just choose
cs-410_2_11_155,"00:11:36,740","00:11:41,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"But if you use another threshold,"
cs-410_2_11_156,"00:11:41,875","00:11:47,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,into the parameter values b and
cs-410_2_11_157,"00:11:48,950","00:11:54,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,"Now if, on the other hand, y i is -1,"
cs-410_2_11_158,"00:11:54,780","00:11:58,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,then we want this classifier
cs-410_2_11_159,"00:11:58,460","00:12:04,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,"in fact a negative value, and we want this"
cs-410_2_11_160,"00:12:04,860","00:12:11,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"Now these are the two different instances,"
cs-410_2_11_161,"00:12:11,110","00:12:13,714",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,How can we combine them together?
cs-410_2_11_162,"00:12:13,714","00:12:18,622",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,Now this is where it's convenient
cs-410_2_11_163,"00:12:18,622","00:12:20,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,"the other category,"
cs-410_2_11_164,"00:12:20,200","00:12:25,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,because it turns out that we can either
cs-410_2_11_165,"00:12:26,832","00:12:32,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,y i multiplied by the classifier value
cs-410_2_11_166,"00:12:33,210","00:12:35,484",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"And obviously when y i is just 1,"
cs-410_2_11_167,"00:12:35,484","00:12:39,968",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,you see this is the same as
cs-410_2_11_168,"00:12:39,968","00:12:48,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,"But when y i is -1, you also see that this"
cs-410_2_11_169,"00:12:48,020","00:12:53,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,So this one actually captures both
cs-410_2_11_170,"00:12:53,060","00:12:56,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,and that's a convenient way of
cs-410_2_11_171,"00:12:56,960","00:12:58,137",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,What's our second goal?
cs-410_2_11_172,"00:12:58,137","00:13:00,414",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,"Well, that's to maximize margin, so"
cs-410_2_11_173,"00:13:00,414","00:13:04,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,we want to ensure that separator
cs-410_2_11_174,"00:13:04,600","00:13:08,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,"But then, among all the cases"
cs-410_2_11_175,"00:13:08,109","00:13:12,172",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,we also would like to choose the separator
cs-410_2_11_176,"00:13:12,172","00:13:18,758",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,Now the margin can be assumed to be
cs-410_2_11_177,"00:13:18,758","00:13:23,777",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,And so
cs-410_2_11_178,"00:13:23,777","00:13:29,893",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,us basically the sum of
cs-410_2_11_179,"00:13:29,893","00:13:35,691",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,So to have a small value for
cs-410_2_11_180,"00:13:35,691","00:13:40,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,it means all the w i's must be small.
cs-410_2_11_181,"00:13:42,440","00:13:45,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,So we've just assumed that
cs-410_2_11_182,"00:13:46,930","00:13:50,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,getting the data on the training
cs-410_2_11_183,"00:13:50,890","00:13:57,649",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,Now we also have the objective that's
cs-410_2_11_184,"00:13:57,649","00:14:03,013",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,and this is simply to minimize
cs-410_2_11_185,"00:14:03,013","00:14:06,251",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=843,and we often denote this by phi of w.
cs-410_2_11_186,"00:14:06,251","00:14:10,616",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,So now you can see this is
cs-410_2_11_187,"00:14:10,616","00:14:15,044",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,"We have some variables to optimize,"
cs-410_2_11_188,"00:14:15,044","00:14:17,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,b and we have some constraints.
cs-410_2_11_189,"00:14:17,540","00:14:18,949",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,These are linear constraints and
cs-410_2_11_190,"00:14:18,949","00:14:22,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=858,the objective function is
cs-410_2_11_191,"00:14:22,380","00:14:25,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,So this a quadratic program
cs-410_2_11_192,"00:14:25,370","00:14:30,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,and there are standard algorithm that
cs-410_2_11_193,"00:14:30,050","00:14:34,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,And once we solve the problem
cs-410_2_11_194,"00:14:34,190","00:14:37,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,And then this would give us
cs-410_2_11_195,"00:14:37,080","00:14:42,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,So we can then use this classifier
cs-410_2_11_196,"00:14:42,160","00:14:47,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,Now the previous formulation did not
cs-410_2_11_197,"00:14:47,190","00:14:50,448",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,but sometimes the data may not
cs-410_2_11_198,"00:14:50,448","00:14:54,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,That means that they may not
cs-410_2_11_199,"00:14:54,690","00:14:59,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,the previous slide where a line
cs-410_2_11_200,"00:14:59,300","00:15:02,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,And what would happen if
cs-410_2_11_201,"00:15:02,850","00:15:04,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,"Well, the principle can stay."
cs-410_2_11_202,"00:15:04,980","00:15:09,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,We want to minimize the training error but
cs-410_2_11_203,"00:15:09,305","00:15:12,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,"But in this case we have a soft margin,"
cs-410_2_11_204,"00:15:12,270","00:15:16,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=912,because the data points may
cs-410_2_11_205,"00:15:17,030","00:15:24,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,So it turns out that we can easily
cs-410_2_11_206,"00:15:24,650","00:15:28,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=924,So what you see here is very similar
cs-410_2_11_207,"00:15:28,090","00:15:31,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,but we have introduced
cs-410_2_11_208,"00:15:31,760","00:15:35,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,And we in fact will have one for
cs-410_2_11_209,"00:15:35,610","00:15:40,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,this is going to model the error
cs-410_2_11_210,"00:15:40,780","00:15:43,245",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,But the optimization problem
cs-410_2_11_211,"00:15:43,245","00:15:44,783",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,"So specifically,"
cs-410_2_11_212,"00:15:44,783","00:15:50,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,you will see we have added something
cs-410_2_11_213,"00:15:50,170","00:15:56,861",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,First we have added some
cs-410_2_11_214,"00:15:56,861","00:16:02,119",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=956,that now we allow a Allow the classifier
cs-410_2_11_215,"00:16:02,119","00:16:06,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,to make some mistakes here.
cs-410_2_11_216,"00:16:06,760","00:16:12,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,"So, this Xi i is allowed an error."
cs-410_2_11_217,"00:16:12,860","00:16:16,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,"If we set Xi i to 0, then we go"
cs-410_2_11_218,"00:16:16,560","00:16:20,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=976,We want every instance to
cs-410_2_11_219,"00:16:20,260","00:16:26,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"But, if we allow this to be non-zero,"
cs-410_2_11_220,"00:16:26,420","00:16:30,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,"In fact, if the length of the Xi i is very"
cs-410_2_11_221,"00:16:30,730","00:16:33,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=990,"So naturally,"
cs-410_2_11_222,"00:16:33,270","00:16:37,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,So we want to then also
cs-410_2_11_223,"00:16:37,570","00:16:41,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,"So, because Xi i needs to be minimized"
cs-410_2_11_224,"00:16:42,940","00:16:46,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1002,"And so, as a result,"
cs-410_2_11_225,"00:16:46,020","00:16:50,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,"we also add more to the original one,"
cs-410_2_11_226,"00:16:50,910","00:16:55,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,by basically ensuring that we not
cs-410_2_11_227,"00:16:55,190","00:16:59,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,"also minimize the errors, as you see here."
cs-410_2_11_228,"00:16:59,130","00:17:02,705",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,Here we simply take a sum
cs-410_2_11_229,"00:17:02,705","00:17:07,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,Each one has a Xi i to model
cs-410_2_11_230,"00:17:07,695","00:17:10,413",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,"And when we combine them together,"
cs-410_2_11_231,"00:17:10,413","00:17:14,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,we basically want to minimize
cs-410_2_11_232,"00:17:16,350","00:17:21,001",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1036,"Now you see there's a parameter C here,"
cs-410_2_11_233,"00:17:21,001","00:17:25,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,the trade-off between minimizing
cs-410_2_11_234,"00:17:25,740","00:17:27,888",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,"If C is set to zero, you can see,"
cs-410_2_11_235,"00:17:27,888","00:17:33,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,we go back to the original object function
cs-410_2_11_236,"00:17:34,340","00:17:38,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,We don't really optimize
cs-410_2_11_237,"00:17:38,368","00:17:43,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,then Xi i can be set to a very large value
cs-410_2_11_238,"00:17:43,730","00:17:46,512",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1063,"That's not very good of course, so"
cs-410_2_11_239,"00:17:46,512","00:17:50,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1066,"C should be set to a non-zero value,"
cs-410_2_11_240,"00:17:50,884","00:17:53,412",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1070,"But when C is set to a very,"
cs-410_2_11_241,"00:17:53,412","00:17:58,143",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1073,we'll see the object of the function will
cs-410_2_11_242,"00:17:58,143","00:18:02,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1078,and so the optimization of margin
cs-410_2_11_243,"00:18:02,420","00:18:06,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1082,"So if that happens, what would happen is"
cs-410_2_11_244,"00:18:07,420","00:18:11,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1087,then we will try to do our best to
cs-410_2_11_245,"00:18:11,420","00:18:14,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,then we're not going to
cs-410_2_11_246,"00:18:14,730","00:18:19,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1094,that affects the generalization factors
cs-410_2_11_247,"00:18:19,270","00:18:20,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,So it's also not good.
cs-410_2_11_248,"00:18:20,548","00:18:28,175",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1100,"So in particular, this parameter C"
cs-410_2_11_249,"00:18:28,175","00:18:32,045",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1108,And this is just like in the case of
cs-410_2_11_250,"00:18:32,045","00:18:34,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,to optimize a number of neighbors.
cs-410_2_11_251,"00:18:34,080","00:18:35,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1114,Here you need to optimize the C.
cs-410_2_11_252,"00:18:35,510","00:18:40,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1115,"And this is, in general,"
cs-410_2_11_253,"00:18:40,510","00:18:43,331",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1120,"Basically, you look at"
cs-410_2_11_254,"00:18:43,331","00:18:47,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,see what value C should be set to in
cs-410_2_11_255,"00:18:49,050","00:18:50,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,"Now with this modification,"
cs-410_2_11_256,"00:18:50,390","00:18:54,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1130,the problem is still quadratic programming
cs-410_2_11_257,"00:18:54,250","00:19:00,003",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1134,algorithm can be actually applied to solve
cs-410_2_11_258,"00:19:02,080","00:19:05,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1142,"Again, once we have obtained"
cs-410_2_11_259,"00:19:05,780","00:19:11,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1145,then we can have classifier that's
cs-410_2_11_260,"00:19:11,360","00:19:13,566",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,So that's the basic idea of SVM.
cs-410_2_11_261,"00:19:16,993","00:19:20,402",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,So to summarize the text
cs-410_2_11_262,"00:19:20,402","00:19:25,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1160,"where we introduce the many methods,"
cs-410_2_11_263,"00:19:25,170","00:19:27,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1165,Some are discriminative methods.
cs-410_2_11_264,"00:19:27,140","00:19:32,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1167,And these tend to perform
cs-410_2_11_265,"00:19:32,230","00:19:37,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,"So there's still no clear winner,"
cs-410_2_11_266,"00:19:37,920","00:19:42,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1177,And the performance might also
cs-410_2_11_267,"00:19:42,460","00:19:44,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1182,different problems.
cs-410_2_11_268,"00:19:44,320","00:19:50,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,And one reason is also because the feature
cs-410_2_11_269,"00:19:52,280","00:19:56,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1192,and these methods all require
cs-410_2_11_270,"00:19:56,470","00:19:59,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1196,"And to design an effective feature set,"
cs-410_2_11_271,"00:19:59,400","00:20:03,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1199,we need domain knowledge and humans
cs-410_2_11_272,"00:20:03,530","00:20:05,608",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1203,although there are new
cs-410_2_11_273,"00:20:05,608","00:20:10,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,algorithm representation learning
cs-410_2_11_274,"00:20:12,640","00:20:18,169",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1212,And another common thing
cs-410_2_11_275,"00:20:18,169","00:20:23,546",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1218,"be performing similarly on the data set,"
cs-410_2_11_276,"00:20:23,546","00:20:28,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1223,but with different mistakes.
cs-410_2_11_277,"00:20:28,220","00:20:30,913",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1228,"And so,"
cs-410_2_11_278,"00:20:30,913","00:20:34,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,then the mistakes they
cs-410_2_11_279,"00:20:34,070","00:20:37,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1234,So that means it's useful to
cs-410_2_11_280,"00:20:37,630","00:20:42,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1237,a particular problem and
cs-410_2_11_281,"00:20:42,690","00:20:49,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1242,because this can improve the robustness
cs-410_2_11_282,"00:20:49,092","00:20:54,192",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1249,So assemble approaches that
cs-410_2_11_283,"00:20:54,192","00:20:59,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1254,methods tend to be more robust and
cs-410_2_11_284,"00:20:59,990","00:21:04,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1259,Most techniques that we introduce
cs-410_2_11_285,"00:21:04,530","00:21:06,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1264,which is a very general method.
cs-410_2_11_286,"00:21:06,990","00:21:10,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1266,So that means that these methods can
cs-410_2_11_287,"00:21:10,975","00:21:12,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1270,categorization problem.
cs-410_2_11_288,"00:21:12,580","00:21:17,554",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1272,As long as we have humans to help
cs-410_2_11_289,"00:21:17,554","00:21:23,493",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1277,"design features, then supervising machine"
cs-410_2_11_290,"00:21:23,493","00:21:29,255",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1283,can be easily applied to those problems
cs-410_2_11_291,"00:21:29,255","00:21:34,431",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1289,allow us to characterize content
cs-410_2_11_292,"00:21:34,431","00:21:38,716",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1294,Or to predict the sum
cs-410_2_11_293,"00:21:38,716","00:21:43,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1298,variables that are associated
cs-410_2_11_294,"00:21:43,250","00:21:47,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1303,"The computers, of course, here are trying"
cs-410_2_11_295,"00:21:47,875","00:21:49,908",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1307,the features provided by human.
cs-410_2_11_296,"00:21:49,908","00:21:53,357",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1309,"And as I said, there are many"
cs-410_2_11_297,"00:21:53,357","00:21:56,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1313,they also optimize different object or
cs-410_2_11_298,"00:21:58,180","00:22:02,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1318,"But in order to achieve good performance,"
cs-410_2_11_299,"00:22:02,240","00:22:03,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,also plenty of training data.
cs-410_2_11_300,"00:22:04,770","00:22:08,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1324,"So as a general rule, and if you can"
cs-410_2_11_301,"00:22:08,870","00:22:13,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1328,"and then provide more training data,"
cs-410_2_11_302,"00:22:13,860","00:22:18,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1333,Performance is often much more
cs-410_2_11_303,"00:22:18,390","00:22:23,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1338,features than by the choice
cs-410_2_11_304,"00:22:23,030","00:22:26,972",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1343,So feature design tends to be more
cs-410_2_11_305,"00:22:26,972","00:22:27,768",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1346,classifier.
cs-410_2_11_306,"00:22:30,844","00:22:34,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1350,"So, how do we design effective features?"
cs-410_2_11_307,"00:22:34,170","00:22:37,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1354,"Well, unfortunately,"
cs-410_2_11_308,"00:22:37,360","00:22:43,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1357,So there's no really much
cs-410_2_11_309,"00:22:43,108","00:22:47,672",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1363,But we can do some analysis of
cs-410_2_11_310,"00:22:47,672","00:22:54,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1367,try to understand what kind of features
cs-410_2_11_311,"00:22:54,400","00:22:59,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1374,"And in general, we can use a lot of domain"
cs-410_2_11_312,"00:23:01,640","00:23:06,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1381,And another way to figure out
cs-410_2_11_313,"00:23:06,180","00:23:10,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1386,to do error analysis on
cs-410_2_11_314,"00:23:10,230","00:23:11,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1390,"You could, for example,"
cs-410_2_11_315,"00:23:11,080","00:23:16,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1391,look at which category tends to be
cs-410_2_11_316,"00:23:16,110","00:23:20,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1396,And you can use a confusion matrix
cs-410_2_11_317,"00:23:20,890","00:23:22,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1400,across categories.
cs-410_2_11_318,"00:23:22,340","00:23:25,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1402,"And then,"
cs-410_2_11_319,"00:23:25,320","00:23:29,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1405,see why the mistake has been made and
cs-410_2_11_320,"00:23:29,780","00:23:35,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1409,And this can allow you to obtain
cs-410_2_11_321,"00:23:35,260","00:23:37,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1415,So error analysis is very
cs-410_2_11_322,"00:23:37,840","00:23:40,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1417,that's where you can get the insights
cs-410_2_11_323,"00:23:42,150","00:23:45,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1422,"And finally, we can leverage this"
cs-410_2_11_324,"00:23:45,220","00:23:48,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1425,"So, for example, feature selection is"
cs-410_2_11_325,"00:23:48,710","00:23:50,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1428,"about, but is very important."
cs-410_2_11_326,"00:23:50,390","00:23:54,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1430,And it has to do with trying to select the
cs-410_2_11_327,"00:23:54,830","00:23:56,276",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1434,train a full classifier.
cs-410_2_11_328,"00:23:56,276","00:24:00,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1436,Sometimes training a classifier will also
cs-410_2_11_329,"00:24:00,900","00:24:01,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1440,values.
cs-410_2_11_330,"00:24:01,419","00:24:04,658",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1441,There are also other ways
cs-410_2_11_331,"00:24:04,658","00:24:07,538",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1444,"Of the model,"
cs-410_2_11_332,"00:24:07,538","00:24:12,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1447,"For example, the SVM actually tries"
cs-410_2_11_333,"00:24:12,870","00:24:16,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1452,"But you can further force some features,"
cs-410_2_11_334,"00:24:16,630","00:24:19,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1456,force to use only a small
cs-410_2_11_335,"00:24:21,080","00:24:25,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1461,There are also techniques for
cs-410_2_11_336,"00:24:25,030","00:24:29,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1465,And that's to reduce a high dimensional
cs-410_2_11_337,"00:24:29,450","00:24:33,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1469,space typically by clustering
cs-410_2_11_338,"00:24:33,150","00:24:38,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1473,So metrics factorization
cs-410_2_11_339,"00:24:38,150","00:24:42,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1478,"such a job, and this is some of the"
cs-410_2_11_340,"00:24:42,860","00:24:44,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1482,the talking models that we'll discuss.
cs-410_2_11_341,"00:24:44,820","00:24:48,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1484,So talking morals like psa or
cs-410_2_11_342,"00:24:48,220","00:24:52,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1488,lda can actually help us reduce
cs-410_2_11_343,"00:24:52,570","00:24:56,331",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1492,Like imagine the words
cs-410_2_11_344,"00:24:56,331","00:25:01,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1496,But the can be matched to the topic
cs-410_2_11_345,"00:25:01,970","00:25:04,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1501,So a document can now be represented
cs-410_2_11_346,"00:25:04,380","00:25:08,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1504,as a vector of just k values
cs-410_2_11_347,"00:25:08,750","00:25:12,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1508,So we can let each topic define one
cs-410_2_11_348,"00:25:12,380","00:25:17,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1512,space instead of the original high
cs-410_2_11_349,"00:25:17,920","00:25:21,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1517,And this is often another way
cs-410_2_11_350,"00:25:21,720","00:25:26,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1521,"Especially, we could also use the"
cs-410_2_11_351,"00:25:26,200","00:25:28,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1526,such low dimensional structures.
cs-410_2_11_352,"00:25:29,850","00:25:36,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1529,"And so, the original worth features"
cs-410_2_11_353,"00:25:36,070","00:25:40,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1536,amazing dimension features or
cs-410_2_11_354,"00:25:40,480","00:25:44,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1540,to provide a multi resolution
cs-410_2_11_355,"00:25:44,810","00:25:49,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1544,Deep learning is a new technique that
cs-410_2_11_356,"00:25:51,190","00:25:54,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1551,It's particularly useful for
cs-410_2_11_357,"00:25:54,890","00:25:59,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1554,So deep learning refers to deep neural
cs-410_2_11_358,"00:25:59,840","00:26:07,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1559,where you can have intermediate
cs-410_2_11_359,"00:26:07,110","00:26:11,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1567,"That it's highly non-linear transpire, and"
cs-410_2_11_360,"00:26:11,570","00:26:17,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1571,some recent events that's allowed us to
cs-410_2_11_361,"00:26:17,220","00:26:23,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1577,And the technique has been shown to be
cs-410_2_11_362,"00:26:23,300","00:26:27,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1583,"computer reasoning, and"
cs-410_2_11_363,"00:26:27,620","00:26:29,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1587,It has shown some promise.
cs-410_2_11_364,"00:26:29,530","00:26:33,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1589,And one important advantage
cs-410_2_11_365,"00:26:34,270","00:26:39,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1594,"relationship with the featured design,"
cs-410_2_11_366,"00:26:39,010","00:26:43,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1599,learn intermediate replantations or
cs-410_2_11_367,"00:26:43,920","00:26:49,193",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1603,And this is very valuable for
cs-410_2_11_368,"00:26:49,193","00:26:51,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1609,for text recalibration.
cs-410_2_11_369,"00:26:51,660","00:26:57,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1611,"Although in text domain, because words are"
cs-410_2_11_370,"00:26:57,390","00:27:01,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1617,because these are human's imaging for
cs-410_2_11_371,"00:27:01,620","00:27:08,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1621,And they are generally sufficient for
cs-410_2_11_372,"00:27:08,160","00:27:11,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1628,If there's a need for
cs-410_2_11_373,"00:27:11,430","00:27:15,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1631,people would have invented a new word.
cs-410_2_11_374,"00:27:15,250","00:27:18,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1635,So because of this we think
cs-410_2_11_375,"00:27:18,320","00:27:22,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1638,text processing tends to be lower than for
cs-410_2_11_376,"00:27:22,610","00:27:26,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1642,And the speech revenue where
cs-410_2_11_377,"00:27:26,490","00:27:29,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1646,where the design that worked as features.
cs-410_2_11_378,"00:27:31,160","00:27:35,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1651,But people only still very promising for
cs-410_2_11_379,"00:27:35,020","00:27:35,857",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1655,complicated tasks.
cs-410_2_11_380,"00:27:35,857","00:27:39,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1655,Like a analysis it has
cs-410_2_11_381,"00:27:41,230","00:27:44,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1661,because it can provide that
cs-410_2_11_382,"00:27:47,030","00:27:50,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1667,Now regarding the training examples.
cs-410_2_11_383,"00:27:50,240","00:27:53,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1670,It's generally hard to get a lot of
cs-410_2_11_384,"00:27:53,940","00:27:54,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1673,human labor.
cs-410_2_11_385,"00:27:56,310","00:27:58,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1676,But there are also some
cs-410_2_11_386,"00:27:58,570","00:28:04,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1678,So one is to assume in some low quality
cs-410_2_11_387,"00:28:04,830","00:28:07,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1684,"So, those can be called"
cs-410_2_11_388,"00:28:07,800","00:28:13,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1687,"For example, if you take reviews from the"
cs-410_2_11_389,"00:28:13,220","00:28:21,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1693,"So, to train a of categorizer,"
cs-410_2_11_390,"00:28:21,250","00:28:24,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1701,And categorize these reviews
cs-410_2_11_391,"00:28:24,860","00:28:31,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1704,Then we could assume five star reviews
cs-410_2_11_392,"00:28:31,570","00:28:33,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1711,One star are negative.
cs-410_2_11_393,"00:28:33,270","00:28:34,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1713,"But of course,"
cs-410_2_11_394,"00:28:34,190","00:28:38,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1714,sometimes even five star reviews will also
cs-410_2_11_395,"00:28:38,520","00:28:43,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1718,"sample is not all of that high quality,"
cs-410_2_11_396,"00:28:45,200","00:28:47,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1725,Another idea is to exploit
cs-410_2_11_397,"00:28:47,970","00:28:50,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1727,there are techniques called
cs-410_2_11_398,"00:28:50,830","00:28:55,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1730,learning techniques that can allow you to
cs-410_2_11_399,"00:28:55,685","00:29:01,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1735,"So, in other case it's easy to see"
cs-410_2_11_400,"00:29:01,070","00:29:03,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1741,both text plus read and
cs-410_2_11_401,"00:29:03,760","00:29:09,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1743,"So you can imagine, if you have a lot of"
cs-410_2_11_402,"00:29:09,220","00:29:15,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1749,then you can actually do clustering
cs-410_2_11_403,"00:29:15,620","00:29:18,088",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1755,And then try to somehow
cs-410_2_11_404,"00:29:18,088","00:29:23,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1758,With the categories defined
cs-410_2_11_405,"00:29:23,230","00:29:26,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1763,where we already know which
cs-410_2_11_406,"00:29:26,390","00:29:31,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1766,So you can in fact use the Algorithm
cs-410_2_11_407,"00:29:31,620","00:29:37,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1771,That would allow you essentially also
cs-410_2_11_408,"00:29:37,390","00:29:39,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1777,You can think of this in another way.
cs-410_2_11_409,"00:29:39,320","00:29:43,804",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1779,"Basically, we can use let's say a to"
cs-410_2_11_410,"00:29:43,804","00:29:48,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1783,classify all of the unlabeled text
cs-410_2_11_411,"00:29:48,480","00:29:54,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1788,assume the high confidence Classification
cs-410_2_11_412,"00:29:54,040","00:29:58,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1794,Then you suddenly have more training
cs-410_2_11_413,"00:29:58,600","00:30:03,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1798,"now know some are labeled as category one,"
cs-410_2_11_414,"00:30:03,450","00:30:06,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1803,All though the label is not
cs-410_2_11_415,"00:30:06,380","00:30:07,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1806,then they can still be useful.
cs-410_2_11_416,"00:30:07,830","00:30:14,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1807,So let's assume they are actually training
cs-410_2_11_417,"00:30:14,720","00:30:19,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1814,with true training examples through
cs-410_2_11_418,"00:30:19,940","00:30:22,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1819,And so this idea is very powerful.
cs-410_2_11_419,"00:30:23,980","00:30:28,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1823,When the enabled data and
cs-410_2_11_420,"00:30:28,280","00:30:32,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1828,we might need to use other advanced
cs-410_2_11_421,"00:30:32,410","00:30:35,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1832,called domain adaptation or
cs-410_2_11_422,"00:30:35,150","00:30:37,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1835,This is when we can
cs-410_2_11_423,"00:30:37,580","00:30:42,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1837,Borrow some training examples from
cs-410_2_11_424,"00:30:42,450","00:30:44,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1842,"Or, from a categorization password"
cs-410_2_11_425,"00:30:46,780","00:30:52,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1846,that follow very different distribution
cs-410_2_11_426,"00:30:52,130","00:30:54,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1852,"But basically,"
cs-410_2_11_427,"00:30:54,190","00:30:57,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1854,then we need to be careful and
cs-410_2_11_428,"00:30:57,640","00:31:02,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1857,"But yet, we can still want to use some"
cs-410_2_11_429,"00:31:02,300","00:31:07,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1862,"So for example,"
cs-410_2_11_430,"00:31:07,270","00:31:12,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1867,give you Effective plus y for
cs-410_2_11_431,"00:31:12,410","00:31:19,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1872,But you can still learn something from
cs-410_2_11_432,"00:31:19,490","00:31:25,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1879,So there are mission learning techniques
cs-410_2_11_433,"00:31:25,470","00:31:30,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1885,Here's a suggested reading where you
cs-410_2_11_434,"00:31:30,259","00:31:33,271",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1890,more of the methods is
cs-410_2_11_435,"00:31:33,271","00:31:43,271",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1893,[MUSIC]
cs-410_6_6_1,"00:00:00,012","00:00:03,574",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_6_6_2,"00:00:09,704","00:00:11,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,There are some interesting challenges
cs-410_6_6_3,"00:00:11,535","00:00:15,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,in threshold for
cs-410_6_6_4,"00:00:15,015","00:00:19,965",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,So here I show the historical data that
cs-410_6_6_5,"00:00:19,965","00:00:25,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,so you can see the scores and
cs-410_6_6_6,"00:00:25,195","00:00:30,682",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,So the first one has a score of 36.5 and
cs-410_6_6_7,"00:00:30,682","00:00:34,852",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,The second one is not relevant and
cs-410_6_6_8,"00:00:34,852","00:00:37,902",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,"Of course, we have a lot of documents for"
cs-410_6_6_9,"00:00:37,902","00:00:40,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,because we have never
cs-410_6_6_10,"00:00:40,910","00:00:42,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"So as you can see here,"
cs-410_6_6_11,"00:00:42,060","00:00:47,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,we only see the judgements of
cs-410_6_6_12,"00:00:47,380","00:00:52,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"So this is not a random sample,"
cs-410_6_6_13,"00:00:52,100","00:00:56,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"It's kind of biased, so that creates"
cs-410_6_6_14,"00:00:58,366","00:01:04,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"Secondly, there are in general very little"
cs-410_6_6_15,"00:01:04,230","00:01:07,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,so it's also challenging for
cs-410_6_6_16,"00:01:07,920","00:01:12,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,typically they require more training data.
cs-410_6_6_17,"00:01:13,830","00:01:17,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,And in the extreme case at
cs-410_6_6_18,"00:01:17,560","00:01:18,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,labeled data as well.
cs-410_6_6_19,"00:01:18,550","00:01:20,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"The system there has to make a decision,"
cs-410_6_6_20,"00:01:20,940","00:01:24,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,so that's a very difficult
cs-410_6_6_21,"00:01:24,440","00:01:29,348",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,"Finally, there is also this issue of"
cs-410_6_6_22,"00:01:29,348","00:01:34,987",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"Now, this means we also want"
cs-410_6_6_23,"00:01:34,987","00:01:39,983",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,space a little bit and
cs-410_6_6_24,"00:01:39,983","00:01:45,899",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,interested in documents that
cs-410_6_6_25,"00:01:45,899","00:01:51,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,"So in other words, we're going to"
cs-410_6_6_26,"00:01:51,330","00:01:54,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,by testing whether the user might be
cs-410_6_6_27,"00:01:56,550","00:02:00,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,currently are not matching
cs-410_6_6_28,"00:02:01,660","00:02:02,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,So how do we do that?
cs-410_6_6_29,"00:02:02,650","00:02:06,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"Well, we could lower the threshold"
cs-410_6_6_30,"00:02:06,580","00:02:11,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,deliver some near misses to the user
cs-410_6_6_31,"00:02:13,160","00:02:18,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,to see how the user would
cs-410_6_6_32,"00:02:20,540","00:02:24,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,"And this is a tradeoff, because on"
cs-410_6_6_33,"00:02:24,920","00:02:28,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"on the other hand,"
cs-410_6_6_34,"00:02:28,130","00:02:31,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,because then you will over
cs-410_6_6_35,"00:02:31,960","00:02:36,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,So exploitation means you would
cs-410_6_6_36,"00:02:36,310","00:02:39,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,Let's say you know the user is
cs-410_6_6_37,"00:02:39,790","00:02:42,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"you don't want to deviate that much, but"
cs-410_6_6_38,"00:02:42,950","00:02:47,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,if you don't deviate at all then you don't
cs-410_6_6_39,"00:02:47,220","00:02:50,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,You might miss opportunity to learn
cs-410_6_6_40,"00:02:51,930","00:02:53,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,So this is a dilemma.
cs-410_6_6_41,"00:02:54,790","00:02:57,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,And that's also a difficulty
cs-410_6_6_42,"00:02:58,890","00:03:00,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"Now, how do we solve these problems?"
cs-410_6_6_43,"00:03:00,320","00:03:04,499",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"In general, I think one can use the"
cs-410_6_6_44,"00:03:04,499","00:03:09,611",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And this strategy is basically to optimize
cs-410_6_6_45,"00:03:09,611","00:03:12,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,just as you have seen
cs-410_6_6_46,"00:03:12,480","00:03:16,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"Right, so you can just compute"
cs-410_6_6_47,"00:03:16,610","00:03:18,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,each candidate score threshold.
cs-410_6_6_48,"00:03:18,950","00:03:21,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"Pretend that, what if I cut at this point."
cs-410_6_6_49,"00:03:21,830","00:03:27,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,What if I cut at the different scoring
cs-410_6_6_50,"00:03:27,090","00:03:28,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,What's utility?
cs-410_6_6_51,"00:03:28,900","00:03:34,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"Since these are training data,"
cs-410_6_6_52,"00:03:34,030","00:03:38,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"and we know that relevant status,"
cs-410_6_6_53,"00:03:38,440","00:03:43,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,relevant status based on
cs-410_6_6_54,"00:03:43,220","00:03:47,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,So then we can just choose the threshold
cs-410_6_6_55,"00:03:47,190","00:03:49,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,on the training data.
cs-410_6_6_56,"00:03:49,810","00:03:55,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"But this of course, doesn't account for"
cs-410_6_6_57,"00:03:56,870","00:04:00,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,And there is also the difficulty of
cs-410_6_6_58,"00:04:01,530","00:04:07,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"So, in general, we can only get the upper"
cs-410_6_6_59,"00:04:07,300","00:04:13,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,because the threshold might
cs-410_6_6_60,"00:04:13,190","00:04:17,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"So, it's possible that this could"
cs-410_6_6_61,"00:04:17,115","00:04:18,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,interesting to the user.
cs-410_6_6_62,"00:04:19,790","00:04:21,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,So how do we solve this problem?
cs-410_6_6_63,"00:04:21,400","00:04:22,896",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"Well, we generally, and"
cs-410_6_6_64,"00:04:22,896","00:04:27,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,as I said we can low with this
cs-410_6_6_65,"00:04:27,190","00:04:30,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So here's on particular approach
cs-410_6_6_66,"00:04:30,760","00:04:32,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,So the idea is falling.
cs-410_6_6_67,"00:04:32,680","00:04:37,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,So here I show a ranked list of all the
cs-410_6_6_68,"00:04:37,400","00:04:40,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"far, and"
cs-410_6_6_69,"00:04:40,610","00:04:44,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"And on the y axis we show the utility,"
cs-410_6_6_70,"00:04:44,680","00:04:48,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,how you specify the coefficients
cs-410_6_6_71,"00:04:48,670","00:04:53,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"we can then imagine, that depending on the"
cs-410_6_6_72,"00:04:54,930","00:04:59,828",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,Suppose I cut at this position and
cs-410_6_6_73,"00:04:59,828","00:05:06,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"For example,"
cs-410_6_6_74,"00:05:06,690","00:05:11,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"The optimal point,"
cs-410_6_6_75,"00:05:11,640","00:05:16,355",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,when it will achieve the maximum utility
cs-410_6_6_76,"00:05:17,510","00:05:23,097",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,And there is also zero utility threshold.
cs-410_6_6_77,"00:05:23,097","00:05:27,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,You can see at this cutoff
cs-410_6_6_78,"00:05:27,720","00:05:28,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,What does that mean?
cs-410_6_6_79,"00:05:28,740","00:05:34,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,That means if I lower the threshold
cs-410_6_6_80,"00:05:34,250","00:05:41,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,The utility would be lower but
cs-410_6_6_81,"00:05:41,305","00:05:45,835",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,So it's not as high as
cs-410_6_6_82,"00:05:45,835","00:05:51,492",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,But it gives us as a safe point
cs-410_6_6_83,"00:05:51,492","00:05:56,052",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"as I have explained, it's desirable"
cs-410_6_6_84,"00:05:56,052","00:06:00,622",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,So it's desirable to lower the threshold
cs-410_6_6_85,"00:06:00,622","00:06:04,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,"So that means, in general, we want to set"
cs-410_6_6_86,"00:06:04,850","00:06:06,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,Let's say we can use the alpha to control
cs-410_6_6_87,"00:06:08,310","00:06:13,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,the deviation from
cs-410_6_6_88,"00:06:13,210","00:06:16,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,So you can see the formula of the
cs-410_6_6_89,"00:06:16,570","00:06:21,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,of the zero utility threshold and
cs-410_6_6_90,"00:06:22,490","00:06:25,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"Now, the question is,"
cs-410_6_6_91,"00:06:27,420","00:06:31,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,And when should we deviate more
cs-410_6_6_92,"00:06:33,720","00:06:38,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"Well, this can depend on multiple factors,"
cs-410_6_6_93,"00:06:38,450","00:06:43,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,encourage this threshold
cs-410_6_6_94,"00:06:43,880","00:06:48,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,"up to the zero point, and"
cs-410_6_6_95,"00:06:48,630","00:06:52,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,we're not going to necessarily reach
cs-410_6_6_96,"00:06:52,990","00:06:57,947",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Rather, we're going to use other"
cs-410_6_6_97,"00:06:57,947","00:07:01,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,this specifically is as follows.
cs-410_6_6_98,"00:07:01,030","00:07:06,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,So there will be a beta parameter to
cs-410_6_6_99,"00:07:06,680","00:07:12,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,threshold and this can be based on can
cs-410_6_6_100,"00:07:12,000","00:07:17,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"to the training data let's say, and so"
cs-410_6_6_101,"00:07:17,960","00:07:20,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,But what's more interesting
cs-410_6_6_102,"00:07:20,500","00:07:25,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"Here, and you can see in this formula,"
cs-410_6_6_103,"00:07:25,510","00:07:31,134",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,gamma is controlling the inference
cs-410_6_6_104,"00:07:31,134","00:07:36,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,of the number of examples
cs-410_6_6_105,"00:07:36,210","00:07:43,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,So you can see in this formula as N which
cs-410_6_6_106,"00:07:43,340","00:07:50,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"becomes bigger, then it would"
cs-410_6_6_107,"00:07:50,820","00:07:55,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"In other words, when these very"
cs-410_6_6_108,"00:07:55,140","00:07:59,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,And that just means if we have seen few
cs-410_6_6_109,"00:07:59,630","00:08:04,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,examples we're not sure whether we
cs-410_6_6_110,"00:08:04,330","00:08:09,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So we need to explore but as we have
cs-410_6_6_111,"00:08:09,590","00:08:13,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,many that have we feel that we
cs-410_6_6_112,"00:08:13,510","00:08:17,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,So this gives us a beta gamma for
cs-410_6_6_113,"00:08:17,950","00:08:21,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,The more examples we have seen
cs-410_6_6_114,"00:08:21,500","00:08:25,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,So the threshold would be closer
cs-410_6_6_115,"00:08:25,960","00:08:28,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,that's the basic idea of this approach.
cs-410_6_6_116,"00:08:28,490","00:08:34,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,This approach actually has been working
cs-410_6_6_117,"00:08:34,120","00:08:36,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,particularly effective.
cs-410_6_6_118,"00:08:36,030","00:08:42,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,And also can work on arbitrary utility
cs-410_6_6_119,"00:08:43,710","00:08:48,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,And explicitly addresses
cs-410_6_6_120,"00:08:48,020","00:08:53,234",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,it kind of uses the zero utility
cs-410_6_6_121,"00:08:53,234","00:08:56,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,exploration-exploitation tradeoff.
cs-410_6_6_122,"00:08:56,810","00:09:02,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,We're not never going to explore
cs-410_6_6_123,"00:09:02,770","00:09:05,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"So if you take the analogy of gambling,"
cs-410_6_6_124,"00:09:05,530","00:09:08,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,you don't want to risk on losing money.
cs-410_6_6_125,"00:09:08,950","00:09:12,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"So it's a safe spend, really"
cs-410_6_6_126,"00:09:13,270","00:09:18,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,"And the problem is of course,"
cs-410_6_6_127,"00:09:18,250","00:09:23,643",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,the zero utility lower boundary is also
cs-410_6_6_128,"00:09:23,643","00:09:28,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"course, more advance in machine learning"
cs-410_6_6_129,"00:09:28,855","00:09:33,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,solving this problems and
cs-410_6_6_130,"00:09:35,225","00:09:41,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"So to summarize, there are two"
cs-410_6_6_131,"00:09:41,550","00:09:47,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"filtering systems, one is content based,"
cs-410_6_6_132,"00:09:47,070","00:09:51,302",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,and the other is collaborative filtering
cs-410_6_6_133,"00:09:51,302","00:09:56,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,We've covered content-based
cs-410_6_6_134,"00:09:56,710","00:09:59,566",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"In the next lecture, we will talk"
cs-410_6_6_135,"00:09:59,566","00:10:07,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"In content-based filtering system,"
cs-410_6_6_136,"00:10:07,030","00:10:11,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,several problems relative to
cs-410_6_6_137,"00:10:11,750","00:10:17,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,And such a system can actually be
cs-410_6_6_138,"00:10:17,130","00:10:22,978",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,by adding a threshold mechanism and
cs-410_6_6_139,"00:10:22,978","00:10:28,011",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,allow the system to learn from
cs-410_6_6_140,"00:10:30,357","00:10:40,357",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,[MUSIC]
cs-410_7_6_1,"00:00:07,400","00:00:09,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_7_6_2,"00:00:11,540","00:00:16,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,In this lecture we're going to continue
cs-410_7_6_3,"00:00:16,250","00:00:21,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In particular, we're going to look at"
cs-410_7_6_4,"00:00:21,390","00:00:25,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,You have seen this slide before when
cs-410_7_6_5,"00:00:25,710","00:00:30,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"answer the basic question,"
cs-410_7_6_6,"00:00:30,310","00:00:31,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"In the previous lecture,"
cs-410_7_6_7,"00:00:31,290","00:00:36,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"we looked at the item similarity,"
cs-410_7_6_8,"00:00:36,180","00:00:39,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,"In this lecture, we're going to"
cs-410_7_6_9,"00:00:39,580","00:00:42,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"This is a different strategy,"
cs-410_7_6_10,"00:00:44,090","00:00:45,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"So first, what is collaborative filtering?"
cs-410_7_6_11,"00:00:47,460","00:00:49,525",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,It is to make filtering decisions for
cs-410_7_6_12,"00:00:49,525","00:00:52,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,individual user based on
cs-410_7_6_13,"00:00:54,240","00:00:58,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,And that is to say we will
cs-410_7_6_14,"00:00:58,000","00:01:02,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,preferences from that
cs-410_7_6_15,"00:01:02,080","00:01:04,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,So the general idea is the following.
cs-410_7_6_16,"00:01:04,530","00:01:11,693",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"Given a user u, we're going to first"
cs-410_7_6_17,"00:01:11,693","00:01:15,581",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,And then we're going to
cs-410_7_6_18,"00:01:15,581","00:01:20,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,based on the preferences of
cs-410_7_6_19,"00:01:22,390","00:01:26,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"Now, the user similarity here can"
cs-410_7_6_20,"00:01:26,960","00:01:29,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,the preferences on a common set of items.
cs-410_7_6_21,"00:01:31,070","00:01:36,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,Now here you can see the exact
cs-410_7_6_22,"00:01:36,020","00:01:40,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,We're going to look at the only the
cs-410_7_6_23,"00:01:41,730","00:01:44,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,So this means this
cs-410_7_6_24,"00:01:44,120","00:01:49,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"It can be applied to any items,"
cs-410_7_6_25,"00:01:49,450","00:01:53,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,So this approach would work well
cs-410_7_6_26,"00:01:53,700","00:01:59,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,"First, users with the same interest"
cs-410_7_6_27,"00:01:59,230","00:02:03,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,"Second, the users with similar preferences"
cs-410_7_6_28,"00:02:03,570","00:02:08,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"So for example, if the interest of"
cs-410_7_6_29,"00:02:08,650","00:02:12,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,then we can infer the user
cs-410_7_6_30,"00:02:14,280","00:02:17,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,So those who are interested in
cs-410_7_6_31,"00:02:17,270","00:02:19,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,probably all favor SIGIR papers.
cs-410_7_6_32,"00:02:19,840","00:02:21,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,That's an assumption that we make.
cs-410_7_6_33,"00:02:21,880","00:02:23,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"And if this assumption is true,"
cs-410_7_6_34,"00:02:23,440","00:02:27,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,then it would help collaborative
cs-410_7_6_35,"00:02:27,490","00:02:34,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,We can also assume that if we see
cs-410_7_6_36,"00:02:34,055","00:02:38,215",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,then we can infer their interest
cs-410_7_6_37,"00:02:38,215","00:02:43,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"So in these simple examples,"
cs-410_7_6_38,"00:02:43,025","00:02:48,492",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,in many cases such assumption
cs-410_7_6_39,"00:02:48,492","00:02:52,896",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,So another assumption we have to make
cs-410_7_6_40,"00:02:52,896","00:02:56,012",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,number of user preferences
cs-410_7_6_41,"00:02:56,012","00:03:00,722",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"So for example, if you see a lot"
cs-410_7_6_42,"00:03:00,722","00:03:03,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,those indicate their
cs-410_7_6_43,"00:03:03,160","00:03:06,832",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"And if you have a lot of such data,"
cs-410_7_6_44,"00:03:06,832","00:03:08,689",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,filtering can be very effective.
cs-410_7_6_45,"00:03:09,960","00:03:14,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"If not, there will be a problem, and"
cs-410_7_6_46,"00:03:14,680","00:03:18,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,That means you don't have many
cs-410_7_6_47,"00:03:18,640","00:03:23,722",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,the system could not fully take advantage
cs-410_7_6_48,"00:03:23,722","00:03:28,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,So let's look at the filtering
cs-410_7_6_49,"00:03:30,340","00:03:33,791",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"So this picture shows that we are,"
cs-410_7_6_50,"00:03:33,791","00:03:38,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"in general, considering a lot of users and"
cs-410_7_6_51,"00:03:38,075","00:03:42,956",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"we're showing m users here, so U1 through."
cs-410_7_6_52,"00:03:42,956","00:03:46,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,And we're also considering
cs-410_7_6_53,"00:03:46,040","00:03:49,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,Let's say n objects in
cs-410_7_6_54,"00:03:49,870","00:03:55,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,And then we will assume that
cs-410_7_6_55,"00:03:55,330","00:04:01,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,objects and the user could for
cs-410_7_6_56,"00:04:01,510","00:04:06,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"For example, those items could be movies,"
cs-410_7_6_57,"00:04:06,490","00:04:10,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,then the users would give
cs-410_7_6_58,"00:04:10,500","00:04:14,829",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,So what you see here is that we have
cs-410_7_6_59,"00:04:14,829","00:04:16,231",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,some combinations.
cs-410_7_6_60,"00:04:16,231","00:04:21,751",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"So some users have watched some movies,"
cs-410_7_6_61,"00:04:21,751","00:04:26,075",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,they obviously won't be able
cs-410_7_6_62,"00:04:26,075","00:04:30,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,some users may actually
cs-410_7_6_63,"00:04:30,040","00:04:34,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,So this is in general a small symmetrics.
cs-410_7_6_64,"00:04:34,410","00:04:38,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,So many items and
cs-410_7_6_65,"00:04:39,160","00:04:46,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,And what's interesting here is we
cs-410_7_6_66,"00:04:46,070","00:04:51,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,of an element in this matrix
cs-410_7_6_67,"00:04:51,780","00:04:56,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,And that's after the essential question
cs-410_7_6_68,"00:04:56,610","00:04:59,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,we assume there's an unknown
cs-410_7_6_69,"00:04:59,950","00:05:04,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,That would map a pair of user and
cs-410_7_6_70,"00:05:04,400","00:05:07,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,And we have observed the sum
cs-410_7_6_71,"00:05:08,960","00:05:14,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,And we want to infer the value
cs-410_7_6_72,"00:05:14,296","00:05:20,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,other pairs that don't have
cs-410_7_6_73,"00:05:20,168","00:05:26,198",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,So this is very similar to other
cs-410_7_6_74,"00:05:26,198","00:05:31,784",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,know the values of the function
cs-410_7_6_75,"00:05:31,784","00:05:37,384",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,And we hope to predict the values of
cs-410_7_6_76,"00:05:37,384","00:05:40,344",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,this is a function approximation.
cs-410_7_6_77,"00:05:40,344","00:05:47,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,And how can we pick out the function
cs-410_7_6_78,"00:05:47,440","00:05:50,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,So this is the setup.
cs-410_7_6_79,"00:05:50,230","00:05:54,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,Now there are many approaches
cs-410_7_6_80,"00:05:54,680","00:06:00,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"In fact,"
cs-410_7_6_81,"00:06:00,415","00:06:09,095",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,reason that there are special
cs-410_7_6_82,"00:06:10,419","00:06:15,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,major conference devoted to the problem.
cs-410_7_6_83,"00:06:15,730","00:06:20,199",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,[MUSIC]
cs-410_9_6_1,"00:00:00,012","00:00:07,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_9_6_2,"00:00:07,878","00:00:12,848",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,to summarize our discussion of
cs-410_9_6_3,"00:00:12,848","00:00:16,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,the filtering task for
cs-410_9_6_4,"00:00:16,640","00:00:21,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"in some other sense,"
cs-410_9_6_5,"00:00:21,020","00:00:24,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,So it's easy because
cs-410_9_6_6,"00:00:24,230","00:00:30,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,In this case the system takes initiative
cs-410_9_6_7,"00:00:30,300","00:00:33,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"The user doesn't really make any effort,"
cs-410_9_6_8,"00:00:33,230","00:00:36,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,any recommendation is better than nothing.
cs-410_9_6_9,"00:00:36,100","00:00:41,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,All right.
cs-410_9_6_10,"00:00:41,710","00:00:44,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,items or useless documents.
cs-410_9_6_11,"00:00:44,180","00:00:47,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,If you can recommend
cs-410_9_6_12,"00:00:47,220","00:00:52,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"users generally will appreciate it,"
cs-410_9_6_13,"00:00:52,390","00:00:56,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"However, filtering is actually much harder"
cs-410_9_6_14,"00:00:56,810","00:01:01,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,make a binary decision and you can't
cs-410_9_6_15,"00:01:01,860","00:01:06,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,then you're going to see whether
cs-410_9_6_16,"00:01:06,520","00:01:10,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,You have to make a decision
cs-410_9_6_17,"00:01:10,040","00:01:11,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,Think about news filtering.
cs-410_9_6_18,"00:01:11,260","00:01:15,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,As soon as you see the news enough
cs-410_9_6_19,"00:01:15,060","00:01:16,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,interesting to the user.
cs-410_9_6_20,"00:01:16,780","00:01:21,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"If you wait for a few days, well, even if"
cs-410_9_6_21,"00:01:21,190","00:01:26,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"the most relevant news, the utility is"
cs-410_9_6_22,"00:01:28,160","00:01:32,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Another reason why it's hard
cs-410_9_6_23,"00:01:32,140","00:01:34,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,if you think of this
cs-410_9_6_24,"00:01:34,620","00:01:36,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"Collaborative filtering, for"
cs-410_9_6_25,"00:01:36,010","00:01:41,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"example, is purely based on"
cs-410_9_6_26,"00:01:41,030","00:01:48,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,So if you don't have many ratings there's
cs-410_9_6_27,"00:01:48,120","00:01:51,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,And yeah I just mentioned
cs-410_9_6_28,"00:01:51,470","00:01:54,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"This is actually a very serious,"
cs-410_9_6_29,"00:01:54,450","00:01:59,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,But of course there are strategies that
cs-410_9_6_30,"00:02:00,680","00:02:04,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,and there are different strategies that
cs-410_9_6_31,"00:02:04,930","00:02:09,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"You can use, for example, more user"
cs-410_9_6_32,"00:02:09,620","00:02:14,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,instead of using the preferences
cs-410_9_6_33,"00:02:14,470","00:02:19,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,items give me additional information
cs-410_9_6_34,"00:02:21,110","00:02:26,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,And we also talk about two strategies for
cs-410_9_6_35,"00:02:26,840","00:02:30,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,One is content-based where
cs-410_9_6_36,"00:02:30,140","00:02:34,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,is collaborative filtering where
cs-410_9_6_37,"00:02:34,670","00:02:37,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,And they obviously can be
cs-410_9_6_38,"00:02:37,990","00:02:41,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,You can imagine they generally
cs-410_9_6_39,"00:02:41,480","00:02:46,166",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,So that would give us a hybrid
cs-410_9_6_40,"00:02:46,166","00:02:52,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And we also could recall that we talked
cs-410_9_6_41,"00:02:52,620","00:02:58,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,about push versus pull as two strategies
cs-410_9_6_42,"00:02:58,470","00:03:03,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,And recommender system easy to
cs-410_9_6_43,"00:03:03,110","00:03:06,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,search engines are serving
cs-410_9_6_44,"00:03:06,650","00:03:09,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"Obviously the two should be combined,"
cs-410_9_6_45,"00:03:09,740","00:03:13,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,The two have a system
cs-410_9_6_46,"00:03:13,400","00:03:16,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,with multiple mode information access.
cs-410_9_6_47,"00:03:16,600","00:03:22,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,So in the future we could anticipate such
cs-410_9_6_48,"00:03:22,870","00:03:27,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"And either,"
cs-410_9_6_49,"00:03:27,570","00:03:33,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,there are a lot of new algorithms
cs-410_9_6_50,"00:03:33,740","00:03:39,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,In particular those new algorithms tend
cs-410_9_6_51,"00:03:39,070","00:03:42,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,Now the context here could be
cs-410_9_6_52,"00:03:42,850","00:03:44,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,could also be the context of the user.
cs-410_9_6_53,"00:03:44,920","00:03:45,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,Items.
cs-410_9_6_54,"00:03:45,750","00:03:47,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,The items are not the isolated.
cs-410_9_6_55,"00:03:47,570","00:03:50,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,They're connected in many ways.
cs-410_9_6_56,"00:03:50,290","00:03:54,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,The users might form
cs-410_9_6_57,"00:03:54,590","00:03:58,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,so there's a rich context there
cs-410_9_6_58,"00:03:59,980","00:04:04,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,really solve the problem well and
cs-410_9_6_59,"00:04:04,100","00:04:09,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,research area where also machine
cs-410_9_6_60,"00:04:09,650","00:04:13,624",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,Here are some additional readings in
cs-410_9_6_61,"00:04:13,624","00:04:18,494",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,the handbook called
cs-410_9_6_62,"00:04:18,494","00:04:23,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,has a collection of a lot
cs-410_9_6_63,"00:04:23,364","00:04:28,362",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,can give you an overview
cs-410_9_6_64,"00:04:28,362","00:04:33,122",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,approaches through recommender systems.
cs-410_9_6_65,"00:04:33,122","00:04:43,122",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,[MUSIC]
cs-410_10_6_1,"00:00:00,012","00:00:03,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[NOISE]
cs-410_10_6_2,"00:00:06,848","00:00:10,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is a summary of this course.
cs-410_10_6_3,"00:00:12,890","00:00:17,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,This map shows the major topics
cs-410_10_6_4,"00:00:19,170","00:00:24,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,And here are some key
cs-410_10_6_5,"00:00:24,230","00:00:28,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"First, we talked about natural"
cs-410_10_6_6,"00:00:29,170","00:00:33,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,Here the main take-away messages
cs-410_10_6_7,"00:00:33,120","00:00:39,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"a foundation for text retrieval, but"
cs-410_10_6_8,"00:00:39,210","00:00:48,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,the battle of wars is generally the main
cs-410_10_6_9,"00:00:48,540","00:00:52,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And it's often sufficient before
cs-410_10_6_10,"00:00:52,730","00:00:58,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,obviously for
cs-410_10_6_11,"00:00:58,290","00:01:02,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,a deeper natural language
cs-410_10_6_12,"00:01:02,640","00:01:05,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,We then talked about the high
cs-410_10_6_13,"00:01:05,070","00:01:09,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,text access and
cs-410_10_6_14,"00:01:09,170","00:01:12,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,In pull we talked about
cs-410_10_6_15,"00:01:13,250","00:01:17,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"Now in general in future search engines,"
cs-410_10_6_16,"00:01:17,800","00:01:20,466",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,to provide a math involved
cs-410_10_6_17,"00:01:23,022","00:01:27,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And now we'll talk about a number of
cs-410_10_6_18,"00:01:27,680","00:01:30,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,We talked about the search problem.
cs-410_10_6_19,"00:01:30,350","00:01:32,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,And we framed that as a ranking problem.
cs-410_10_6_20,"00:01:34,830","00:01:38,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,And we talked about a number
cs-410_10_6_21,"00:01:38,680","00:01:42,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,We start with the overview
cs-410_10_6_22,"00:01:42,170","00:01:46,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,the probabilistic model and then we talked
cs-410_10_6_23,"00:01:48,400","00:01:53,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We also later talked about
cs-410_10_6_24,"00:01:53,730","00:01:56,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,that's probabilistic model.
cs-410_10_6_25,"00:01:56,280","00:02:01,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"And here, many take-away message is that"
cs-410_10_6_26,"00:02:01,910","00:02:07,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"look similar, and"
cs-410_10_6_27,"00:02:07,510","00:02:13,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"Most important ones are TF-IDF weighting,"
cs-410_10_6_28,"00:02:13,730","00:02:20,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,And the TF is often transformed through
cs-410_10_6_29,"00:02:22,070","00:02:27,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,And then we talked about how to
cs-410_10_6_30,"00:02:27,730","00:02:33,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"the main techniques that we talked about,"
cs-410_10_6_31,"00:02:33,940","00:02:39,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,that we can prepare the system
cs-410_10_6_32,"00:02:39,590","00:02:45,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,And we talked about how to do a faster
cs-410_10_6_33,"00:02:46,180","00:02:50,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And we then talked about how to
cs-410_10_6_34,"00:02:50,800","00:02:54,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,mainly introduced to
cs-410_10_6_35,"00:02:54,860","00:02:58,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,This was a very important
cs-410_10_6_36,"00:02:58,770","00:03:00,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,applied to many tasks.
cs-410_10_6_37,"00:03:01,980","00:03:05,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,We talked about the major
cs-410_10_6_38,"00:03:05,450","00:03:10,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"So, the most important measures for"
cs-410_10_6_39,"00:03:10,800","00:03:16,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"are MAP, mean average precision,"
cs-410_10_6_40,"00:03:16,400","00:03:20,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,accumulative gain and also precision and
cs-410_10_6_41,"00:03:22,580","00:03:25,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,And we then talked about
cs-410_10_6_42,"00:03:25,540","00:03:29,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,And we talked about the Rocchio
cs-410_10_6_43,"00:03:29,180","00:03:32,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,the mixture model and
cs-410_10_6_44,"00:03:32,200","00:03:36,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,Feedback is a very important
cs-410_10_6_45,"00:03:36,630","00:03:41,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,the opportunity of learning from
cs-410_10_6_46,"00:03:42,960","00:03:45,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,We then talked about Web search.
cs-410_10_6_47,"00:03:45,800","00:03:50,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,And here we talked about how
cs-410_10_6_48,"00:03:50,150","00:03:55,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,to solve the scalability issue in that
cs-410_10_6_49,"00:03:55,330","00:03:59,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,Then we talked about how to use linking
cs-410_10_6_50,"00:03:59,130","00:04:01,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,We talked about page rank and
cs-410_10_6_51,"00:04:01,490","00:04:06,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,hits as the major hours is to
cs-410_10_6_52,"00:04:07,320","00:04:09,562",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,We then talked about
cs-410_10_6_53,"00:04:09,562","00:04:14,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,This is the use of machine learning
cs-410_10_6_54,"00:04:14,810","00:04:16,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,improvement scoring.
cs-410_10_6_55,"00:04:16,640","00:04:21,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,Not only that the effectiveness can be
cs-410_10_6_56,"00:04:21,460","00:04:23,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,we can also improve the robustness of the.
cs-410_10_6_57,"00:04:23,620","00:04:28,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,The ranking function so that it's
cs-410_10_6_58,"00:04:28,560","00:04:34,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,It just some features to promote the page.
cs-410_10_6_59,"00:04:36,270","00:04:39,279",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,And finally we talked about
cs-410_10_6_60,"00:04:40,460","00:04:45,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,About the some major reactions
cs-410_10_6_61,"00:04:45,730","00:04:49,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,in the future in improving the count
cs-410_10_6_62,"00:04:50,610","00:04:54,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,And then finally we talked about
cs-410_10_6_63,"00:04:54,030","00:04:57,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,these are systems to
cs-410_10_6_64,"00:04:57,890","00:05:02,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"And we'll talk about the two approaches,"
cs-410_10_6_65,"00:05:02,120","00:05:06,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,one is collaborative filtering and
cs-410_10_6_66,"00:05:07,330","00:05:11,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,"Now, an obvious missing piece"
cs-410_10_6_67,"00:05:11,930","00:05:16,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"in this picture is the user,"
cs-410_10_6_68,"00:05:16,884","00:05:21,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,so user interface is also an important
cs-410_10_6_69,"00:05:21,620","00:05:25,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,Even though the current search interface
cs-410_10_6_70,"00:05:25,850","00:05:32,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,done a lot of studies of user interfaces
cs-410_10_6_71,"00:05:32,020","00:05:34,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"And this is the topic to that,"
cs-410_10_6_72,"00:05:34,680","00:05:40,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,you can learn more by reading this book.
cs-410_10_6_73,"00:05:40,350","00:05:47,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,It's an excellent book about all kinds
cs-410_10_6_74,"00:05:48,800","00:05:53,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,If you want to know more about
cs-410_10_6_75,"00:05:53,360","00:05:57,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,you can also read some additional
cs-410_10_6_76,"00:05:57,590","00:06:01,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,In this short course we only
cs-410_10_6_77,"00:06:01,110","00:06:03,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,topics in text retrievals and
cs-410_10_6_78,"00:06:04,770","00:06:09,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,And these resources provide additional
cs-410_10_6_79,"00:06:09,930","00:06:16,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,they give a more thorough treatment of
cs-410_10_6_80,"00:06:16,220","00:06:19,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,And a main source is
cs-410_10_6_81,"00:06:21,570","00:06:26,916",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,that you can see a lot of short
cs-410_10_6_82,"00:06:26,916","00:06:30,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,or long tutorials.
cs-410_10_6_83,"00:06:30,290","00:06:35,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,They tend to provide a lot of
cs-410_10_6_84,"00:06:35,260","00:06:40,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,And there a lot of series that
cs-410_10_6_85,"00:06:40,830","00:06:44,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"One is information concepts,"
cs-410_10_6_86,"00:06:44,660","00:06:46,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,One is human langauge technology.
cs-410_10_6_87,"00:06:46,310","00:06:49,452",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,And yet another is artificial
cs-410_10_6_88,"00:06:49,452","00:06:55,535",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,There are also some major journals and
cs-410_10_6_89,"00:06:55,535","00:07:00,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,tend to have a lot of research papers
cs-410_10_6_90,"00:07:00,485","00:07:05,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"And finally, for more information"
cs-410_10_6_91,"00:07:05,370","00:07:08,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"tool kits, etc you can check out his URL."
cs-410_10_6_92,"00:07:10,010","00:07:16,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"So, if you have not taken the text"
cs-410_10_6_93,"00:07:16,320","00:07:22,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,specialization series then naturally
cs-410_10_6_94,"00:07:22,630","00:07:27,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"As this picture shows,"
cs-410_10_6_95,"00:07:27,900","00:07:31,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,we generally need two kinds of techniques.
cs-410_10_6_96,"00:07:31,800","00:07:34,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"One is text retrieval,"
cs-410_10_6_97,"00:07:34,710","00:07:39,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,And these techniques will help us
cs-410_10_6_98,"00:07:39,490","00:07:45,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"relevant text data, which are actually"
cs-410_10_6_99,"00:07:45,550","00:07:51,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now human plays important role in mining
cs-410_10_6_100,"00:07:51,190","00:07:54,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,written for humans to consume.
cs-410_10_6_101,"00:07:54,630","00:08:00,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,So involving humans in the process
cs-410_10_6_102,"00:08:00,580","00:08:05,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,in this course we have covered
cs-410_10_6_103,"00:08:05,050","00:08:08,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,access to the most relevant data.
cs-410_10_6_104,"00:08:08,300","00:08:13,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,These techniques are always so
cs-410_10_6_105,"00:08:13,210","00:08:17,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,to help provide prominence and
cs-410_10_6_106,"00:08:17,770","00:08:23,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,patterns that the user will
cs-410_10_6_107,"00:08:23,990","00:08:27,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,"So, in general, the user would have"
cs-410_10_6_108,"00:08:27,870","00:08:29,359",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,better understand the patterns.
cs-410_10_6_109,"00:08:30,360","00:08:36,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"So the text mining cause, or rather,"
cs-410_10_6_110,"00:08:36,200","00:08:41,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,will be dealing with what to do once
cs-410_10_6_111,"00:08:41,660","00:08:46,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this is a second step in this
cs-410_10_6_112,"00:08:46,010","00:08:48,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,the text data into actionable knowledge.
cs-410_10_6_113,"00:08:49,830","00:08:55,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,And this has to do with helping users to
cs-410_10_6_114,"00:08:55,900","00:08:59,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,to find the patterns and
cs-410_10_6_115,"00:08:59,750","00:09:04,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,In text and such knowledge can
cs-410_10_6_116,"00:09:04,640","00:09:10,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,systems to help decision making or
cs-410_10_6_117,"00:09:10,500","00:09:16,624",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"So, if you have not taken that course,"
cs-410_10_6_118,"00:09:16,624","00:09:22,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,that natural next step would
cs-410_10_6_119,"00:09:24,000","00:09:25,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,Thank you for taking this course.
cs-410_10_6_120,"00:09:25,770","00:09:29,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,I hope you had fun and
cs-410_10_6_121,"00:09:29,570","00:09:34,236",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,And I look forward to interacting
cs-410_10_6_122,"00:09:34,236","00:09:44,236",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,[MUSIC]
cs-410_2_6_1,"00:00:00,076","00:00:03,466",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_2_6_2,"00:00:06,698","00:00:14,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,So now let's take a look at the specific
cs-410_2_6_3,"00:00:14,732","00:00:17,627",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"Now, this is one of the many"
cs-410_2_6_4,"00:00:17,627","00:00:19,309",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,it's one of the simplest methods.
cs-410_2_6_5,"00:00:19,309","00:00:24,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,And I choose this to explain
cs-410_2_6_6,"00:00:26,360","00:00:34,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,"So in this approach, we simply assume"
cs-410_2_6_7,"00:00:34,350","00:00:39,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,respect to a query is related to a linear
cs-410_2_6_8,"00:00:39,760","00:00:47,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,Here I used Xi to denote the feature.
cs-410_2_6_9,"00:00:47,400","00:00:51,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,So Xi of Q and D is a feature.
cs-410_2_6_10,"00:00:51,770","00:00:54,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And we can have as many
cs-410_2_6_11,"00:00:55,890","00:01:01,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,And we assume that these features
cs-410_2_6_12,"00:01:03,580","00:01:06,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,And each feature is controlled
cs-410_2_6_13,"00:01:06,150","00:01:08,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,and this beta i is a parameter.
cs-410_2_6_14,"00:01:08,880","00:01:10,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,That's a weighting parameter.
cs-410_2_6_15,"00:01:10,790","00:01:15,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,A larger value would mean the feature
cs-410_2_6_16,"00:01:15,940","00:01:18,525",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,and it would contribute more
cs-410_2_6_17,"00:01:18,525","00:01:23,069",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,This specific form of the function
cs-410_2_6_18,"00:01:23,069","00:01:27,154",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,a transformation of
cs-410_2_6_19,"00:01:27,154","00:01:30,428",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So this is the probability of relevance.
cs-410_2_6_20,"00:01:30,428","00:01:35,611",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,And we know that the probability of
cs-410_2_6_21,"00:01:36,870","00:01:41,863",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,And we could have just assumed that
cs-410_2_6_22,"00:01:41,863","00:01:43,856",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,this linear combination.
cs-410_2_6_23,"00:01:43,856","00:01:47,479",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,So we can do a linear regression.
cs-410_2_6_24,"00:01:47,479","00:01:53,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"But then, the value of this linear"
cs-410_2_6_25,"00:01:53,940","00:01:59,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,So this transformation
cs-410_2_6_26,"00:01:59,220","00:02:05,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,to 1 range to the whole
cs-410_2_6_27,"00:02:05,540","00:02:08,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,you can verify it by yourself.
cs-410_2_6_28,"00:02:10,350","00:02:16,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,So this allows us then to connect
cs-410_2_6_29,"00:02:16,700","00:02:23,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,which is between 0 and 1 to a linear
cs-410_2_6_30,"00:02:23,010","00:02:28,133",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And if we rewrite this into a probability
cs-410_2_6_31,"00:02:28,133","00:02:34,299",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"So on this equation, now we'll"
cs-410_2_6_32,"00:02:35,690","00:02:39,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"And on the right hand side,"
cs-410_2_6_33,"00:02:39,168","00:02:42,448",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"Now, this form is clearly nonnegative, and"
cs-410_2_6_34,"00:02:42,448","00:02:46,344",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,it still involves a linear
cs-410_2_6_35,"00:02:46,344","00:02:50,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"And it's also clear that if this value is,"
cs-410_2_6_36,"00:02:50,890","00:02:58,991",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,this is actually negative of the linear
cs-410_2_6_37,"00:02:58,991","00:03:04,415",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"If this value here is large,"
cs-410_2_6_38,"00:03:04,415","00:03:11,879",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,then it would mean this value is small.
cs-410_2_6_39,"00:03:11,879","00:03:17,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"And therefore,"
cs-410_2_6_40,"00:03:17,278","00:03:22,034",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"And that's we expect, that basically,"
cs-410_2_6_41,"00:03:22,034","00:03:26,496",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"gives us a high value, then"
cs-410_2_6_42,"00:03:26,496","00:03:29,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,So this is our hypothesis.
cs-410_2_6_43,"00:03:29,015","00:03:33,955",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Again, this is not necessarily the best"
cs-410_2_6_44,"00:03:33,955","00:03:39,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,way to connect these features with
cs-410_2_6_45,"00:03:40,470","00:03:44,578",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,So now we have this combination function.
cs-410_2_6_46,"00:03:44,578","00:03:48,617",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,The next task is to
cs-410_2_6_47,"00:03:48,617","00:03:52,346",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,that the function cache will be applied.
cs-410_2_6_48,"00:03:52,346","00:03:57,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"But without knowing the beta values,"
cs-410_2_6_49,"00:03:58,520","00:04:04,068",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,So let's see how can
cs-410_2_6_50,"00:04:04,068","00:04:07,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"All right,"
cs-410_2_6_51,"00:04:08,780","00:04:11,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"In this example, we have three features."
cs-410_2_6_52,"00:04:11,405","00:04:15,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,One is the BM25 score of the document and
cs-410_2_6_53,"00:04:15,060","00:04:19,044",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"One is the PageRank score of the document,"
cs-410_2_6_54,"00:04:19,044","00:04:21,211",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,might not depend on the query.
cs-410_2_6_55,"00:04:21,211","00:04:25,681",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"We might have a topic-sensitive PageRank,"
cs-410_2_6_56,"00:04:25,681","00:04:29,946",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,"Otherwise, the general PageRank"
cs-410_2_6_57,"00:04:29,946","00:04:35,221",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,And then we have BM25 score on
cs-410_2_6_58,"00:04:35,221","00:04:40,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"Now, these are then the feature values for"
cs-410_2_6_59,"00:04:41,910","00:04:47,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"And in this case, the document is D1 and"
cs-410_2_6_60,"00:04:48,790","00:04:54,547",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,Here's another training instance and
cs-410_2_6_61,"00:04:54,547","00:04:57,832",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"but in this case, it's not relevant."
cs-410_2_6_62,"00:04:57,832","00:05:02,806",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,This is an oversimplified case where
cs-410_2_6_63,"00:05:02,806","00:05:06,675",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,it's sufficient to illustrate the point.
cs-410_2_6_64,"00:05:06,675","00:05:09,885",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,So what we can do is we use
cs-410_2_6_65,"00:05:09,885","00:05:11,797",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,actually estimate the parameters.
cs-410_2_6_66,"00:05:13,170","00:05:17,801",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"Basically, we're going to"
cs-410_2_6_67,"00:05:17,801","00:05:22,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,of the document based
cs-410_2_6_68,"00:05:22,040","00:05:25,534",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"That is, given that we observed"
cs-410_2_6_69,"00:05:28,264","00:05:32,653",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,Can we predict the relevance here?
cs-410_2_6_70,"00:05:32,653","00:05:39,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"Now, of course, the prediction would be"
cs-410_2_6_71,"00:05:39,070","00:05:42,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,And we hypothesize that the probability
cs-410_2_6_72,"00:05:42,680","00:05:43,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,features in this way.
cs-410_2_6_73,"00:05:43,920","00:05:51,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"So we are going to see, for what values of"
cs-410_2_6_74,"00:05:51,260","00:05:58,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,What do we mean by predicting
cs-410_2_6_75,"00:05:58,480","00:06:02,037",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"Well, we just mean, in the first case, for"
cs-410_2_6_76,"00:06:02,037","00:06:06,667",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,D1 this expression right here
cs-410_2_6_77,"00:06:06,667","00:06:10,452",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"In fact, we'll hope this"
cs-410_2_6_78,"00:06:10,452","00:06:13,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,Why?
cs-410_2_6_79,"00:06:14,750","00:06:17,954",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"On the other hand,"
cs-410_2_6_80,"00:06:17,954","00:06:22,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"we hope this value will be small, right."
cs-410_2_6_81,"00:06:22,310","00:06:23,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,Why?
cs-410_2_6_82,"00:06:23,040","00:06:26,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,Because it's a non-relevant document.
cs-410_2_6_83,"00:06:26,310","00:06:30,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,So now let's see how this can
cs-410_2_6_84,"00:06:30,250","00:06:34,954",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,And this is similar to expressing
cs-410_2_6_85,"00:06:34,954","00:06:39,657",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,only that we are not talking about
cs-410_2_6_86,"00:06:39,657","00:06:43,771",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,talking about the probability
cs-410_2_6_87,"00:06:43,771","00:06:48,736",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,So what's the probability
cs-410_2_6_88,"00:06:48,736","00:06:52,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,relevant if it has these feature values?
cs-410_2_6_89,"00:06:54,250","00:06:57,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,"Well, this is just this expression."
cs-410_2_6_90,"00:06:57,890","00:07:00,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,We just need to plug in the Xi's.
cs-410_2_6_91,"00:07:00,970","00:07:03,296",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,So that's what we will get.
cs-410_2_6_92,"00:07:03,296","00:07:08,116",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"It's exactly like what we have seen above,"
cs-410_2_6_93,"00:07:08,116","00:07:14,772",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,only that we replaced these
cs-410_2_6_94,"00:07:14,772","00:07:21,247",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,"So for example, this 0.7 goes to here and"
cs-410_2_6_95,"00:07:21,247","00:07:25,451",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,this 0.11 goes to here.
cs-410_2_6_96,"00:07:25,451","00:07:27,369",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,"And these are different feature values,"
cs-410_2_6_97,"00:07:27,369","00:07:29,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,and we combine them in
cs-410_2_6_98,"00:07:29,405","00:07:31,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,The beta values are still unknown.
cs-410_2_6_99,"00:07:31,770","00:07:37,202",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,But this gives us the probability
cs-410_2_6_100,"00:07:37,202","00:07:39,342",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,if we assume such a model.
cs-410_2_6_101,"00:07:39,342","00:07:39,853",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Okay?
cs-410_2_6_102,"00:07:39,853","00:07:44,553",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"And we want to maximize this probability,"
cs-410_2_6_103,"00:07:44,553","00:07:47,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,What do we do for the second document?
cs-410_2_6_104,"00:07:47,850","00:07:53,309",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Well, we want to compute the probability"
cs-410_2_6_105,"00:07:53,309","00:08:00,257",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,So this would mean we have to
cs-410_2_6_106,"00:08:00,257","00:08:07,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,since this expression is actually
cs-410_2_6_107,"00:08:07,880","00:08:12,524",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So to compute the non-relevance
cs-410_2_6_108,"00:08:12,524","00:08:17,062",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,we just do 1 minus
cs-410_2_6_109,"00:08:17,062","00:08:18,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,Okay?
cs-410_2_6_110,"00:08:18,480","00:08:24,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,So this whole expression then
cs-410_2_6_111,"00:08:24,450","00:08:29,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,predicting these two relevance values.
cs-410_2_6_112,"00:08:29,220","00:08:32,759",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"One is 1 here, one is 0."
cs-410_2_6_113,"00:08:32,759","00:08:37,782",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,And this whole equation
cs-410_2_6_114,"00:08:37,782","00:08:42,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,observing a 1 here and observing a 0 here.
cs-410_2_6_115,"00:08:44,090","00:08:48,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"Of course, this probability"
cs-410_2_6_116,"00:08:50,130","00:08:55,318",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,So then our goal is to adjust
cs-410_2_6_117,"00:08:55,318","00:09:00,121",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"thing reach its maximum,"
cs-410_2_6_118,"00:09:00,121","00:09:02,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,So that means we're going to compute this.
cs-410_2_6_119,"00:09:02,540","00:09:07,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,The beta is just the parameter
cs-410_2_6_120,"00:09:07,280","00:09:11,914",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,maximize this whole likelihood expression.
cs-410_2_6_121,"00:09:11,914","00:09:16,284",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"And what it means is,"
cs-410_2_6_122,"00:09:16,284","00:09:21,224",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,we're going to choose betas to
cs-410_2_6_123,"00:09:21,224","00:09:26,449",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"make this also as large as possible,"
cs-410_2_6_124,"00:09:26,449","00:09:29,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,make this part as small as possible.
cs-410_2_6_125,"00:09:30,560","00:09:32,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,And this is precisely what we want.
cs-410_2_6_126,"00:09:34,530","00:09:38,834",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"So once we do the training,"
cs-410_2_6_127,"00:09:38,834","00:09:43,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,So then this function
cs-410_2_6_128,"00:09:43,330","00:09:50,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"Once beta values are known, both this and"
cs-410_2_6_129,"00:09:50,690","00:09:53,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"So for any new query and new document,"
cs-410_2_6_130,"00:09:53,380","00:09:56,924",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,we can simply compute the features for
cs-410_2_6_131,"00:09:56,924","00:10:00,941",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,And then we just use this formula
cs-410_2_6_132,"00:10:00,941","00:10:06,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,And this scoring function can be used to
cs-410_2_6_133,"00:10:06,700","00:10:11,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,So that's the basic idea
cs-410_2_6_134,"00:10:11,787","00:10:21,787",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,[MUSIC]
cs-410_3_6_1,"00:00:00,008","00:00:07,386",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_6_2,"00:00:07,386","00:00:11,551",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,more of the Munster learning algorithms
cs-410_3_6_3,"00:00:11,551","00:00:15,009",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,they generally attempt to direct
cs-410_3_6_4,"00:00:16,690","00:00:18,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,Like a MAP or nDCG.
cs-410_3_6_5,"00:00:19,020","00:00:24,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,Note that the optimization object or
cs-410_3_6_6,"00:00:24,640","00:00:29,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,on the previous slide is not directly
cs-410_3_6_7,"00:00:31,390","00:00:33,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,By maximizing the prediction of one or
cs-410_3_6_8,"00:00:33,870","00:00:39,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"zero, we don't necessarily optimize"
cs-410_3_6_9,"00:00:39,430","00:00:44,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,One can imagine that our
cs-410_3_6_10,"00:00:44,106","00:00:46,626",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,And let's say both are around 0.5.
cs-410_3_6_11,"00:00:46,626","00:00:51,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,So it's kind of in the middle of zero and
cs-410_3_6_12,"00:00:51,230","00:00:58,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"But the ranking can be wrong, so we might"
cs-410_3_6_13,"00:01:00,750","00:01:04,235",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,So that won't be good from
cs-410_3_6_14,"00:01:04,235","00:01:07,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"even though function, it's not bad."
cs-410_3_6_15,"00:01:07,420","00:01:11,773",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"In contrast, we might have another"
cs-410_3_6_16,"00:01:11,773","00:01:14,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"around the 0.9, it said."
cs-410_3_6_17,"00:01:14,000","00:01:17,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"And by the objective function,"
cs-410_3_6_18,"00:01:17,580","00:01:20,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,But if we didn't get the order
cs-410_3_6_19,"00:01:20,500","00:01:22,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,that's actually a better result.
cs-410_3_6_20,"00:01:22,970","00:01:28,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"So these new, more advanced approaches"
cs-410_3_6_21,"00:01:28,070","00:01:32,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"Of course, then the challenge is"
cs-410_3_6_22,"00:01:32,120","00:01:33,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,be harder to solve.
cs-410_3_6_23,"00:01:33,700","00:01:39,143",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"And then, researchers have posed"
cs-410_3_6_24,"00:01:39,143","00:01:46,153",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,and you can read more of the references at
cs-410_3_6_25,"00:01:46,153","00:01:50,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Now, these learning ranked"
cs-410_3_6_26,"00:01:50,540","00:01:53,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,So there accounts would be be applied
cs-410_3_6_27,"00:01:53,530","00:01:55,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,not just the retrieval problem.
cs-410_3_6_28,"00:01:55,350","00:01:58,993",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So some people will go
cs-410_3_6_29,"00:01:58,993","00:02:02,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,"computational advertising,"
cs-410_3_6_30,"00:02:02,810","00:02:08,636",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,there are many others that you can
cs-410_3_6_31,"00:02:11,157","00:02:15,884",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,To summarize this lecture we
cs-410_3_6_32,"00:02:15,884","00:02:21,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,learning to combine much more
cs-410_3_6_33,"00:02:22,780","00:02:24,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,Actually the use of machine learning
cs-410_3_6_34,"00:02:25,810","00:02:29,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,in information retrieval has
cs-410_3_6_35,"00:02:29,840","00:02:35,212",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"So for example, the Rocchio feedback"
cs-410_3_6_36,"00:02:35,212","00:02:40,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,was a machine learning approach
cs-410_3_6_37,"00:02:40,700","00:02:46,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,But the most recent use of machine
cs-410_3_6_38,"00:02:46,750","00:02:51,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,changes in the environment of
cs-410_3_6_39,"00:02:52,550","00:02:58,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"First, it's mostly freedom of"
cs-410_3_6_40,"00:02:58,650","00:03:04,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"in the form of critical, such as"
cs-410_3_6_41,"00:03:04,250","00:03:11,106",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,So the data can provide a lot of
cs-410_3_6_42,"00:03:11,106","00:03:17,487",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,machine learning methods can be
cs-410_3_6_43,"00:03:17,487","00:03:21,744",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"Secondly, it's also freedom by"
cs-410_3_6_44,"00:03:21,744","00:03:24,464",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,and this is not only just
cs-410_3_6_45,"00:03:24,464","00:03:29,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,features available on the web that can
cs-410_3_6_46,"00:03:29,840","00:03:36,208",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"It's also because by combining them,"
cs-410_3_6_47,"00:03:36,208","00:03:41,168",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"of ranking, so this is desired for"
cs-410_3_6_48,"00:03:41,168","00:03:45,887",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,Modern search engines all use some
cs-410_3_6_49,"00:03:45,887","00:03:48,855",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,combine many features
cs-410_3_6_50,"00:03:48,855","00:03:53,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,this is a major feature of these
cs-410_3_6_51,"00:03:56,190","00:04:02,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,The topic of learning to rank is still
cs-410_3_6_52,"00:04:02,368","00:04:08,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,and so we can expect to see new results
cs-410_3_6_53,"00:04:08,265","00:04:09,119",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,perhaps.
cs-410_3_6_54,"00:04:12,753","00:04:17,686",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,Here are some additional readings
cs-410_3_6_55,"00:04:17,686","00:04:22,544",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,about how learning to rank at works and
cs-410_3_6_56,"00:04:25,281","00:04:35,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,[MUSIC]
cs-410_1_6_1,"00:00:00,000","00:00:02,695",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_1_6_2,"00:00:07,363","00:00:10,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_1_6_3,"00:00:10,900","00:00:15,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,"In this lecture, we are going to"
cs-410_1_6_4,"00:00:15,210","00:00:17,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,In particular we're going to talk
cs-410_1_6_5,"00:00:17,860","00:00:21,339",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,to combine different features
cs-410_1_6_6,"00:00:22,340","00:00:28,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So the question that we address in
cs-410_1_6_7,"00:00:28,500","00:00:36,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,many features to generate a single ranking
cs-410_1_6_8,"00:00:36,230","00:00:42,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,In the previous lectures we have talked
cs-410_1_6_9,"00:00:42,140","00:00:48,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,We have talked about some retrieval
cs-410_1_6_10,"00:00:48,270","00:00:54,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,They can generate a based this course for
cs-410_1_6_11,"00:00:54,760","00:00:58,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,And we also talked about the link
cs-410_1_6_12,"00:00:59,230","00:01:02,759",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,that can give additional scores
cs-410_1_6_13,"00:01:03,940","00:01:07,313",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"Now the question now is,"
cs-410_1_6_14,"00:01:07,313","00:01:09,843",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,potentially many other
cs-410_1_6_15,"00:01:09,843","00:01:14,912",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,And this will be very useful for
cs-410_1_6_16,"00:01:14,912","00:01:19,833",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"accuracy, but also to improve"
cs-410_1_6_17,"00:01:19,833","00:01:24,176",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,So that it's not easy for
cs-410_1_6_18,"00:01:24,176","00:01:26,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,a few features to promote a page.
cs-410_1_6_19,"00:01:27,910","00:01:32,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So the general idea of learning
cs-410_1_6_20,"00:01:32,000","00:01:36,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,learning to combine this
cs-410_1_6_21,"00:01:36,030","00:01:39,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,on different features to generate
cs-410_1_6_22,"00:01:40,610","00:01:44,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,So we will assume that the given
cs-410_1_6_23,"00:01:44,720","00:01:49,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,we can define a number of features.
cs-410_1_6_24,"00:01:49,680","00:01:55,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,And these features can vary from
cs-410_1_6_25,"00:01:55,180","00:01:59,875",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,a score of the document with
cs-410_1_6_26,"00:01:59,875","00:02:05,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,a retrieval function such as BM25 or
cs-410_1_6_27,"00:02:05,060","00:02:10,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,of punitive commands from a machine or
cs-410_1_6_28,"00:02:10,440","00:02:15,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,It can also be a link based score like or
cs-410_1_6_29,"00:02:15,410","00:02:23,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,It can be also application of retrieval
cs-410_1_6_30,"00:02:23,470","00:02:28,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,Those are the types of descriptions
cs-410_1_6_31,"00:02:29,520","00:02:33,909",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"So, these can all the clues whether"
cs-410_1_6_32,"00:02:35,070","00:02:41,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,We can even include a feature
cs-410_1_6_33,"00:02:41,320","00:02:46,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,has a tilde because this might be
cs-410_1_6_34,"00:02:48,170","00:02:52,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,So all these features can then be combined
cs-410_1_6_35,"00:02:52,180","00:02:53,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"The question is, of course."
cs-410_1_6_36,"00:02:53,610","00:02:55,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,How can we combine them?
cs-410_1_6_37,"00:02:55,250","00:03:00,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"In this approach,"
cs-410_1_6_38,"00:03:00,580","00:03:07,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,that this document isn't relevant to this
cs-410_1_6_39,"00:03:07,730","00:03:10,329",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,So we can hypothesize this
cs-410_1_6_40,"00:03:11,450","00:03:16,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,that the probability of relevance
cs-410_1_6_41,"00:03:16,730","00:03:22,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,through a particular form of
cs-410_1_6_42,"00:03:22,070","00:03:25,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,These parameters can control
cs-410_1_6_43,"00:03:25,510","00:03:29,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,the influence of different
cs-410_1_6_44,"00:03:29,410","00:03:33,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,Now this is of course just an assumption.
cs-410_1_6_45,"00:03:33,820","00:03:38,925",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,Whether this assumption really
cs-410_1_6_46,"00:03:38,925","00:03:43,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,that's they have to empirically
cs-410_1_6_47,"00:03:45,020","00:03:50,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,But by hypothesizing that
cs-410_1_6_48,"00:03:50,450","00:03:55,783",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"features in the particular way, we can"
cs-410_1_6_49,"00:03:55,783","00:04:00,805",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,the potential more powerful ranking
cs-410_1_6_50,"00:04:00,805","00:04:05,342",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,Naturally the next question is how
cs-410_1_6_51,"00:04:05,342","00:04:08,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,How do we know which features
cs-410_1_6_52,"00:04:08,732","00:04:11,922",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,and which features will have lower weight?
cs-410_1_6_53,"00:04:11,922","00:04:15,732",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So this is the task of training or
cs-410_1_6_54,"00:04:15,732","00:04:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,in this approach what we will
cs-410_1_6_55,"00:04:20,000","00:04:24,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,Those are the data that have
cs-410_1_6_56,"00:04:24,910","00:04:27,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,that we already know
cs-410_1_6_57,"00:04:27,370","00:04:31,443",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,We already know which documents should
cs-410_1_6_58,"00:04:31,443","00:04:36,074",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,And this information can be based
cs-410_1_6_59,"00:04:36,074","00:04:41,508",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,this can also be approximated by just
cs-410_1_6_60,"00:04:41,508","00:04:47,477",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,where we can assume the clicked documents
cs-410_1_6_61,"00:04:47,477","00:04:53,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,clicked documents are relevant and
cs-410_1_6_62,"00:04:53,500","00:04:58,222",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,So in general with the fit
cs-410_1_6_63,"00:04:58,222","00:05:00,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,function to the training data
cs-410_1_6_64,"00:05:00,960","00:05:06,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,meaning that we will try to optimize it's
cs-410_1_6_65,"00:05:06,650","00:05:08,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,And we can adjust these parameters to see
cs-410_1_6_66,"00:05:09,960","00:05:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,how we can optimize the performance of
cs-410_1_6_67,"00:05:16,030","00:05:19,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,in terms of some measures such as MAP or
cs-410_1_6_68,"00:05:20,600","00:05:25,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,So the training date would
cs-410_1_6_69,"00:05:25,440","00:05:32,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"Each tuple has three elements, the query,"
cs-410_1_6_70,"00:05:32,800","00:05:37,469",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So it looks very much like our
cs-410_1_6_71,"00:05:37,469","00:05:40,933",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,about in the evaluation
cs-410_1_6_72,"00:05:40,933","00:05:50,933",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,[MUSIC]
cs-410_5_6_1,"00:00:00,000","00:00:07,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_5_6_2,"00:00:07,248","00:00:09,548",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_5_6_3,"00:00:12,888","00:00:18,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,So far we have talked about a lot
cs-410_5_6_4,"00:00:19,680","00:00:24,675",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We have talked about the problem
cs-410_5_6_5,"00:00:24,675","00:00:30,134",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"different methods for ranking,"
cs-410_5_6_6,"00:00:30,134","00:00:33,198",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"how to evaluate a search engine, etc."
cs-410_5_6_7,"00:00:36,028","00:00:40,719",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,This is important because we know
cs-410_5_6_8,"00:00:40,719","00:00:44,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,the most important applications
cs-410_5_6_9,"00:00:44,980","00:00:49,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,And they are the most useful tools
cs-410_5_6_10,"00:00:49,820","00:00:53,889",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,data into a small set
cs-410_5_6_11,"00:00:56,330","00:01:00,959",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,Another reason why we spend so
cs-410_5_6_12,"00:01:00,959","00:01:06,961",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,is because many techniques used in search
cs-410_5_6_13,"00:01:06,961","00:01:11,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Recommender Systems,"
cs-410_5_6_14,"00:01:11,266","00:01:16,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"And so, overall, the two systems"
cs-410_5_6_15,"00:01:16,840","00:01:19,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,And there are many techniques
cs-410_5_6_16,"00:01:22,690","00:01:24,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,So this is a slide that
cs-410_5_6_17,"00:01:24,860","00:01:29,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,when we talked about the two
cs-410_5_6_18,"00:01:29,020","00:01:30,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,Pull and the Push.
cs-410_5_6_19,"00:01:31,240","00:01:36,362",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,And we mentioned that recommender
cs-410_5_6_20,"00:01:36,362","00:01:42,079",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"users in the Push Mode, where the systems"
cs-410_5_6_21,"00:01:42,079","00:01:47,228",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,the information to the user or
cs-410_5_6_22,"00:01:47,228","00:01:51,429",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,And this often works
cs-410_5_6_23,"00:01:51,429","00:01:56,341",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,stable information need
cs-410_5_6_24,"00:01:56,341","00:02:01,649",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,So a Recommender System is sometimes
cs-410_5_6_25,"00:02:01,649","00:02:07,431",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,it's because recommending useful
cs-410_5_6_26,"00:02:07,431","00:02:10,749",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"filtering out the the useless articles,"
cs-410_5_6_27,"00:02:10,749","00:02:14,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,and so
cs-410_5_6_28,"00:02:16,070","00:02:20,412",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,And in all the cases the system
cs-410_5_6_29,"00:02:20,412","00:02:24,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,usually there's a dynamic source
cs-410_5_6_30,"00:02:24,840","00:02:29,028",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,that you have some knowledge
cs-410_5_6_31,"00:02:29,028","00:02:31,788",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And then the system would make a decision
cs-410_5_6_32,"00:02:31,788","00:02:34,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,about whether this item is
cs-410_5_6_33,"00:02:34,950","00:02:39,678",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,then if it's interesting then the system
cs-410_5_6_34,"00:02:43,008","00:02:49,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,So the basic filtering question here is
cs-410_5_6_35,"00:02:49,520","00:02:52,426",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,Will U like item X?
cs-410_5_6_36,"00:02:52,426","00:02:55,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,And there are two ways to answer this
cs-410_5_6_37,"00:02:56,738","00:03:00,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,And one is look at what items U likes and
cs-410_5_6_38,"00:03:00,040","00:03:03,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,then we can see if X is
cs-410_5_6_39,"00:03:05,610","00:03:10,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"The other is to look at who likes X,"
cs-410_5_6_40,"00:03:10,460","00:03:16,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"user looks like a one of those users,"
cs-410_5_6_41,"00:03:16,000","00:03:18,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,And these strategies can be combined.
cs-410_5_6_42,"00:03:18,640","00:03:20,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,If we follow the first strategy and
cs-410_5_6_43,"00:03:20,800","00:03:26,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,look at item similarity in the case
cs-410_5_6_44,"00:03:26,170","00:03:31,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,then we're talking about a content-based
cs-410_5_6_45,"00:03:31,460","00:03:38,195",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"If we look at the second strategy, then,"
cs-410_5_6_46,"00:03:38,195","00:03:43,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,we're user similarity and the technique
cs-410_5_6_47,"00:03:46,010","00:03:49,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,"So, let's first look at"
cs-410_5_6_48,"00:03:49,190","00:03:51,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,This is what the system would look like.
cs-410_5_6_49,"00:03:52,600","00:03:56,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"Inside the system, there will be"
cs-410_5_6_50,"00:03:56,420","00:04:00,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"knowledge about the user's interests, and"
cs-410_5_6_51,"00:04:02,210","00:04:06,815",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,It maintains this profile to keep
cs-410_5_6_52,"00:04:06,815","00:04:10,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,then there is a utility function
cs-410_5_6_53,"00:04:10,865","00:04:13,955",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,a nice plan utility
cs-410_5_6_54,"00:04:13,955","00:04:17,977",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,It helps the system decide
cs-410_5_6_55,"00:04:17,977","00:04:21,307",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And then the accepted documents will
cs-410_5_6_56,"00:04:21,307","00:04:23,457",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,according to the classified.
cs-410_5_6_57,"00:04:23,457","00:04:28,327",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,There should be also an initialization
cs-410_5_6_58,"00:04:28,327","00:04:34,167",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,maybe from a user's specified keywords or
cs-410_5_6_59,"00:04:34,167","00:04:38,519",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"etc., and this would be to feed into"
cs-410_5_6_60,"00:04:39,900","00:04:43,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,There is also typically a learning
cs-410_5_6_61,"00:04:43,210","00:04:45,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,users' feedback over time.
cs-410_5_6_62,"00:04:45,310","00:04:49,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,Now note that in this case typical
cs-410_5_6_63,"00:04:49,310","00:04:53,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,the system would have a lot more
cs-410_5_6_64,"00:04:53,420","00:04:58,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"If the user has taken a recommended item,"
cs-410_5_6_65,"00:04:58,590","00:05:04,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,this a signal to indicate that
cs-410_5_6_66,"00:05:04,020","00:05:07,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"If the user discarded it,"
cs-410_5_6_67,"00:05:07,010","00:05:11,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,And so such feedback can be a long term
cs-410_5_6_68,"00:05:11,640","00:05:16,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,And the system can collect a lot of
cs-410_5_6_69,"00:05:16,660","00:05:19,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,this then can then be used
cs-410_5_6_70,"00:05:19,500","00:05:23,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,Now what's the criteria for
cs-410_5_6_71,"00:05:24,860","00:05:31,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,How do we know this filtering
cs-410_5_6_72,"00:05:31,190","00:05:36,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,Now in this case we cannot use the ranking
cs-410_5_6_73,"00:05:36,440","00:05:39,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,because we can't afford waiting for
cs-410_5_6_74,"00:05:39,300","00:05:42,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,then rank the documents to
cs-410_5_6_75,"00:05:42,960","00:05:47,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,And so the system must make
cs-410_5_6_76,"00:05:47,930","00:05:51,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,to decide whether the item is
cs-410_5_6_77,"00:05:51,830","00:05:55,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"So in other words, we're trying"
cs-410_5_6_78,"00:05:56,800","00:05:57,899",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"So in this case,"
cs-410_5_6_79,"00:05:57,899","00:06:03,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,one common user strategy is to use
cs-410_5_6_80,"00:06:03,600","00:06:06,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"So here, I show linear utility function."
cs-410_5_6_81,"00:06:06,560","00:06:11,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,That's defined as for example three
cs-410_5_6_82,"00:06:11,550","00:06:17,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"you delivered, minus two multiplied by the"
cs-410_5_6_83,"00:06:17,490","00:06:20,775",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"So in other words, we could kind of just"
cs-410_5_6_84,"00:06:22,245","00:06:26,215",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,treat this as almost in a gambling game.
cs-410_5_6_85,"00:06:26,215","00:06:32,767",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"If you delete one good item,"
cs-410_5_6_86,"00:06:32,767","00:06:37,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,you gain three dollars but if you deliver
cs-410_5_6_87,"00:06:37,660","00:06:41,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,And this utility function
cs-410_5_6_88,"00:06:41,120","00:06:45,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,how much money you are get by
cs-410_5_6_89,"00:06:45,375","00:06:52,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,And so it's clear that if you want
cs-410_5_6_90,"00:06:52,420","00:06:57,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,this strategy should be delivered
cs-410_5_6_91,"00:06:57,760","00:07:01,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,and minimize the delivery of bad articles.
cs-410_5_6_92,"00:07:01,160","00:07:02,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"That's obvious, right?"
cs-410_5_6_93,"00:07:03,570","00:07:08,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,Now one interesting question here is
cs-410_5_6_94,"00:07:08,130","00:07:14,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,I just showed a three and
cs-410_5_6_95,"00:07:14,140","00:07:16,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,"But one can ask the question,"
cs-410_5_6_96,"00:07:17,990","00:07:19,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,So what do you think?
cs-410_5_6_97,"00:07:21,080","00:07:23,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,Do you think that's a reasonable choice?
cs-410_5_6_98,"00:07:23,450","00:07:24,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,What about the other choices?
cs-410_5_6_99,"00:07:26,220","00:07:33,058",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"So for example, we can have 10 and"
cs-410_5_6_100,"00:07:33,058","00:07:34,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,What's the difference?
cs-410_5_6_101,"00:07:34,750","00:07:35,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,What do you think?
cs-410_5_6_102,"00:07:36,920","00:07:41,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,How would this utility function affect
cs-410_5_6_103,"00:07:43,600","00:07:45,589",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"Right, you can think of"
cs-410_5_6_104,"00:07:45,589","00:07:51,284",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"(10, -1) + (1, -10), which one do"
cs-410_5_6_105,"00:07:51,284","00:07:57,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,system to over do it and which one would
cs-410_5_6_106,"00:07:57,760","00:08:03,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,If you think about it you will see that
cs-410_5_6_107,"00:08:03,380","00:08:08,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,our good document you incur only a small
cs-410_5_6_108,"00:08:08,410","00:08:11,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Intuitively, you would be"
cs-410_5_6_109,"00:08:11,740","00:08:16,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,And you can try to deliver more in
cs-410_5_6_110,"00:08:16,370","00:08:17,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,And then we'll get a big reward.
cs-410_5_6_111,"00:08:19,600","00:08:23,364",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"So on the other hand,"
cs-410_5_6_112,"00:08:23,364","00:08:28,228",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,you really don't get such a big prize
cs-410_5_6_113,"00:08:28,228","00:08:31,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"On the other hand, you will have"
cs-410_5_6_114,"00:08:31,250","00:08:32,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"You can imagine that,"
cs-410_5_6_115,"00:08:32,710","00:08:36,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,the system would be very reluctant
cs-410_5_6_116,"00:08:36,590","00:08:41,198",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,It has to be absolutely
cs-410_5_6_117,"00:08:41,198","00:08:45,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this utility function has to be
cs-410_5_6_118,"00:08:45,990","00:08:49,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,The three basic problems in content-based
cs-410_5_6_119,"00:08:49,660","00:08:53,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,"first, it has to make"
cs-410_5_6_120,"00:08:53,620","00:08:58,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"So it has to be a binary decision maker,"
cs-410_5_6_121,"00:08:58,200","00:09:03,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Given a text document and
cs-410_5_6_122,"00:09:03,620","00:09:07,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"it has to say yes or no, whether this"
cs-410_5_6_123,"00:09:08,050","00:09:12,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"So that's a decision module, and"
cs-410_5_6_124,"00:09:12,375","00:09:17,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,module as you have seen earlier and
cs-410_5_6_125,"00:09:17,220","00:09:22,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,And we have to initialize the system
cs-410_5_6_126,"00:09:22,050","00:09:25,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,text exclusion or
cs-410_5_6_127,"00:09:26,710","00:09:30,375",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,And the third model is
cs-410_5_6_128,"00:09:30,375","00:09:35,445",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,has to be able to learn from limited
cs-410_5_6_129,"00:09:35,445","00:09:41,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,counted them from the user about their
cs-410_5_6_130,"00:09:41,100","00:09:45,702",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,If we don't deliver document
cs-410_5_6_131,"00:09:45,702","00:09:48,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,be able to know whether
cs-410_5_6_132,"00:09:50,460","00:09:55,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,And we had accumulate a lot of documents
cs-410_5_6_133,"00:09:56,220","00:10:01,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,All these modules will have to be
cs-410_5_6_134,"00:10:01,470","00:10:03,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,So how can we deal with such a system?
cs-410_5_6_135,"00:10:03,050","00:10:05,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,And there are many different approaches.
cs-410_5_6_136,"00:10:05,260","00:10:09,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,Here we're going to talk about
cs-410_5_6_137,"00:10:09,600","00:10:12,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,a search engine for information filtering.
cs-410_5_6_138,"00:10:12,120","00:10:15,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"Again, here's why we've spent a lot of"
cs-410_5_6_139,"00:10:15,880","00:10:20,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Because it's actually not very hard
cs-410_5_6_140,"00:10:20,830","00:10:22,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,information filtering.
cs-410_5_6_141,"00:10:22,320","00:10:26,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,So here's the basic idea for
cs-410_5_6_142,"00:10:26,410","00:10:27,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,information filtering.
cs-410_5_6_143,"00:10:27,960","00:10:31,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,"First, we can reuse a lot of"
cs-410_5_6_144,"00:10:31,180","00:10:34,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,"Right, so we know how to score"
cs-410_5_6_145,"00:10:34,950","00:10:39,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,We're going to match the similarity
cs-410_5_6_146,"00:10:39,620","00:10:40,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,a document.
cs-410_5_6_147,"00:10:40,930","00:10:44,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,And then we can use a score threshold for
cs-410_5_6_148,"00:10:44,320","00:10:49,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,We do retrieval and then we kind of find
cs-410_5_6_149,"00:10:49,290","00:10:56,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,apply a threshold to see whether the
cs-410_5_6_150,"00:10:56,890","00:10:58,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,"And if it's passing the threshold,"
cs-410_5_6_151,"00:10:58,230","00:11:02,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,we're going to say it's relevant and
cs-410_5_6_152,"00:11:02,900","00:11:08,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"Another component that we have to add is,"
cs-410_5_6_153,"00:11:08,310","00:11:13,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,we had used is the traditional feedback
cs-410_5_6_154,"00:11:13,080","00:11:18,632",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,And we know rock hill can be using for
cs-410_5_6_155,"00:11:18,632","00:11:25,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,"And, but we have to develop a new"
cs-410_5_6_156,"00:11:25,008","00:11:27,279",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,And we need to set it initially and
cs-410_5_6_157,"00:11:27,279","00:11:32,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,then we have to learn how to
cs-410_5_6_158,"00:11:32,170","00:11:37,276",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,So here's what the system
cs-410_5_6_159,"00:11:37,276","00:11:45,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,generalize the vector-space model for
cs-410_5_6_160,"00:11:45,040","00:11:49,348",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,So you can see the document vector could
cs-410_5_6_161,"00:11:49,348","00:11:53,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,already exists in a search engine
cs-410_5_6_162,"00:11:53,820","00:11:58,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,And the profile will be treated
cs-410_5_6_163,"00:11:58,630","00:12:02,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,the profile vector can be matched with
cs-410_5_6_164,"00:12:03,130","00:12:06,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,And then this score would be fed into a
cs-410_5_6_165,"00:12:06,960","00:12:13,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,"no, and then the evaluation would be based"
cs-410_5_6_166,"00:12:13,690","00:12:16,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,If it says yes and then the document
cs-410_5_6_167,"00:12:16,870","00:12:19,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,And then user could give some feedback.
cs-410_5_6_168,"00:12:19,660","00:12:25,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,The feedback information would be
cs-410_5_6_169,"00:12:25,530","00:12:28,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,to adjust the vector representation.
cs-410_5_6_170,"00:12:28,500","00:12:33,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,So the vector learning is essentially
cs-410_5_6_171,"00:12:33,150","00:12:36,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,feedback in the case of search.
cs-410_5_6_172,"00:12:36,140","00:12:39,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,The threshold of learning
cs-410_5_6_173,"00:12:39,480","00:12:42,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,that we need to talk
cs-410_5_6_174,"00:12:42,580","00:12:52,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,[MUSIC]
cs-410_8_6_1,"00:00:00,012","00:00:09,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_8_6_2,"00:00:09,135","00:00:12,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,here we're going to talk
cs-410_8_6_3,"00:00:12,960","00:00:18,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,And that would be based on
cs-410_8_6_4,"00:00:18,430","00:00:23,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,then predicting the rating of and
cs-410_8_6_5,"00:00:23,220","00:00:32,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,object by an active user using the ratings
cs-410_8_6_6,"00:00:32,540","00:00:38,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,This is called a memory based approach
cs-410_8_6_7,"00:00:40,120","00:00:44,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,storing all the user information and
cs-410_8_6_8,"00:00:44,460","00:00:49,713",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,when we are considering a particular
cs-410_8_6_9,"00:00:49,713","00:00:56,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,retrieve the rating users or
cs-410_8_6_10,"00:00:56,210","00:01:01,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And then try to use this
cs-410_8_6_11,"00:01:01,140","00:01:05,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,to predict the preference of this user.
cs-410_8_6_12,"00:01:05,120","00:01:11,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,So here is the general idea and
cs-410_8_6_13,"00:01:11,700","00:01:16,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,x sub i j denotes the rating
cs-410_8_6_14,"00:01:17,910","00:01:23,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,and n sub i is average rating
cs-410_8_6_15,"00:01:26,100","00:01:31,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,So this n i is needed because
cs-410_8_6_16,"00:01:31,050","00:01:35,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,we would like to normalize
cs-410_8_6_17,"00:01:35,500","00:01:39,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,So how do you do normalization?
cs-410_8_6_18,"00:01:39,190","00:01:46,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"Well, we're going to just subtract"
cs-410_8_6_19,"00:01:46,440","00:01:49,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Now, this is to normalize these ratings so"
cs-410_8_6_20,"00:01:49,890","00:01:53,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,that the ratings from different
cs-410_8_6_21,"00:01:55,590","00:02:00,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"Because some users might be more generous,"
cs-410_8_6_22,"00:02:00,220","00:02:05,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,ratings but some others might be
cs-410_8_6_23,"00:02:05,160","00:02:10,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,cannot be directly compared with each
cs-410_8_6_24,"00:02:10,850","00:02:13,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,So we need to do this normalization.
cs-410_8_6_25,"00:02:13,450","00:02:18,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,Another prediction of
cs-410_8_6_26,"00:02:18,420","00:02:22,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,by another user or
cs-410_8_6_27,"00:02:24,460","00:02:29,419",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,can be based on the average
cs-410_8_6_28,"00:02:30,630","00:02:36,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,So the user u sub a is the user that we
cs-410_8_6_29,"00:02:36,960","00:02:42,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,And we now are interested in
cs-410_8_6_30,"00:02:42,400","00:02:47,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So we're interested in knowing how
cs-410_8_6_31,"00:02:47,910","00:02:49,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,How do we know that?
cs-410_8_6_32,"00:02:50,370","00:02:55,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,Where the idea here is to look at
cs-410_8_6_33,"00:02:55,560","00:02:57,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,have liked this object.
cs-410_8_6_34,"00:02:59,530","00:03:04,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,So mathematically this is to say
cs-410_8_6_35,"00:03:04,720","00:03:12,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"this user on this app object,"
cs-410_8_6_36,"00:03:12,130","00:03:18,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,combination of the normalized
cs-410_8_6_37,"00:03:18,640","00:03:23,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"and in fact here,"
cs-410_8_6_38,"00:03:23,950","00:03:29,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,But not all users contribute
cs-410_8_6_39,"00:03:29,180","00:03:31,748",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,and this is conjured by the weights.
cs-410_8_6_40,"00:03:31,748","00:03:37,191",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,So this weight controls the inference
cs-410_8_6_41,"00:03:37,191","00:03:41,618",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,of the user on the prediction.
cs-410_8_6_42,"00:03:41,618","00:03:46,763",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"And of course,"
cs-410_8_6_43,"00:03:46,763","00:03:51,917",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,the similarity between ua and
cs-410_8_6_44,"00:03:51,917","00:03:57,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"The more similar they are,"
cs-410_8_6_45,"00:03:57,650","00:04:02,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,user ui can make in predicting
cs-410_8_6_46,"00:04:03,950","00:04:06,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"So, the formula is extremely simple."
cs-410_8_6_47,"00:04:06,060","00:04:10,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"You can see,"
cs-410_8_6_48,"00:04:10,140","00:04:14,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"And inside the sum we have their ratings,"
cs-410_8_6_49,"00:04:14,040","00:04:17,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,their normalized ratings
cs-410_8_6_50,"00:04:17,380","00:04:21,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,The ratings need to be normalized in
cs-410_8_6_51,"00:04:22,690","00:04:25,739",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,And then these ratings
cs-410_8_6_52,"00:04:26,750","00:04:33,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,So you can imagine w of a and i is just
cs-410_8_6_53,"00:04:34,470","00:04:35,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,Now what's k here?
cs-410_8_6_54,"00:04:35,350","00:04:39,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,Well k is simply a normalizer.
cs-410_8_6_55,"00:04:39,120","00:04:45,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,It's just one over the sum of all
cs-410_8_6_56,"00:04:47,860","00:04:54,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"So this means, basically, if you consider"
cs-410_8_6_57,"00:04:54,680","00:04:59,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,we have coefficients of weight that
cs-410_8_6_58,"00:05:00,430","00:05:05,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,And it's just a normalization strategy so
cs-410_8_6_59,"00:05:05,690","00:05:12,319",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,in the same range as these ratings
cs-410_8_6_60,"00:05:13,650","00:05:14,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,Right?
cs-410_8_6_61,"00:05:14,560","00:05:20,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,So this is basically the main idea
cs-410_8_6_62,"00:05:20,320","00:05:21,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,collaborative filtering.
cs-410_8_6_63,"00:05:22,750","00:05:27,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"Once we make this prediction,"
cs-410_8_6_64,"00:05:27,880","00:05:33,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,back through the rating that
cs-410_8_6_65,"00:05:33,270","00:05:38,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"the user would actually make,"
cs-410_8_6_66,"00:05:38,520","00:05:44,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,and this is to further
cs-410_8_6_67,"00:05:44,110","00:05:49,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,average rating of this user u
cs-410_8_6_68,"00:05:49,980","00:05:54,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,This would recover a meaningful rating for
cs-410_8_6_69,"00:05:54,290","00:05:59,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"So if this user is generous, then"
cs-410_8_6_70,"00:05:59,410","00:06:04,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,and when we add that the rating will be
cs-410_8_6_71,"00:06:04,580","00:06:10,459",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,Now when you recommend an item to a user
cs-410_8_6_72,"00:06:10,459","00:06:15,093",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,because you are interested in
cs-410_8_6_73,"00:06:15,093","00:06:17,158",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,that's more meaningful.
cs-410_8_6_74,"00:06:17,158","00:06:22,624",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,But when they evaluate these
cs-410_8_6_75,"00:06:22,624","00:06:27,563",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,they typically assume that
cs-410_8_6_76,"00:06:27,563","00:06:32,923",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,these objects to be unknown and
cs-410_8_6_77,"00:06:32,923","00:06:38,938",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,then you compare the predicted
cs-410_8_6_78,"00:06:38,938","00:06:42,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"So, you do have access"
cs-410_8_6_79,"00:06:42,020","00:06:44,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"But, then you pretend that you don't know,"
cs-410_8_6_80,"00:06:44,100","00:06:48,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,then you compare your systems
cs-410_8_6_81,"00:06:48,420","00:06:54,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"In that case, obviously, the systems"
cs-410_8_6_82,"00:06:54,130","00:06:59,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,the actual ratings of the user and
cs-410_8_6_83,"00:07:01,040","00:07:05,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Okay so this is the memory based approach.
cs-410_8_6_84,"00:07:05,160","00:07:07,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"Now, of course,"
cs-410_8_6_85,"00:07:07,000","00:07:09,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,if you want to write
cs-410_8_6_86,"00:07:09,430","00:07:15,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,you still face the problem of
cs-410_8_6_87,"00:07:15,510","00:07:20,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"Once you know the w function, then"
cs-410_8_6_88,"00:07:22,740","00:07:28,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"So, indeed, there are many different ways"
cs-410_8_6_89,"00:07:28,910","00:07:33,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,"w, and specific approaches generally"
cs-410_8_6_90,"00:07:35,500","00:07:38,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So here are some possibilities and
cs-410_8_6_91,"00:07:38,220","00:07:42,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,you can imagine there
cs-410_8_6_92,"00:07:42,010","00:07:46,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,One popular approach is we use
cs-410_8_6_93,"00:07:48,130","00:07:52,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,This would be a sum over
cs-410_8_6_94,"00:07:52,380","00:07:56,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,And the formula is a standard
cs-410_8_6_95,"00:07:56,280","00:07:58,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,coefficient formula as shown here.
cs-410_8_6_96,"00:08:00,060","00:08:05,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,So this basically measures
cs-410_8_6_97,"00:08:05,300","00:08:10,229",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,to all give higher ratings to similar
cs-410_8_6_98,"00:08:11,780","00:08:15,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"Another measure is the cosine measure,"
cs-410_8_6_99,"00:08:15,990","00:08:20,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,vectors as vectors in the vector space.
cs-410_8_6_100,"00:08:20,820","00:08:24,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"And then,"
cs-410_8_6_101,"00:08:24,400","00:08:27,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,compute the cosine of
cs-410_8_6_102,"00:08:27,880","00:08:32,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,And this measure has been using the vector
cs-410_8_6_103,"00:08:32,590","00:08:36,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,So as you can imagine there are just
cs-410_8_6_104,"00:08:36,400","00:08:41,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"In all these cases, note that the user's"
cs-410_8_6_105,"00:08:41,330","00:08:47,135",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,on items and we did not actually use
cs-410_8_6_106,"00:08:47,135","00:08:51,802",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"It didn't matter these items are,"
cs-410_8_6_107,"00:08:51,802","00:08:55,276",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,"they can be books, they can be products,"
cs-410_8_6_108,"00:08:55,276","00:09:00,541",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,they can be text documents which
cs-410_8_6_109,"00:09:00,541","00:09:07,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,so this allows such approach to be
cs-410_8_6_110,"00:09:07,120","00:09:08,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"Now in some newer approaches of course,"
cs-410_8_6_111,"00:09:08,920","00:09:11,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,we would like to use more
cs-410_8_6_112,"00:09:11,830","00:09:18,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"Clearly, we know more about the user,"
cs-410_8_6_113,"00:09:18,750","00:09:23,659",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"So in the actual filtering system,"
cs-410_8_6_114,"00:09:23,659","00:09:27,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,we could also combine that
cs-410_8_6_115,"00:09:27,820","00:09:34,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"We could use more context information,"
cs-410_8_6_116,"00:09:34,040","00:09:39,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"that people are just starting, and"
cs-410_8_6_117,"00:09:39,140","00:09:44,147",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"But, this memory based approach has"
cs-410_8_6_118,"00:09:44,147","00:09:48,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,and it's easy to implement in
cs-410_8_6_119,"00:09:48,750","00:09:53,698",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,a starting point to see if the strategy
cs-410_8_6_120,"00:09:56,108","00:10:01,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"So, there are some obvious ways"
cs-410_8_6_121,"00:10:01,305","00:10:07,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,mainly we would like to improve
cs-410_8_6_122,"00:10:07,070","00:10:09,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,And there are some practical
cs-410_8_6_123,"00:10:09,690","00:10:11,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"So for example,"
cs-410_8_6_124,"00:10:11,960","00:10:12,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,What do you do with them?
cs-410_8_6_125,"00:10:12,990","00:10:18,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"Well, you can set them to default values"
cs-410_8_6_126,"00:10:18,060","00:10:20,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,And that would be a simple solution.
cs-410_8_6_127,"00:10:20,310","00:10:26,388",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,But there are advanced approaches that
cs-410_8_6_128,"00:10:26,388","00:10:32,878",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,"missing values, and then use predictive"
cs-410_8_6_129,"00:10:32,878","00:10:38,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,So in fact that the memory based apology
cs-410_8_6_130,"00:10:38,880","00:10:43,128",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,So you get you have iterative approach
cs-410_8_6_131,"00:10:43,128","00:10:43,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,prediction and
cs-410_8_6_132,"00:10:43,895","00:10:48,095",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,then you can use the predictive values to
cs-410_8_6_133,"00:10:49,525","00:10:54,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,So this is a heuristic
cs-410_8_6_134,"00:10:54,840","00:10:59,639",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,And the strategy obviously would affect
cs-410_8_6_135,"00:10:59,639","00:11:04,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,just like any other heuristics would
cs-410_8_6_136,"00:11:06,290","00:11:10,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,Another idea which is actually very
cs-410_8_6_137,"00:11:10,460","00:11:15,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,have seen in text search is called
cs-410_8_6_138,"00:11:15,150","00:11:23,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Now here the idea is to look at where
cs-410_8_6_139,"00:11:23,980","00:11:29,092",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,If the item is a popular item that
cs-410_8_6_140,"00:11:29,092","00:11:35,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,seen [INAUDIBLE] to people interested
cs-410_8_6_141,"00:11:35,110","00:11:40,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,"interesting but if it's a rare item,"
cs-410_8_6_142,"00:11:40,620","00:11:44,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,But these two users deal with this
cs-410_8_6_143,"00:11:44,770","00:11:47,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"And, that says more"
cs-410_8_6_144,"00:11:47,370","00:11:52,177",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,It's kind of to emphasize
cs-410_8_6_145,"00:11:52,177","00:11:56,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,on items that are not
cs-410_8_6_146,"00:11:56,738","00:12:06,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,[MUSIC]
cs-410_4_6_1,"00:00:00,012","00:00:04,047",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_4_6_2,"00:00:07,043","00:00:10,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_4_6_3,"00:00:12,660","00:00:17,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we're going to talk"
cs-410_4_6_4,"00:00:17,560","00:00:22,489",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,of web search and intelligent information
cs-410_4_6_5,"00:00:24,370","00:00:28,561",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,In order to further improve
cs-410_4_6_6,"00:00:28,561","00:00:33,752",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,it's important that to consider
cs-410_4_6_7,"00:00:33,752","00:00:39,056",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So one particular trend could be to
cs-410_4_6_8,"00:00:39,056","00:00:44,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"customized search engines, and they"
cs-410_4_6_9,"00:00:46,260","00:00:50,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,These vertical search engines can be
cs-410_4_6_10,"00:00:50,180","00:00:55,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,the current general search engines
cs-410_4_6_11,"00:00:55,940","00:01:02,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,users are a special group of users that
cs-410_4_6_12,"00:01:02,070","00:01:06,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,and then the search engine can be
cs-410_4_6_13,"00:01:07,970","00:01:12,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"And because of the customization,"
cs-410_4_6_14,"00:01:12,150","00:01:14,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"So the search can be personalized,"
cs-410_4_6_15,"00:01:15,430","00:01:18,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,because we have a better
cs-410_4_6_16,"00:01:20,330","00:01:25,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,"Because of the restrictions with domain,"
cs-410_4_6_17,"00:01:25,550","00:01:29,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"in handling the documents, because we can"
cs-410_4_6_18,"00:01:29,590","00:01:33,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"For example, particular words may"
cs-410_4_6_19,"00:01:33,880","00:01:36,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,So we can bypass the problem of ambiguity.
cs-410_4_6_20,"00:01:38,390","00:01:41,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"Another trend we can expect to see,"
cs-410_4_6_21,"00:01:41,460","00:01:45,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,is the search engine will
cs-410_4_6_22,"00:01:45,600","00:01:52,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,It's like a lifetime learning or
cs-410_4_6_23,"00:01:52,430","00:01:57,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,very attractive because that means the
cs-410_4_6_24,"00:01:57,800","00:02:01,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,"As more people are using it, the search"
cs-410_4_6_25,"00:02:01,780","00:02:03,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"this is already happening,"
cs-410_4_6_26,"00:02:03,260","00:02:06,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,because the search engines can learn
cs-410_4_6_27,"00:02:06,980","00:02:10,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"More users use it, and the quality"
cs-410_4_6_28,"00:02:10,800","00:02:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,the popular queries that are typed in by
cs-410_4_6_29,"00:02:15,840","00:02:19,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,so this is sort of another
cs-410_4_6_30,"00:02:21,260","00:02:24,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,The third trend might be
cs-410_4_6_31,"00:02:24,600","00:02:27,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,bottles of information access.
cs-410_4_6_32,"00:02:27,190","00:02:32,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"So search, navigation, and"
cs-410_4_6_33,"00:02:32,050","00:02:37,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,combined to form a full-fledged
cs-410_4_6_34,"00:02:37,480","00:02:42,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"And in the beginning of this course,"
cs-410_4_6_35,"00:02:42,470","00:02:47,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,These are different modes of information
cs-410_4_6_36,"00:02:48,170","00:02:53,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"And similarly, in the pull mode, querying"
cs-410_4_6_37,"00:02:53,660","00:02:58,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"And in fact we're doing that basically,"
cs-410_4_6_38,"00:02:58,390","00:03:02,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"We are querying, sometimes browsing,"
cs-410_4_6_39,"00:03:02,000","00:03:05,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,Sometimes we've got some
cs-410_4_6_40,"00:03:05,120","00:03:11,466",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,Although most of the cases the information
cs-410_4_6_41,"00:03:11,466","00:03:16,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"But in the future, you can imagine"
cs-410_4_6_42,"00:03:16,305","00:03:21,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"multi-mode for information access, and"
cs-410_4_6_43,"00:03:23,160","00:03:27,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,Another trend is that we might see systems
cs-410_4_6_44,"00:03:27,160","00:03:30,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,that try to go beyond the searches
cs-410_4_6_45,"00:03:30,970","00:03:36,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"After all, the reason why people want"
cs-410_4_6_46,"00:03:36,730","00:03:39,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,to make a decision or perform a task.
cs-410_4_6_47,"00:03:39,690","00:03:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,For example consumers might search for
cs-410_4_6_48,"00:03:42,380","00:03:45,385",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,opinions about products in
cs-410_4_6_49,"00:03:45,385","00:03:50,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,"choose a good product by, so"
cs-410_4_6_50,"00:03:50,160","00:03:55,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,support the whole workflow of purchasing
cs-410_4_6_51,"00:03:56,732","00:04:00,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"In this era, after the common search"
cs-410_4_6_52,"00:04:00,300","00:04:04,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"For example, you can sometimes look at the"
cs-410_4_6_53,"00:04:04,190","00:04:09,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,you can just click on the button to go the
cs-410_4_6_54,"00:04:09,040","00:04:12,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"But it does not provide a,"
cs-410_4_6_55,"00:04:12,840","00:04:14,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"For example, for researchers,"
cs-410_4_6_56,"00:04:14,720","00:04:18,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,you might want to find the realm in
cs-410_4_6_57,"00:04:18,800","00:04:26,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,"And then, there's no, not much support for"
cs-410_4_6_58,"00:04:26,550","00:04:31,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"So, in general, I think,"
cs-410_4_6_59,"00:04:31,130","00:04:34,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,"So in the following few slides, I'll"
cs-410_4_6_60,"00:04:34,980","00:04:39,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"specific ideas or thoughts that hopefully,"
cs-410_4_6_61,"00:04:39,900","00:04:43,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,can help you in imagining new
cs-410_4_6_62,"00:04:43,720","00:04:51,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,Some of them might be already relevant
cs-410_4_6_63,"00:04:51,330","00:04:55,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,"In general, we can think about any"
cs-410_4_6_64,"00:04:55,370","00:05:02,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"information system, as we specified"
cs-410_4_6_65,"00:05:02,390","00:05:05,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,And so
cs-410_4_6_66,"00:05:05,680","00:05:09,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,then we'll able to specify
cs-410_4_6_67,"00:05:09,250","00:05:12,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,And I call this
cs-410_4_6_68,"00:05:12,480","00:05:18,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So basically the three questions you
cs-410_4_6_69,"00:05:18,110","00:05:23,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,what kind of data are you are managing and
cs-410_4_6_70,"00:05:24,580","00:05:29,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"Right there, this would help us"
cs-410_4_6_71,"00:05:30,650","00:05:33,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,And there are many different ways
cs-410_4_6_72,"00:05:33,400","00:05:36,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"how you connect them,"
cs-410_4_6_73,"00:05:36,040","00:05:37,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,So let me give you some examples.
cs-410_4_6_74,"00:05:37,650","00:05:40,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"On the top,"
cs-410_4_6_75,"00:05:40,470","00:05:45,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"On the left side, you can see different"
cs-410_4_6_76,"00:05:45,250","00:05:48,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"on the bottom,"
cs-410_4_6_77,"00:05:48,740","00:05:51,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,Now imagine you can connect
cs-410_4_6_78,"00:05:51,760","00:05:55,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"So, for example, you can connect"
cs-410_4_6_79,"00:05:55,990","00:05:59,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,the support search and
cs-410_4_6_80,"00:05:59,140","00:06:01,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"Well, that's web search, right?"
cs-410_4_6_81,"00:06:02,440","00:06:07,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,What if we connect UIUC employees with
cs-410_4_6_82,"00:06:07,680","00:06:12,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,documents to support the search and
cs-410_4_6_83,"00:06:12,720","00:06:17,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,If you connect the scientist
cs-410_4_6_84,"00:06:17,110","00:06:22,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"to provide all kinds of service,"
cs-410_4_6_85,"00:06:22,050","00:06:28,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,alert of new random documents or
cs-410_4_6_86,"00:06:28,310","00:06:31,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,or provide the task with support or
cs-410_4_6_87,"00:06:31,490","00:06:36,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"For example, we might be,"
cs-410_4_6_88,"00:06:36,600","00:06:40,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,automatically generating
cs-410_4_6_89,"00:06:40,140","00:06:44,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"a research paper, and"
cs-410_4_6_90,"00:06:44,440","00:06:45,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,Right?
cs-410_4_6_91,"00:06:45,270","00:06:48,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,we can imagine this would
cs-410_4_6_92,"00:06:48,010","00:06:52,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,If we connect the online shoppers
cs-410_4_6_93,"00:06:53,890","00:06:59,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,then we can help these people
cs-410_4_6_94,"00:06:59,825","00:07:05,465",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,"So we can provide, for example data mining"
cs-410_4_6_95,"00:07:05,465","00:07:11,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"to compare products, compare sentiment of"
cs-410_4_6_96,"00:07:11,950","00:07:15,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,decision support to have them
cs-410_4_6_97,"00:07:15,950","00:07:21,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,Or we can connect customer service
cs-410_4_6_98,"00:07:22,630","00:07:27,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"and, and we can imagine a system"
cs-410_4_6_99,"00:07:27,660","00:07:31,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,of these emails to find that the major
cs-410_4_6_100,"00:07:31,460","00:07:35,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,We can imagine a system we
cs-410_4_6_101,"00:07:35,150","00:07:39,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,by automatically generating
cs-410_4_6_102,"00:07:39,630","00:07:45,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Maybe intelligently attach
cs-410_4_6_103,"00:07:45,720","00:07:49,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"if appropriate, if they detect that that's"
cs-410_4_6_104,"00:07:49,830","00:07:55,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,then you might take this opportunity
cs-410_4_6_105,"00:07:55,290","00:07:57,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"Whereas if it's a complaint,"
cs-410_4_6_106,"00:07:59,510","00:08:03,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,automatically generate some
cs-410_4_6_107,"00:08:03,810","00:08:08,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,tell the customer that he or she can
cs-410_4_6_108,"00:08:08,790","00:08:14,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,All of these are trying to help
cs-410_4_6_109,"00:08:15,570","00:08:19,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,So this shows that
cs-410_4_6_110,"00:08:19,850","00:08:22,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,It's just only restricted
cs-410_4_6_111,"00:08:22,090","00:08:27,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So this picture shows the trend
cs-410_4_6_112,"00:08:27,400","00:08:33,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"it characterizes the, intelligent"
cs-410_4_6_113,"00:08:33,770","00:08:39,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,"You can see in the center, there's"
cs-410_4_6_114,"00:08:39,065","00:08:41,225",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,to search a bag of words representation.
cs-410_4_6_115,"00:08:41,225","00:08:46,721",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,That means the current search engines
cs-410_4_6_116,"00:08:46,721","00:08:54,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,to users and mostly model
cs-410_4_6_117,"00:08:54,085","00:08:59,105",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,and sees the data through
cs-410_4_6_118,"00:08:59,105","00:09:06,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,So it's a very simple approximation of
cs-410_4_6_119,"00:09:06,190","00:09:08,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,But that's what the current system does.
cs-410_4_6_120,"00:09:08,500","00:09:12,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,It connects these three nodes
cs-410_4_6_121,"00:09:12,150","00:09:17,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,it only provides a basic search function
cs-410_4_6_122,"00:09:17,655","00:09:24,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,and it doesn't really understand that
cs-410_4_6_123,"00:09:24,405","00:09:31,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"Now, I showed some trends to push each"
cs-410_4_6_124,"00:09:31,862","00:09:35,332",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"So think about the user node here, right?"
cs-410_4_6_125,"00:09:35,332","00:09:39,432",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"So we can go beyond the keyword queries,"
cs-410_4_6_126,"00:09:39,432","00:09:43,882",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,and then further model the user
cs-410_4_6_127,"00:09:43,882","00:09:49,622",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"the user's task environment,"
cs-410_4_6_128,"00:09:49,622","00:09:55,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"Okay, so this is pushing for"
cs-410_4_6_129,"00:09:55,120","00:09:58,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,And this is a major
cs-410_4_6_130,"00:09:58,630","00:10:01,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,in order to build intelligent
cs-410_4_6_131,"00:10:01,810","00:10:05,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"On the document side,"
cs-410_4_6_132,"00:10:05,810","00:10:10,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,go beyond bag of words implementation
cs-410_4_6_133,"00:10:10,640","00:10:16,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,"This means we'll recognize people's names,"
cs-410_4_6_134,"00:10:16,040","00:10:20,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,And this is already feasible with
cs-410_4_6_135,"00:10:20,430","00:10:24,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,And Google is the reason
cs-410_4_6_136,"00:10:24,130","00:10:28,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,"If you haven't heard of it,"
cs-410_4_6_137,"00:10:28,310","00:10:33,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,And once we can get to that level without
cs-410_4_6_138,"00:10:33,820","00:10:38,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,it can enable the search engine
cs-410_4_6_139,"00:10:38,170","00:10:41,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,In the future we would like to have
cs-410_4_6_140,"00:10:41,470","00:10:45,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,knowledge representation where we
cs-410_4_6_141,"00:10:45,450","00:10:47,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,then the search engine would
cs-410_4_6_142,"00:10:49,390","00:10:53,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,So this calls for
cs-410_4_6_143,"00:10:53,490","00:10:57,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,perhaps this is more feasible for
cs-410_4_6_144,"00:10:57,150","00:10:59,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,It's easier to make progress
cs-410_4_6_145,"00:10:59,800","00:11:01,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"Now on the service side,"
cs-410_4_6_146,"00:11:01,240","00:11:05,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,we see we need to go beyond the search of
cs-410_4_6_147,"00:11:07,510","00:11:13,702",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,So search is only one way to get access
cs-410_4_6_148,"00:11:13,702","00:11:19,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,systems and push and pull so different
cs-410_4_6_149,"00:11:19,980","00:11:21,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"But going beyond access,"
cs-410_4_6_150,"00:11:21,630","00:11:25,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,we also need to help people digest the
cs-410_4_6_151,"00:11:25,820","00:11:30,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,and this step has to do with analysis
cs-410_4_6_152,"00:11:30,560","00:11:35,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,We have to find patterns or
cs-410_4_6_153,"00:11:35,540","00:11:38,865",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,real knowledge that can
cs-410_4_6_154,"00:11:38,865","00:11:43,055",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,actionable knowledge that can be used for
cs-410_4_6_155,"00:11:43,055","00:11:47,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,And furthermore the knowledge
cs-410_4_6_156,"00:11:47,165","00:11:52,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"improve productivity in finishing a task,"
cs-410_4_6_157,"00:11:52,580","00:11:54,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"Right, so this is a trend."
cs-410_4_6_158,"00:11:54,000","00:11:59,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,"And, and, and so basically,"
cs-410_4_6_159,"00:11:59,370","00:12:04,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,in the future intelligent information
cs-410_4_6_160,"00:12:04,210","00:12:06,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,interactive task support.
cs-410_4_6_161,"00:12:06,940","00:12:11,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,Now I should also emphasize interactive
cs-410_4_6_162,"00:12:11,200","00:12:16,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,the combined intelligence of the users and
cs-410_4_6_163,"00:12:16,790","00:12:22,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"So we, we can get some help"
cs-410_4_6_164,"00:12:22,140","00:12:26,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=742,And we don't have to assume the system
cs-410_4_6_165,"00:12:26,970","00:12:32,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,"user, and the machine can collaborate in"
cs-410_4_6_166,"00:12:32,290","00:12:37,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,then the combined intelligence
cs-410_4_6_167,"00:12:37,270","00:12:41,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,we can minimize the user's overall
cs-410_4_6_168,"00:12:42,700","00:12:47,947",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,So this is the big picture of future
cs-410_4_6_169,"00:12:47,947","00:12:52,582",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,and this hopefully can provide
cs-410_4_6_170,"00:12:52,582","00:12:57,313",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,how to make further innovations
cs-410_4_6_171,"00:12:57,313","00:13:07,313",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,[MUSIC]
cs-410_3_3_1,"00:00:00,199","00:00:03,699",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_3_3_2,"00:00:07,099","00:00:11,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,"This lecture is about,"
cs-410_3_3_3,"00:00:13,290","00:00:17,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we will continue"
cs-410_3_3_4,"00:00:17,810","00:00:18,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"In particular,"
cs-410_3_3_5,"00:00:18,470","00:00:21,799",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"we are going to look at, how we can"
cs-410_3_3_6,"00:00:24,970","00:00:30,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"In the previous lecture,"
cs-410_3_3_7,"00:00:30,410","00:00:33,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"These are the two basic measures for,"
cs-410_3_3_8,"00:00:33,430","00:00:38,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,quantitatively measuring
cs-410_3_3_9,"00:00:40,420","00:00:44,247",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"But, as we talked about, ranking, before,"
cs-410_3_3_10,"00:00:44,247","00:00:49,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,we framed that the text of retrieval
cs-410_3_3_11,"00:00:50,800","00:00:55,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,"So, we also need to evaluate the,"
cs-410_3_3_12,"00:00:56,910","00:01:01,097",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,How can we use precision-recall
cs-410_3_3_13,"00:01:01,097","00:01:07,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Well, naturally, we have to look after the"
cs-410_3_3_14,"00:01:07,180","00:01:12,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"Because in the end, the approximation"
cs-410_3_3_15,"00:01:12,330","00:01:17,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"given by a ranked list, is determined"
cs-410_3_3_16,"00:01:17,640","00:01:21,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,Right?
cs-410_3_3_17,"00:01:21,470","00:01:25,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"the list of results, the user would,"
cs-410_3_3_18,"00:01:25,520","00:01:27,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,that point would determine the set.
cs-410_3_3_19,"00:01:27,790","00:01:31,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"And then,"
cs-410_3_3_20,"00:01:31,680","00:01:35,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"that we have to consider,"
cs-410_3_3_21,"00:01:35,400","00:01:37,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,Without knowing where
cs-410_3_3_22,"00:01:37,990","00:01:42,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"then we have to consider, all"
cs-410_3_3_23,"00:01:42,380","00:01:44,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"So, let's look at these positions."
cs-410_3_3_24,"00:01:44,720","00:01:49,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"Look at this slide, and"
cs-410_3_3_25,"00:01:49,020","00:01:51,718",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"what if the user stops at the,"
cs-410_3_3_26,"00:01:51,718","00:01:55,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,What's the precision-recall at this point?
cs-410_3_3_27,"00:01:55,140","00:01:55,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,What do you think?
cs-410_3_3_28,"00:01:56,970","00:02:02,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"Well, it's easy to see, that this document"
cs-410_3_3_29,"00:02:02,920","00:02:05,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"We have, got one document,"
cs-410_3_3_30,"00:02:05,960","00:02:07,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,What about the recall?
cs-410_3_3_31,"00:02:07,380","00:02:11,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"Well, note that, we're assuming that,"
cs-410_3_3_32,"00:02:11,990","00:02:14,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"this query in the collection,"
cs-410_3_3_33,"00:02:16,310","00:02:18,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,What if the user stops
cs-410_3_3_34,"00:02:19,820","00:02:20,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,Top two.
cs-410_3_3_35,"00:02:21,470","00:02:25,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"Well, the precision is the same,"
cs-410_3_3_36,"00:02:25,820","00:02:27,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"And, the record is two out of ten."
cs-410_3_3_37,"00:02:28,600","00:02:31,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,What if the user stops
cs-410_3_3_38,"00:02:31,630","00:02:35,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"Well, this is interesting,"
cs-410_3_3_39,"00:02:35,980","00:02:40,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"additional relevant document,"
cs-410_3_3_40,"00:02:41,170","00:02:45,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"But the precision is lower,"
cs-410_3_3_41,"00:02:45,600","00:02:46,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,what's exactly the precision?
cs-410_3_3_42,"00:02:49,110","00:02:52,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"Well, it's two out of three, right?"
cs-410_3_3_43,"00:02:52,020","00:02:54,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"And, recall is the same, two out of ten."
cs-410_3_3_44,"00:02:54,920","00:02:58,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"So, when would see another point,"
cs-410_3_3_45,"00:02:58,930","00:03:02,473",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"Now, if you look down the list,"
cs-410_3_3_46,"00:03:02,473","00:03:06,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,"we have, seeing another relevant document."
cs-410_3_3_47,"00:03:06,110","00:03:10,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"In this case D5, at that point, the,"
cs-410_3_3_48,"00:03:10,800","00:03:13,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"three out of ten, and,"
cs-410_3_3_49,"00:03:15,150","00:03:20,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"So, you can see, if we keep doing this,"
cs-410_3_3_50,"00:03:20,200","00:03:23,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"And then, we will have"
cs-410_3_3_51,"00:03:23,780","00:03:26,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"because there are eight documents,"
cs-410_3_3_52,"00:03:26,150","00:03:28,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"And, the recall is a four out of ten."
cs-410_3_3_53,"00:03:29,540","00:03:33,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Now, when can we get,"
cs-410_3_3_54,"00:03:33,500","00:03:39,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"Well, in this list, we don't have it,"
cs-410_3_3_55,"00:03:39,740","00:03:40,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"We don't know, where it is?"
cs-410_3_3_56,"00:03:40,560","00:03:45,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,"But, as convenience, we often assume that,"
cs-410_3_3_57,"00:03:47,230","00:03:51,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"at all the, the othe,"
cs-410_3_3_58,"00:03:51,890","00:03:56,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"all the other levels of recall,"
cs-410_3_3_59,"00:03:56,550","00:03:59,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"So, of course,"
cs-410_3_3_60,"00:03:59,140","00:04:04,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"the actual position would be higher,"
cs-410_3_3_61,"00:04:05,230","00:04:09,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"in order to, have an easy way to,"
cs-410_3_3_62,"00:04:09,390","00:04:12,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,compute another measure called Average
cs-410_3_3_63,"00:04:14,300","00:04:16,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"Now, I should also say, now, here you see,"
cs-410_3_3_64,"00:04:16,560","00:04:21,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,we make these assumptions that
cs-410_3_3_65,"00:04:22,270","00:04:28,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"But, this is okay, for"
cs-410_3_3_66,"00:04:28,950","00:04:34,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,"And, this is for the relative comparison,"
cs-410_3_3_67,"00:04:34,870","00:04:39,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"or actual, actual number deviates"
cs-410_3_3_68,"00:04:39,560","00:04:41,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"As long as the deviation,"
cs-410_3_3_69,"00:04:41,970","00:04:46,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,is not biased toward any particular
cs-410_3_3_70,"00:04:46,810","00:04:50,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"We can still,"
cs-410_3_3_71,"00:04:50,560","00:04:53,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"And, this is important point,"
cs-410_3_3_72,"00:04:53,360","00:04:55,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"When you compare different algorithms,"
cs-410_3_3_73,"00:04:55,550","00:04:58,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,the key's to avoid any
cs-410_3_3_74,"00:04:58,810","00:05:02,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"And, as long as, you can avoid that."
cs-410_3_3_75,"00:05:02,130","00:05:06,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"It's okay, for you to do transformation"
cs-410_3_3_76,"00:05:06,580","00:05:07,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,you can preserve the order.
cs-410_3_3_77,"00:05:09,380","00:05:11,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"Okay, so, we'll just talk about,"
cs-410_3_3_78,"00:05:11,170","00:05:16,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,we can get a lot of precision-recall
cs-410_3_3_79,"00:05:16,030","00:05:19,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,"So, now, you can imagine,"
cs-410_3_3_80,"00:05:19,000","00:05:22,389",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"And, this just shows on the,"
cs-410_3_3_81,"00:05:23,610","00:05:30,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,"And, on the y-axis, we show the precision."
cs-410_3_3_82,"00:05:30,110","00:05:35,336",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"So, the precision line was marked as .1,"
cs-410_3_3_83,"00:05:35,336","00:05:35,998",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,Right?
cs-410_3_3_84,"00:05:35,998","00:05:38,618",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"this is, the different, levels of recall."
cs-410_3_3_85,"00:05:38,618","00:05:44,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"And,, the y-axis also has,"
cs-410_3_3_86,"00:05:45,450","00:05:49,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"So, we plot the, these, precision-recall"
cs-410_3_3_87,"00:05:49,150","00:05:51,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,as points on this picture.
cs-410_3_3_88,"00:05:51,360","00:05:56,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"Now, we can further, and"
cs-410_3_3_89,"00:05:56,410","00:05:57,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"As you'll see,"
cs-410_3_3_90,"00:05:57,290","00:06:02,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"we assumed all the other, precision"
cs-410_3_3_91,"00:06:02,040","00:06:08,232",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"And, that's why, they are down here,"
cs-410_3_3_92,"00:06:08,232","00:06:14,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"And this, the actual curve probably will"
cs-410_3_3_93,"00:06:14,980","00:06:19,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"discussed, it, it doesn't matter that"
cs-410_3_3_94,"00:06:20,430","00:06:24,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,"because this would be,"
cs-410_3_3_95,"00:06:25,950","00:06:31,016",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"Okay, so, now that we,"
cs-410_3_3_96,"00:06:31,016","00:06:34,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,how can we compare ranked to back list?
cs-410_3_3_97,"00:06:34,290","00:06:37,049",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"All right, so, that means,"
cs-410_3_3_98,"00:06:38,430","00:06:40,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"And here, we show, two cases."
cs-410_3_3_99,"00:06:40,880","00:06:47,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"Where system A is showing red,"
cs-410_3_3_100,"00:06:48,610","00:06:50,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"All right, so, which one is better?"
cs-410_3_3_101,"00:06:50,820","00:06:54,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"I hope you can see,"
cs-410_3_3_102,"00:06:54,080","00:06:56,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,Why?
cs-410_3_3_103,"00:06:58,340","00:07:01,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"see same level of recall here,"
cs-410_3_3_104,"00:07:01,500","00:07:06,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"the precision point by system A is better,"
cs-410_3_3_105,"00:07:06,800","00:07:08,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"So, there's no question."
cs-410_3_3_106,"00:07:08,260","00:07:13,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,"In here, you can imagine, what does the"
cs-410_3_3_107,"00:07:13,360","00:07:17,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"Well, it has to have perfect,"
cs-410_3_3_108,"00:07:17,470","00:07:18,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,it has to be this line.
cs-410_3_3_109,"00:07:18,450","00:07:21,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,That would be the ideal system.
cs-410_3_3_110,"00:07:21,300","00:07:24,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,"In general, the higher the curve is,"
cs-410_3_3_111,"00:07:24,230","00:07:27,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"The problem is that,"
cs-410_3_3_112,"00:07:27,160","00:07:29,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,This actually happens often.
cs-410_3_3_113,"00:07:29,110","00:07:30,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,"Like, the two curves cross each other."
cs-410_3_3_114,"00:07:32,430","00:07:34,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"Now, in this case, which one is better?"
cs-410_3_3_115,"00:07:35,300","00:07:35,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,What do you think?
cs-410_3_3_116,"00:07:38,240","00:07:41,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"Now, this is a real problem,"
cs-410_3_3_117,"00:07:41,730","00:07:47,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"Suppose, you build a search engine,"
cs-410_3_3_118,"00:07:47,150","00:07:50,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"that's shown here in blue, or system B."
cs-410_3_3_119,"00:07:50,990","00:07:53,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"And, you have come up with a new idea."
cs-410_3_3_120,"00:07:53,580","00:07:54,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"And, you test it."
cs-410_3_3_121,"00:07:54,500","00:07:58,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"And, the results are shown in red,"
cs-410_3_3_122,"00:07:59,990","00:08:04,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"Now, your question is, is your new"
cs-410_3_3_123,"00:08:05,630","00:08:10,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"Or more, practically,"
cs-410_3_3_124,"00:08:10,510","00:08:15,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"you're already using, your, in your search"
cs-410_3_3_125,"00:08:15,410","00:08:20,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"So, should we use system,"
cs-410_3_3_126,"00:08:20,760","00:08:23,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"This is going to be a real decision,"
cs-410_3_3_127,"00:08:23,250","00:08:29,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,"If you make the replacement, the search"
cs-410_3_3_128,"00:08:29,430","00:08:34,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"whereas, if you don't do that,"
cs-410_3_3_129,"00:08:34,170","00:08:34,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,"So, what do you do?"
cs-410_3_3_130,"00:08:36,210","00:08:40,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"Now, if you want to spend more time"
cs-410_3_3_131,"00:08:40,580","00:08:42,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"And, it's actually very"
cs-410_3_3_132,"00:08:42,840","00:08:46,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"As I said, it's a real decision that you"
cs-410_3_3_133,"00:08:46,350","00:08:51,329",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"search engine, or if you're working, for"
cs-410_3_3_134,"00:08:52,330","00:08:54,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"Now, if you have thought about this for"
cs-410_3_3_135,"00:08:54,630","00:08:59,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"a moment, you might realize that,"
cs-410_3_3_136,"00:08:59,630","00:09:04,615",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,"Now, some users might like a system A,"
cs-410_3_3_137,"00:09:04,615","00:09:05,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"So, what's the difference here?"
cs-410_3_3_138,"00:09:05,895","00:09:08,545",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"Well, the difference is just that,"
cs-410_3_3_139,"00:09:08,545","00:09:14,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"in the, low level of recall,"
cs-410_3_3_140,"00:09:14,145","00:09:15,845",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,There's a higher precision.
cs-410_3_3_141,"00:09:15,845","00:09:19,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"But in high recall region,"
cs-410_3_3_142,"00:09:20,910","00:09:24,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"Now, so, that also means,"
cs-410_3_3_143,"00:09:24,040","00:09:28,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"cares about the high recall, or"
cs-410_3_3_144,"00:09:28,630","00:09:32,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"You can imagine, if someone is just going"
cs-410_3_3_145,"00:09:32,200","00:09:33,489",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,want to find out something
cs-410_3_3_146,"00:09:34,750","00:09:36,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"Well, which one is better?"
cs-410_3_3_147,"00:09:36,530","00:09:37,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,What do you think?
cs-410_3_3_148,"00:09:38,110","00:09:41,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,"In this case, clearly, system B is better,"
cs-410_3_3_149,"00:09:41,510","00:09:44,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,because the user is unlikely
cs-410_3_3_150,"00:09:44,920","00:09:46,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,The user doesn't care about high recall.
cs-410_3_3_151,"00:09:47,780","00:09:50,673",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"On the other hand,"
cs-410_3_3_152,"00:09:50,673","00:09:54,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"where a user is doing you are,"
cs-410_3_3_153,"00:09:54,800","00:10:00,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,"You want to find, whether your idea ha,"
cs-410_3_3_154,"00:10:00,320","00:10:03,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,"In that case, you emphasize high recall."
cs-410_3_3_155,"00:10:03,130","00:10:06,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,"So, you want to see,"
cs-410_3_3_156,"00:10:06,630","00:10:09,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"Therefore, you might, favor, system A."
cs-410_3_3_157,"00:10:09,570","00:10:12,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"So, that means, which one is better?"
cs-410_3_3_158,"00:10:12,090","00:10:18,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"That actually depends on users,"
cs-410_3_3_159,"00:10:19,520","00:10:24,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,"So, this means, you may not necessarily"
cs-410_3_3_160,"00:10:25,290","00:10:28,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,that would accurately
cs-410_3_3_161,"00:10:29,860","00:10:31,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,You have to look at the overall picture.
cs-410_3_3_162,"00:10:31,750","00:10:35,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,"Yet, as I said, when you have"
cs-410_3_3_163,"00:10:35,620","00:10:38,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"whether you replace ours with another,"
cs-410_3_3_164,"00:10:38,210","00:10:44,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,then you may have to actually come up with
cs-410_3_3_165,"00:10:44,320","00:10:49,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"Or, when we compare many different"
cs-410_3_3_166,"00:10:49,800","00:10:54,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,"one number to compare, them with, so, that"
cs-410_3_3_167,"00:10:54,590","00:11:00,258",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"So, for all these reasons, it is desirable"
cs-410_3_3_168,"00:11:00,258","00:11:01,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"So, how do we do that?"
cs-410_3_3_169,"00:11:01,510","00:11:05,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,"And, that,"
cs-410_3_3_170,"00:11:05,860","00:11:09,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,"So, here again it's"
cs-410_3_3_171,"00:11:09,560","00:11:13,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,"And, one way to summarize"
cs-410_3_3_172,"00:11:13,570","00:11:18,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,"this whole curve,"
cs-410_3_3_173,"00:11:19,330","00:11:21,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,Right?
cs-410_3_3_174,"00:11:21,820","00:11:25,209",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"There are other ways to measure that,"
cs-410_3_3_175,"00:11:26,430","00:11:31,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,this particular way of matching
cs-410_3_3_176,"00:11:31,110","00:11:36,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"has been used, since a long time ago for"
cs-410_3_3_177,"00:11:36,260","00:11:41,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"basically, in this way, and"
cs-410_3_3_178,"00:11:41,140","00:11:46,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,"Basically, we're going to take a, a look"
cs-410_3_3_179,"00:11:47,600","00:11:49,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"And then, look out for the precision."
cs-410_3_3_180,"00:11:49,540","00:11:51,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,"So, we know, you know,"
cs-410_3_3_181,"00:11:51,930","00:11:56,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,"And, this is another,"
cs-410_3_3_182,"00:11:56,590","00:11:59,362",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,"Now, this, we don't count to this one,"
cs-410_3_3_183,"00:11:59,362","00:12:04,511",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,"because the recall level is the same,"
cs-410_3_3_184,"00:12:04,511","00:12:10,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"this number, and that's precision at"
cs-410_3_3_185,"00:12:10,120","00:12:13,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,"So, we have all these, you know, added up."
cs-410_3_3_186,"00:12:13,580","00:12:16,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,These are the precisions
cs-410_3_3_187,"00:12:16,180","00:12:21,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,corresponding to retrieving the first
cs-410_3_3_188,"00:12:21,130","00:12:25,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,"then, the third, that follows, et cetera."
cs-410_3_3_189,"00:12:25,260","00:12:29,265",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,"Now, we missed the many relevant"
cs-410_3_3_190,"00:12:29,265","00:12:32,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"we just, assume,"
cs-410_3_3_191,"00:12:33,540","00:12:35,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"And then, finally, we take the average."
cs-410_3_3_192,"00:12:35,740","00:12:37,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,"So, we divide it by ten, and"
cs-410_3_3_193,"00:12:37,900","00:12:40,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,which is the total number of relevant
cs-410_3_3_194,"00:12:41,670","00:12:46,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"Note that here,"
cs-410_3_3_195,"00:12:46,610","00:12:49,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,Which is a number retrieved
cs-410_3_3_196,"00:12:49,440","00:12:52,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"Now, imagine, if I divide by four,"
cs-410_3_3_197,"00:12:54,370","00:12:55,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,"Now, think about this, for a moment."
cs-410_3_3_198,"00:12:57,050","00:13:01,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,"It's a common mistake that people,"
cs-410_3_3_199,"00:13:02,720","00:13:08,208",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=782,"Right, so, if we, we divide this by four,"
cs-410_3_3_200,"00:13:08,208","00:13:13,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"In fact, that you are favoring a system,"
cs-410_3_3_201,"00:13:13,180","00:13:18,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,"documents, as in that case,"
cs-410_3_3_202,"00:13:18,785","00:13:22,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"So, this would be, not a good matching."
cs-410_3_3_203,"00:13:22,115","00:13:25,862",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,"So, note that this denomina,"
cs-410_3_3_204,"00:13:25,862","00:13:29,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,the total number of relevant documents.
cs-410_3_3_205,"00:13:29,170","00:13:33,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,"And, this will basically ,compute"
cs-410_3_3_206,"00:13:33,620","00:13:40,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,"And, this is the standard method,"
cs-410_3_3_207,"00:13:41,210","00:13:44,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,"Note that, it actually combines"
cs-410_3_3_208,"00:13:44,860","00:13:49,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,"But first, you know, we have"
cs-410_3_3_209,"00:13:49,230","00:13:53,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,"we also consider recall, because if missed"
cs-410_3_3_210,"00:13:53,240","00:13:57,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"All right, so,"
cs-410_3_3_211,"00:13:57,470","00:14:02,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,"And furthermore, you can see this"
cs-410_3_3_212,"00:14:02,190","00:14:04,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,of a position of a relevant document.
cs-410_3_3_213,"00:14:04,770","00:14:09,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"Let's say, if I move this relevant"
cs-410_3_3_214,"00:14:09,520","00:14:12,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,"it would increase this means,"
cs-410_3_3_215,"00:14:12,670","00:14:17,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,"Whereas, if I move any relevant document,"
cs-410_3_3_216,"00:14:17,630","00:14:23,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,"document down, then it would decrease,"
cs-410_3_3_217,"00:14:23,720","00:14:25,266",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,"So, this is a very good,"
cs-410_3_3_218,"00:14:25,266","00:14:30,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,because it's a very sensitive to
cs-410_3_3_219,"00:14:30,570","00:14:34,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,"It can tell, small differences"
cs-410_3_3_220,"00:14:34,740","00:14:35,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,"And, that is what we want,"
cs-410_3_3_221,"00:14:35,880","00:14:40,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,sometimes one algorithm only works
cs-410_3_3_222,"00:14:40,430","00:14:42,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,"And, we want to see this difference."
cs-410_3_3_223,"00:14:42,440","00:14:46,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,"In contrast, if we look at"
cs-410_3_3_224,"00:14:46,110","00:14:49,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,"If we look at this, this whole set, well,"
cs-410_3_3_225,"00:14:49,520","00:14:52,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,"what, what's the precision,"
cs-410_3_3_226,"00:14:52,000","00:14:54,328",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=892,"Well, it's easy to see,"
cs-410_3_3_227,"00:14:54,328","00:15:02,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"So, that precision is very meaningful,"
cs-410_3_3_228,"00:15:02,200","00:15:04,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,"So, that's pretty useful, right?"
cs-410_3_3_229,"00:15:04,580","00:15:07,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,"So, it's a meaningful measure,"
cs-410_3_3_230,"00:15:07,850","00:15:11,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,"But, if we use this measure to"
cs-410_3_3_231,"00:15:11,770","00:15:16,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,because it wouldn't be sensitive to where
cs-410_3_3_232,"00:15:16,480","00:15:21,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,If I move them around the precision
cs-410_3_3_233,"00:15:21,910","00:15:22,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,Right.
cs-410_3_3_234,"00:15:22,570","00:15:25,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,this is not a good measure for
cs-410_3_3_235,"00:15:25,590","00:15:29,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,"In contrast, the average precision"
cs-410_3_3_236,"00:15:29,990","00:15:34,511",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=929,"It can tell the difference of, different,"
cs-410_3_3_237,"00:15:34,511","00:15:39,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,"a difference in ranked list in,"
cs-410_3_3_238,"00:15:39,269","00:15:49,269",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,[MUSIC]
cs-410_4_3_1,"00:00:00,012","00:00:03,467",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_3_2,"00:00:11,647","00:00:13,963",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,So average precision is computer for
cs-410_4_3_3,"00:00:13,963","00:00:14,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,just one.
cs-410_4_3_4,"00:00:14,690","00:00:18,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,one query.
cs-410_4_3_5,"00:00:18,290","00:00:24,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,different queries and this is to
cs-410_4_3_6,"00:00:24,570","00:00:29,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,Depending on the queries you use you
cs-410_4_3_7,"00:00:29,530","00:00:31,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"Right, so"
cs-410_4_3_8,"00:00:33,610","00:00:36,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"If you use more queries then,"
cs-410_4_3_9,"00:00:36,580","00:00:39,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,take the average of the average
cs-410_4_3_10,"00:00:41,600","00:00:42,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,So how can we do that?
cs-410_4_3_11,"00:00:43,560","00:00:46,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,"Well, you can naturally."
cs-410_4_3_12,"00:00:46,160","00:00:49,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,Think of just doing arithmetic mean as we
cs-410_4_3_13,"00:00:50,670","00:00:56,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,"always tend to, to think in, in this way."
cs-410_4_3_14,"00:00:56,000","00:01:02,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"So, this would give us what's called"
cs-410_4_3_15,"00:01:02,000","00:01:02,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"In this case,"
cs-410_4_3_16,"00:01:02,540","00:01:08,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,we take arithmetic mean of all the average
cs-410_4_3_17,"00:01:09,930","00:01:13,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,But as I just mentioned in
cs-410_4_3_18,"00:01:15,370","00:01:16,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,We call that.
cs-410_4_3_19,"00:01:16,580","00:01:21,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,We talked about the different ways
cs-410_4_3_20,"00:01:21,190","00:01:27,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,And we conclude that the arithmetic
cs-410_4_3_21,"00:01:27,340","00:01:28,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,But here it's the same.
cs-410_4_3_22,"00:01:28,420","00:01:32,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,We can also think about the alternative
cs-410_4_3_23,"00:01:32,120","00:01:34,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"Don't just automatically assume that,"
cs-410_4_3_24,"00:01:34,850","00:01:37,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,Let's just also take the arithmetic
cs-410_4_3_25,"00:01:37,785","00:01:38,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,these queries.
cs-410_4_3_26,"00:01:38,590","00:01:42,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,Let's think about what's
cs-410_4_3_27,"00:01:42,910","00:01:46,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"If you think about the different ways,"
cs-410_4_3_28,"00:01:46,660","00:01:49,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,probably be able to think about
cs-410_4_3_29,"00:01:51,230","00:01:53,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,And we call this kind of average a gMAP.
cs-410_4_3_30,"00:01:55,650","00:01:56,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,This is another way.
cs-410_4_3_31,"00:01:56,860","00:01:59,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"So now, once you think about"
cs-410_4_3_32,"00:01:59,520","00:02:00,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,Of doing the same thing.
cs-410_4_3_33,"00:02:00,820","00:02:03,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"The natural question to ask is,"
cs-410_4_3_34,"00:02:03,400","00:02:03,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,So.
cs-410_4_3_35,"00:02:05,230","00:02:08,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So, do you use MAP or gMAP?"
cs-410_4_3_36,"00:02:09,830","00:02:11,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,"Again, that's important question."
cs-410_4_3_37,"00:02:11,200","00:02:14,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,Imagine you are again
cs-410_4_3_38,"00:02:14,490","00:02:17,109",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,by comparing the ways your old
cs-410_4_3_39,"00:02:18,390","00:02:22,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,Now you tested multiple topics.
cs-410_4_3_40,"00:02:22,080","00:02:25,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,Now you've got the average precision for
cs-410_4_3_41,"00:02:25,150","00:02:28,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,Now you are thinking of looking
cs-410_4_3_42,"00:02:28,470","00:02:29,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,You have to take the average.
cs-410_4_3_43,"00:02:30,950","00:02:32,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,"But which, which strategy would you use?"
cs-410_4_3_44,"00:02:34,040","00:02:38,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"Now first, you should also think about the"
cs-410_4_3_45,"00:02:38,360","00:02:43,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,Can you think of scenarios where using
cs-410_4_3_46,"00:02:43,100","00:02:45,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,That is they would give different
cs-410_4_3_47,"00:02:45,920","00:02:52,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,And that also means depending on
cs-410_4_3_48,"00:02:52,440","00:02:54,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Average of these average positions.
cs-410_4_3_49,"00:02:55,600","00:02:57,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,You will get different conclusions.
cs-410_4_3_50,"00:02:57,460","00:03:00,379",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,This makes the question
cs-410_4_3_51,"00:03:01,620","00:03:03,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,Right?
cs-410_4_3_52,"00:03:05,350","00:03:08,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"Well again, if you look at"
cs-410_4_3_53,"00:03:08,320","00:03:12,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,Different ways of aggregating
cs-410_4_3_54,"00:03:12,750","00:03:18,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"You'll realize in arithmetic mean,"
cs-410_4_3_55,"00:03:18,510","00:03:20,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,So what does large value here mean?
cs-410_4_3_56,"00:03:20,250","00:03:22,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,It means the query is relatively easy.
cs-410_4_3_57,"00:03:22,260","00:03:24,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"You can have a high pres,"
cs-410_4_3_58,"00:03:25,950","00:03:29,707",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Whereas gMAP tends to be
cs-410_4_3_59,"00:03:30,870","00:03:34,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,And those are the queries that
cs-410_4_3_60,"00:03:34,790","00:03:36,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,The average precision is low.
cs-410_4_3_61,"00:03:37,410","00:03:41,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"So if you think about the,"
cs-410_4_3_62,"00:03:41,260","00:03:45,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"those difficult queries,"
cs-410_4_3_63,"00:03:47,480","00:03:50,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"On the other hand, if you just want to."
cs-410_4_3_64,"00:03:50,000","00:03:50,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,Have improved a lot.
cs-410_4_3_65,"00:03:52,060","00:03:55,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,Over all the kinds of queries or
cs-410_4_3_66,"00:03:55,750","00:04:00,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,easy and you want to make the perfect and
cs-410_4_3_67,"00:04:00,890","00:04:05,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"So again, the answer depends on"
cs-410_4_3_68,"00:04:05,150","00:04:06,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"their pref, their preferences."
cs-410_4_3_69,"00:04:08,020","00:04:13,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,So the point that here is to think
cs-410_4_3_70,"00:04:13,720","00:04:18,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"the same problem, and then compare them,"
cs-410_4_3_71,"00:04:18,750","00:04:20,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,And which one makes more sense.
cs-410_4_3_72,"00:04:20,610","00:04:24,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"Often, when one of them might"
cs-410_4_3_73,"00:04:24,970","00:04:27,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,another might make more sense
cs-410_4_3_74,"00:04:27,640","00:04:31,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So it's important to pick out under
cs-410_4_3_75,"00:04:35,209","00:04:38,967",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,As a special case of the mean average
cs-410_4_3_76,"00:04:38,967","00:04:43,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,the case where there was precisely
cs-410_4_3_77,"00:04:43,100","00:04:47,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,"And this happens often, for example,"
cs-410_4_3_78,"00:04:47,210","00:04:52,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"Where you know a target page, let's"
cs-410_4_3_79,"00:04:52,670","00:04:56,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"You have one relevant document there,"
cs-410_4_3_80,"00:04:56,140","00:04:58,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"That's call a ""known item search""."
cs-410_4_3_81,"00:04:58,250","00:05:01,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"In that case,"
cs-410_4_3_82,"00:05:01,330","00:05:03,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"Or in another application,"
cs-410_4_3_83,"00:05:03,470","00:05:04,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,maybe there's only one answer.
cs-410_4_3_84,"00:05:04,640","00:05:05,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,Are there.
cs-410_4_3_85,"00:05:05,250","00:05:07,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"So if you rank the answers,"
cs-410_4_3_86,"00:05:07,110","00:05:12,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,then your goal is to rank that one
cs-410_4_3_87,"00:05:12,100","00:05:16,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"So in this case, you can easily"
cs-410_4_3_88,"00:05:16,480","00:05:21,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,will basically boil down
cs-410_4_3_89,"00:05:21,710","00:05:28,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,"That is, 1 over r where r is the rank"
cs-410_4_3_90,"00:05:28,220","00:05:32,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,So if that document is ranked
cs-410_4_3_91,"00:05:32,210","00:05:35,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,then it's 1 for reciprocal rank.
cs-410_4_3_92,"00:05:35,930","00:05:39,515",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"If it's ranked at the,"
cs-410_4_3_93,"00:05:39,515","00:05:40,015",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,Et cetera.
cs-410_4_3_94,"00:05:41,145","00:05:45,335",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"And then we can also take a, a average"
cs-410_4_3_95,"00:05:45,335","00:05:48,025",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"reciprocal rank over a set of topics, and"
cs-410_4_3_96,"00:05:48,025","00:05:52,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,that would give us something
cs-410_4_3_97,"00:05:52,555","00:05:54,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,It's a very popular measure.
cs-410_4_3_98,"00:05:54,830","00:05:57,273",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"For no item search or, you know,"
cs-410_4_3_99,"00:05:57,273","00:06:01,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,an problem where you have
cs-410_4_3_100,"00:06:03,070","00:06:09,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"Now again here, you can see this"
cs-410_4_3_101,"00:06:09,170","00:06:13,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And this r is basically
cs-410_4_3_102,"00:06:13,570","00:06:18,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,a user would have to make in order
cs-410_4_3_103,"00:06:18,700","00:06:23,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,If it's ranked on the top it's low effort
cs-410_4_3_104,"00:06:23,780","00:06:26,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,But if it's ranked at 100
cs-410_4_3_105,"00:06:27,940","00:06:32,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,read presumably 100 documents
cs-410_4_3_106,"00:06:32,260","00:06:37,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"So, in this sense r is also a meaningful"
cs-410_4_3_107,"00:06:37,380","00:06:41,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"take the reciprocal of r,"
cs-410_4_3_108,"00:06:42,750","00:06:45,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So my natural question here
cs-410_4_3_109,"00:06:45,895","00:06:50,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,I imagine if you were to design
cs-410_4_3_110,"00:06:50,550","00:06:54,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"of a random system,"
cs-410_4_3_111,"00:06:55,760","00:07:00,070",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,You might have thought about
cs-410_4_3_112,"00:07:00,070","00:07:02,906",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"After all,"
cs-410_4_3_113,"00:07:02,906","00:07:10,959",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"But, think about if you take a average"
cs-410_4_3_114,"00:07:12,200","00:07:13,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,Again it would make a difference.
cs-410_4_3_115,"00:07:13,730","00:07:16,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"Right, for one single topic, using r or"
cs-410_4_3_116,"00:07:16,330","00:07:19,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,using 1 over r wouldn't
cs-410_4_3_117,"00:07:19,140","00:07:21,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,It's the same.
cs-410_4_3_118,"00:07:21,640","00:07:24,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,Larger r with corresponds
cs-410_4_3_119,"00:07:26,400","00:07:32,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"But the difference would only show when,"
cs-410_4_3_120,"00:07:32,700","00:07:39,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"So again, think about the average of Mean"
cs-410_4_3_121,"00:07:39,290","00:07:39,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,What's the difference?
cs-410_4_3_122,"00:07:39,930","00:07:41,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Do you see any difference?
cs-410_4_3_123,"00:07:41,730","00:07:46,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"And would, would this difference"
cs-410_4_3_124,"00:07:46,050","00:07:46,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,In our conclusion.
cs-410_4_3_125,"00:07:49,050","00:07:53,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"And this, it turns out that,"
cs-410_4_3_126,"00:07:53,380","00:07:57,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"if you think about it, if you want to"
cs-410_4_3_127,"00:07:57,210","00:07:58,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,then pause the video.
cs-410_4_3_128,"00:07:59,410","00:08:04,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"Basically, the difference is,"
cs-410_4_3_129,"00:08:04,350","00:08:07,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,Again it will be dominated
cs-410_4_3_130,"00:08:07,810","00:08:08,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So what are those values?
cs-410_4_3_131,"00:08:08,840","00:08:15,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,Those are basically large values that
cs-410_4_3_132,"00:08:15,240","00:08:20,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,That means the relevant items
cs-410_4_3_133,"00:08:20,530","00:08:25,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,And the sum that's also the average
cs-410_4_3_134,"00:08:25,160","00:08:28,328",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,Where those relevant documents
cs-410_4_3_135,"00:08:28,328","00:08:30,774",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,in the lower portion of the ranked.
cs-410_4_3_136,"00:08:30,774","00:08:35,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,But from a users perspective we care
cs-410_4_3_137,"00:08:35,850","00:08:39,529",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,So by taking this transformation
cs-410_4_3_138,"00:08:40,650","00:08:43,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,Here we emphasize more on
cs-410_4_3_139,"00:08:43,920","00:08:48,121",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,"You know, think about"
cs-410_4_3_140,"00:08:48,121","00:08:52,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"it would make a big difference, in 1 over"
cs-410_4_3_141,"00:08:52,390","00:08:57,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,where and when won't make much
cs-410_4_3_142,"00:08:57,030","00:09:01,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,But if you use this there will
cs-410_4_3_143,"00:09:01,370","00:09:03,468",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"let's say 1,000, right."
cs-410_4_3_144,"00:09:03,468","00:09:05,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,So this is not the desirable.
cs-410_4_3_145,"00:09:06,260","00:09:09,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,"On the other hand, a 1 and"
cs-410_4_3_146,"00:09:09,320","00:09:13,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,So this is yet another case where there
cs-410_4_3_147,"00:09:13,150","00:09:15,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,thing and then you need to figure
cs-410_4_3_148,"00:09:17,470","00:09:22,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,"So to summarize,"
cs-410_4_3_149,"00:09:22,360","00:09:25,738",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,Can characterize the overall
cs-410_4_3_150,"00:09:25,738","00:09:30,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,And we emphasized that the actual
cs-410_4_3_151,"00:09:30,650","00:09:34,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,on how many top ranked results
cs-410_4_3_152,"00:09:34,570","00:09:37,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,Some users will examine more.
cs-410_4_3_153,"00:09:37,000","00:09:38,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,Than others.
cs-410_4_3_154,"00:09:38,390","00:09:42,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,An average person uses a standard measure
cs-410_4_3_155,"00:09:42,100","00:09:44,837",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,It combines precision and recall and
cs-410_4_3_156,"00:09:44,837","00:09:48,904",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,it's sensitive to the rank
cs-410_4_3_157,"00:09:48,904","00:09:58,904",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,[MUSIC]
cs-410_1_3_1,"00:00:00,000","00:00:03,655",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_1_3_2,"00:00:07,739","00:00:14,807",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about Evaluation of
cs-410_1_3_3,"00:00:14,807","00:00:19,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"lectures, we have talked about"
cs-410_1_3_4,"00:00:19,490","00:00:22,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,different kinds of ranking functions.
cs-410_1_3_5,"00:00:23,550","00:00:27,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,But how do we know which
cs-410_1_3_6,"00:00:27,040","00:00:28,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,"In order to answer this question,"
cs-410_1_3_7,"00:00:28,390","00:00:33,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,we have to compare them and that means we
cs-410_1_3_8,"00:00:34,790","00:00:37,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,So this is the main topic of this lecture.
cs-410_1_3_9,"00:00:40,462","00:00:42,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"First, lets think about why"
cs-410_1_3_10,"00:00:42,730","00:00:44,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,I already give one reason.
cs-410_1_3_11,"00:00:44,290","00:00:48,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"That is, we have to use evaluation"
cs-410_1_3_12,"00:00:48,770","00:00:50,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,works better.
cs-410_1_3_13,"00:00:50,330","00:00:54,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,Now this is very important for
cs-410_1_3_14,"00:00:54,310","00:01:00,173",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"Otherwise, we wouldn't know whether a new"
cs-410_1_3_15,"00:01:00,173","00:01:04,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"In the beginning of this course, we talked"
cs-410_1_3_16,"00:01:04,710","00:01:07,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,We compare it with data base retrieval.
cs-410_1_3_17,"00:01:08,440","00:01:14,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,There we mentioned that text retrieval
cs-410_1_3_18,"00:01:14,390","00:01:18,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,So evaluation must rely on users.
cs-410_1_3_19,"00:01:18,240","00:01:22,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"Which system works better,"
cs-410_1_3_20,"00:01:25,020","00:01:28,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"So, this becomes a very"
cs-410_1_3_21,"00:01:28,850","00:01:32,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,because how can we get users
cs-410_1_3_22,"00:01:32,510","00:01:35,281",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,How can we do a fair comparison
cs-410_1_3_23,"00:01:37,208","00:01:39,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,So just go back to the reasons for
cs-410_1_3_24,"00:01:41,210","00:01:42,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,I listed two reasons here.
cs-410_1_3_25,"00:01:42,660","00:01:47,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"The second reason, is basically what I"
cs-410_1_3_26,"00:01:47,060","00:01:51,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,reason which is to assess the actual
cs-410_1_3_27,"00:01:51,910","00:01:55,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,Imagine you're building your
cs-410_1_3_28,"00:01:55,200","00:02:01,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,it would be interesting knowing how well
cs-410_1_3_29,"00:02:01,660","00:02:02,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"So in this case,"
cs-410_1_3_30,"00:02:02,350","00:02:07,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,matches must reflect the utility to
cs-410_1_3_31,"00:02:07,850","00:02:11,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"And typically, this has to be"
cs-410_1_3_32,"00:02:11,840","00:02:13,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,using the real search engine.
cs-410_1_3_33,"00:02:16,340","00:02:18,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"In the second case, or the second reason,"
cs-410_1_3_34,"00:02:19,970","00:02:26,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,the measures actually all need to collated
cs-410_1_3_35,"00:02:26,130","00:02:30,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"Thus, they don't have to accurately"
cs-410_1_3_36,"00:02:31,980","00:02:37,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,So the measure only needs to be good
cs-410_1_3_37,"00:02:38,860","00:02:41,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,And this is usually done
cs-410_1_3_38,"00:02:41,780","00:02:48,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,And this is the main idea that we'll
cs-410_1_3_39,"00:02:48,110","00:02:53,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,This has been very important for
cs-410_1_3_40,"00:02:53,520","00:02:56,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,for improving search
cs-410_1_3_41,"00:02:58,910","00:03:01,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,So let's talk about what to measure.
cs-410_1_3_42,"00:03:01,880","00:03:06,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,There are many aspects of searching
cs-410_1_3_43,"00:03:06,750","00:03:09,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"And here,"
cs-410_1_3_44,"00:03:09,000","00:03:11,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"One, is effectiveness or accuracy."
cs-410_1_3_45,"00:03:11,190","00:03:13,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,How accurate are the search results?
cs-410_1_3_46,"00:03:13,710","00:03:18,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"In this case, we're measuring a system's"
cs-410_1_3_47,"00:03:18,110","00:03:20,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,on top of non relevant ones.
cs-410_1_3_48,"00:03:20,150","00:03:21,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"The second, is efficiency."
cs-410_1_3_49,"00:03:21,850","00:03:24,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,How quickly can you get the results?
cs-410_1_3_50,"00:03:24,470","00:03:27,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,How much computing resources
cs-410_1_3_51,"00:03:27,710","00:03:31,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"In this case, we need to measure the space"
cs-410_1_3_52,"00:03:32,540","00:03:34,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,The third aspect is usability.
cs-410_1_3_53,"00:03:34,890","00:03:38,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"Basically the question is,"
cs-410_1_3_54,"00:03:38,950","00:03:40,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Here, obviously, interfaces and"
cs-410_1_3_55,"00:03:40,840","00:03:45,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,many other things also important and
cs-410_1_3_56,"00:03:47,410","00:03:51,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"Now in this course, we're going to"
cs-410_1_3_57,"00:03:51,670","00:03:52,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,accuracy measures.
cs-410_1_3_58,"00:03:52,710","00:03:55,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,Because the efficiency and
cs-410_1_3_59,"00:03:55,340","00:04:00,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,usability dimensions are not
cs-410_1_3_60,"00:04:00,230","00:04:08,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,And so they are needed for
cs-410_1_3_61,"00:04:08,640","00:04:13,347",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,And there is also good coverage
cs-410_1_3_62,"00:04:13,347","00:04:18,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,But how to evaluate search
cs-410_1_3_63,"00:04:18,780","00:04:23,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,something unique to text retrieval and
cs-410_1_3_64,"00:04:23,110","00:04:28,428",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,The main idea that people have proposed
cs-410_1_3_65,"00:04:28,428","00:04:33,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,the text retrieval algorithm is called
cs-410_1_3_66,"00:04:33,850","00:04:40,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,This one actually was developed
cs-410_1_3_67,"00:04:40,145","00:04:44,785",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,It's a methodology for
cs-410_1_3_68,"00:04:45,985","00:04:49,305",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,Its sampling methodology that has
cs-410_1_3_69,"00:04:49,305","00:04:50,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,search engine evaluation.
cs-410_1_3_70,"00:04:50,880","00:04:55,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,But also for evaluating virtually
cs-410_1_3_71,"00:04:55,930","00:05:01,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,for example in natural language processing
cs-410_1_3_72,"00:05:01,180","00:05:05,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"is empirical to find, we typically"
cs-410_1_3_73,"00:05:05,620","00:05:09,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,And today with the big data challenging
cs-410_1_3_74,"00:05:09,450","00:05:13,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,with the use of machine
cs-410_1_3_75,"00:05:13,590","00:05:17,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"This methodology has been very popular,"
cs-410_1_3_76,"00:05:17,430","00:05:20,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,a search engine application in the 1960s.
cs-410_1_3_77,"00:05:20,100","00:05:25,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,So the basic idea of this approach is
cs-410_1_3_78,"00:05:25,250","00:05:26,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,define measures.
cs-410_1_3_79,"00:05:27,220","00:05:30,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Once such a test collection is built,"
cs-410_1_3_80,"00:05:30,350","00:05:32,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,again to test different algorithms.
cs-410_1_3_81,"00:05:32,990","00:05:36,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,And we're going to define measures
cs-410_1_3_82,"00:05:36,180","00:05:39,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,performance of a system and algorithm.
cs-410_1_3_83,"00:05:41,070","00:05:42,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,So how exactly will this work?
cs-410_1_3_84,"00:05:42,960","00:05:47,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Well we can do have a sample collection of
cs-410_1_3_85,"00:05:47,090","00:05:49,986",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,the real document collection
cs-410_1_3_86,"00:05:49,986","00:05:53,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,We're going to also have a sample
cs-410_1_3_87,"00:05:53,210","00:05:55,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,This is a little simulator
cs-410_1_3_88,"00:05:56,270","00:05:58,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"Then, we'll have to have"
cs-410_1_3_89,"00:05:58,980","00:06:03,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,These are judgments of which documents
cs-410_1_3_90,"00:06:03,930","00:06:08,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"Ideally, they have to be made by"
cs-410_1_3_91,"00:06:08,250","00:06:12,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,Because those are the people that know
cs-410_1_3_92,"00:06:12,930","00:06:14,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,"And finally, we have to have matches for"
cs-410_1_3_93,"00:06:14,690","00:06:19,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,quantify how well our system's result
cs-410_1_3_94,"00:06:19,830","00:06:24,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,That would be constructed base
cs-410_1_3_95,"00:06:24,560","00:06:30,917",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,So this methodology is very useful for
cs-410_1_3_96,"00:06:30,917","00:06:36,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,because the test can be reused many times.
cs-410_1_3_97,"00:06:36,130","00:06:41,340",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,And it will also provide a fair
cs-410_1_3_98,"00:06:41,340","00:06:43,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,We have the same criteria or
cs-410_1_3_99,"00:06:43,370","00:06:47,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,same dataset to be used to
cs-410_1_3_100,"00:06:47,570","00:06:50,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,This allows us to compare
cs-410_1_3_101,"00:06:50,810","00:06:55,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,an old algorithm that was divided many
cs-410_1_3_102,"00:06:55,660","00:06:59,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"So this is the illustration of this works,"
cs-410_1_3_103,"00:06:59,580","00:07:03,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,we need our queries that are showing here.
cs-410_1_3_104,"00:07:03,930","00:07:05,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"We have Q1, Q2 etc."
cs-410_1_3_105,"00:07:05,180","00:07:08,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,We also need the documents and
cs-410_1_3_106,"00:07:08,300","00:07:10,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,on the right side you will see
cs-410_1_3_107,"00:07:10,580","00:07:19,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,These are basically the binary judgments
cs-410_1_3_108,"00:07:19,150","00:07:23,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,"So for example,"
cs-410_1_3_109,"00:07:23,790","00:07:27,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,"D2 is judged as being relevant as well,"
cs-410_1_3_110,"00:07:27,920","00:07:28,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,And the Q1 etc.
cs-410_1_3_111,"00:07:28,980","00:07:32,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,These will be created by users.
cs-410_1_3_112,"00:07:34,190","00:07:38,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"Once we have these, and"
cs-410_1_3_113,"00:07:38,460","00:07:43,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"And then if you have two systems,"
cs-410_1_3_114,"00:07:43,560","00:07:47,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,then you can just run each
cs-410_1_3_115,"00:07:47,260","00:07:50,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,the documents and
cs-410_1_3_116,"00:07:50,580","00:07:56,347",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,Let's say if the queries Q1 and
cs-410_1_3_117,"00:07:56,347","00:08:02,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,Here I show R sub A as
cs-410_1_3_118,"00:08:02,350","00:08:05,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,"So this is, remember we talked about"
cs-410_1_3_119,"00:08:05,170","00:08:09,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,task of computing approximation
cs-410_1_3_120,"00:08:09,750","00:08:13,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,R sub A is system A's approximation here.
cs-410_1_3_121,"00:08:14,980","00:08:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,And R sub B is system B's
cs-410_1_3_122,"00:08:21,100","00:08:22,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,"Now, let's take a look at these results."
cs-410_1_3_123,"00:08:22,810","00:08:24,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So which is better?
cs-410_1_3_124,"00:08:24,190","00:08:26,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,"Now imagine if a user,"
cs-410_1_3_125,"00:08:26,810","00:08:31,145",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,Now let's take a look at the both results.
cs-410_1_3_126,"00:08:31,145","00:08:33,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,And there are some differences and
cs-410_1_3_127,"00:08:33,270","00:08:40,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,there are some documents that
cs-410_1_3_128,"00:08:40,160","00:08:44,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"But if you look at the results,"
cs-410_1_3_129,"00:08:44,200","00:08:48,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,A is better in the sense that we don't
cs-410_1_3_130,"00:08:48,640","00:08:52,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"And among the three documents returned,"
cs-410_1_3_131,"00:08:52,260","00:08:55,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"So that's good, it's precise."
cs-410_1_3_132,"00:08:55,430","00:08:58,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,On the other hand one council
cs-410_1_3_133,"00:08:58,770","00:09:01,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,because we've got all of
cs-410_1_3_134,"00:09:01,280","00:09:03,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,We've got three instead of two.
cs-410_1_3_135,"00:09:03,500","00:09:06,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,So which one is better and
cs-410_1_3_136,"00:09:08,820","00:09:12,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"Well, obviously this question"
cs-410_1_3_137,"00:09:12,670","00:09:14,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,It depends on users as well.
cs-410_1_3_138,"00:09:14,820","00:09:19,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,You might even imagine for
cs-410_1_3_139,"00:09:19,950","00:09:23,747",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,If the user is not interested in
cs-410_1_3_140,"00:09:23,747","00:09:28,582",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"Right, in this case the user doesn't"
cs-410_1_3_141,"00:09:28,582","00:09:31,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,see most of the relevant documents.
cs-410_1_3_142,"00:09:31,020","00:09:34,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"On the other hand,"
cs-410_1_3_143,"00:09:34,230","00:09:37,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,to have as many random
cs-410_1_3_144,"00:09:37,130","00:09:41,617",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"For example, if you're doing a literature"
cs-410_1_3_145,"00:09:41,617","00:09:43,844",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,and you might find that
cs-410_1_3_146,"00:09:43,844","00:09:48,985",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"So in the case, we will have to also"
cs-410_1_3_147,"00:09:48,985","00:09:53,408",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,And we might need it to define multiple
cs-410_1_3_148,"00:09:53,408","00:09:55,798",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,perspectives of looking at the results.
cs-410_1_3_149,"00:09:58,259","00:10:08,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,[MUSIC]
cs-410_5_3_1,"00:00:00,883","00:00:05,127",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_5_3_2,"00:00:07,433","00:00:12,376",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about how to evaluate
cs-410_5_3_3,"00:00:12,376","00:00:15,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,multiple levels of judgements.
cs-410_5_3_4,"00:00:15,560","00:00:19,994",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"In this lecture, we will continue"
cs-410_5_3_5,"00:00:19,994","00:00:23,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We're going to look at how to
cs-410_5_3_6,"00:00:23,410","00:00:26,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,when we have multiple
cs-410_5_3_7,"00:00:27,760","00:00:31,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,So far we have talked about
cs-410_5_3_8,"00:00:31,180","00:00:34,169",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,that means a document is judged as
cs-410_5_3_9,"00:00:35,270","00:00:40,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,"But earlier, we also talk about"
cs-410_5_3_10,"00:00:40,310","00:00:45,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,So we often can distinguish
cs-410_5_3_11,"00:00:45,580","00:00:50,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,"those are very useful documents,"
cs-410_5_3_12,"00:00:50,230","00:00:53,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,"They are okay, they are useful perhaps."
cs-410_5_3_13,"00:00:53,000","00:00:56,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"And further from now, we're adding"
cs-410_5_3_14,"00:00:57,450","00:01:01,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,So imagine you can have ratings for
cs-410_5_3_15,"00:01:01,490","00:01:05,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Then, you would have"
cs-410_5_3_16,"00:01:05,390","00:01:10,803",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"For example, here I show example of three"
cs-410_5_3_17,"00:01:10,803","00:01:15,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"very relevant, 2 for marginally relevant,"
cs-410_5_3_18,"00:01:15,780","00:01:18,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Now, how do we evaluate the search"
cs-410_5_3_19,"00:01:18,990","00:01:23,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"Obvious that the map doesn't work, average"
cs-410_5_3_20,"00:01:23,330","00:01:28,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"recall doesn't work,"
cs-410_5_3_21,"00:01:28,190","00:01:33,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,So let's look at some top ranked
cs-410_5_3_22,"00:01:33,510","00:01:38,518",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,Imagine the user would be mostly
cs-410_5_3_23,"00:01:43,122","00:01:48,165",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"And we marked the rating levels,"
cs-410_5_3_24,"00:01:48,165","00:01:54,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"for these documents as shown here,"
cs-410_5_3_25,"00:01:54,620","00:01:57,122",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,And we call these gain.
cs-410_5_3_26,"00:01:57,122","00:02:02,345",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,And the reason why we call it
cs-410_5_3_27,"00:02:02,345","00:02:08,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,that we are infusing is called the NDCG
cs-410_5_3_28,"00:02:10,090","00:02:14,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"So this gain, basically,"
cs-410_5_3_29,"00:02:14,900","00:02:19,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,information a user can obtain by
cs-410_5_3_30,"00:02:19,790","00:02:24,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"So looking at the first document,"
cs-410_5_3_31,"00:02:24,120","00:02:28,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,Looking at the non-relevant document
cs-410_5_3_32,"00:02:29,510","00:02:32,703",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,Looking at the moderator or
cs-410_5_3_33,"00:02:32,703","00:02:35,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"document the user would get 2 points,"
cs-410_5_3_34,"00:02:35,910","00:02:40,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"So, this gain to each of the measures is"
cs-410_5_3_35,"00:02:40,560","00:02:41,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,perspective.
cs-410_5_3_36,"00:02:41,890","00:02:46,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"Of course, if we assume the user"
cs-410_5_3_37,"00:02:46,140","00:02:51,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"we're looking at the cutoff at 10,"
cs-410_5_3_38,"00:02:51,060","00:02:51,774",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,And what's that?
cs-410_5_3_39,"00:02:51,774","00:02:55,825",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"Well, that's simply the sum of these,"
cs-410_5_3_40,"00:02:55,825","00:02:59,275",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"So if the user stops after the position 1,"
cs-410_5_3_41,"00:02:59,275","00:03:03,163",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"If the user looks at another document,"
cs-410_5_3_42,"00:03:03,163","00:03:08,221",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"If the user looks at the more documents,"
cs-410_5_3_43,"00:03:08,221","00:03:13,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,Of course this is at the cost of
cs-410_5_3_44,"00:03:13,200","00:03:16,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,So cumulative gain gives
cs-410_5_3_45,"00:03:16,390","00:03:21,368",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,much total gain the user would have if
cs-410_5_3_46,"00:03:21,368","00:03:28,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,"Now, in NDCG, we also have another letter"
cs-410_5_3_47,"00:03:29,170","00:03:32,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"So, why do we want to do discounting?"
cs-410_5_3_48,"00:03:32,060","00:03:35,685",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,"Well, if you look at this cumulative gain,"
cs-410_5_3_49,"00:03:35,685","00:03:41,975",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,which is it did not consider the rank
cs-410_5_3_50,"00:03:41,975","00:03:46,115",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"So for example, looking at this sum here,"
cs-410_5_3_51,"00:03:46,115","00:03:51,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,and we only know there is 1
cs-410_5_3_52,"00:03:51,485","00:03:54,945",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"1 marginally relevant document,"
cs-410_5_3_53,"00:03:54,945","00:03:57,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,We don't really care
cs-410_5_3_54,"00:03:57,300","00:04:02,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,"Ideally, we want these two to be ranked"
cs-410_5_3_55,"00:04:03,120","00:04:06,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,But how can we capture that intuition?
cs-410_5_3_56,"00:04:06,420","00:04:13,209",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"Well we have to say, well this is 3 here"
cs-410_5_3_57,"00:04:13,209","00:04:18,114",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,And that means the contribution
cs-410_5_3_58,"00:04:18,114","00:04:22,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,positions has to be
cs-410_5_3_59,"00:04:22,750","00:04:24,666",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"And this is the idea of discounting,"
cs-410_5_3_60,"00:04:24,666","00:04:29,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"So we're going to to say, well, the first"
cs-410_5_3_61,"00:04:29,530","00:04:33,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,because the user can be assumed
cs-410_5_3_62,"00:04:33,910","00:04:38,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"But the second one,"
cs-410_5_3_63,"00:04:38,030","00:04:42,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,because there's a small possibility
cs-410_5_3_64,"00:04:42,370","00:04:48,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,So we divide this gain by
cs-410_5_3_65,"00:04:48,690","00:04:52,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"So log of 2,"
cs-410_5_3_66,"00:04:52,640","00:04:57,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"And when we go to the third position,"
cs-410_5_3_67,"00:04:57,080","00:05:01,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"because the normalizer is log of 3,"
cs-410_5_3_68,"00:05:01,270","00:05:06,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,So when we take such a sum that a lower
cs-410_5_3_69,"00:05:06,690","00:05:10,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,that much as a highly ranked document.
cs-410_5_3_70,"00:05:10,000","00:05:15,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"So that means if you, for example,"
cs-410_5_3_71,"00:05:15,120","00:05:20,726",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"this position, and this one, and then"
cs-410_5_3_72,"00:05:20,726","00:05:27,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,for example very relevant
cs-410_5_3_73,"00:05:27,050","00:05:31,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Imagine if you put the 3 here,"
cs-410_5_3_74,"00:05:31,290","00:05:34,635",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,So it's not as good as if
cs-410_5_3_75,"00:05:34,635","00:05:36,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So this is the idea of discounting.
cs-410_5_3_76,"00:05:37,900","00:05:43,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Okay, so now at this point that we have"
cs-410_5_3_77,"00:05:43,210","00:05:50,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,measuring the utility of this ranked
cs-410_5_3_78,"00:05:51,480","00:05:53,125",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,So are we happy with this?
cs-410_5_3_79,"00:05:53,125","00:05:55,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"Well, we can use this to rank systems."
cs-410_5_3_80,"00:05:55,680","00:05:58,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,"Now, we still need to do a little bit more"
cs-410_5_3_81,"00:05:58,510","00:06:03,272",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,in order to make this measure
cs-410_5_3_82,"00:06:03,272","00:06:10,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"And this is the last step, and by the way,"
cs-410_5_3_83,"00:06:10,580","00:06:16,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"so this is the total sum of DCG,"
cs-410_5_3_84,"00:06:16,820","00:06:20,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"So the last step is called N,"
cs-410_5_3_85,"00:06:20,880","00:06:25,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,"And if we do that,"
cs-410_5_3_86,"00:06:25,240","00:06:26,463",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,So how do we do that?
cs-410_5_3_87,"00:06:26,463","00:06:31,241",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Well, the idea here is we're"
cs-410_5_3_88,"00:06:31,241","00:06:35,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,the ideal DCG at the same cutoff.
cs-410_5_3_89,"00:06:35,280","00:06:37,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,What is the ideal DCG?
cs-410_5_3_90,"00:06:37,130","00:06:40,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"Well, this is the DCG of an ideal ranking."
cs-410_5_3_91,"00:06:40,830","00:06:47,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,So imagine if we have 9 documents in
cs-410_5_3_92,"00:06:47,690","00:06:52,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,And that means in total we
cs-410_5_3_93,"00:06:53,840","00:07:00,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,Then our ideal rank lister would have put
cs-410_5_3_94,"00:07:00,640","00:07:05,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,So all these would have to be 3 and
cs-410_5_3_95,"00:07:05,730","00:07:10,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,Because that's the best we could
cs-410_5_3_96,"00:07:10,040","00:07:11,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,But all these positions would be 3.
cs-410_5_3_97,"00:07:11,800","00:07:13,938",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,Right?
cs-410_5_3_98,"00:07:13,938","00:07:16,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,So this would our ideal ranked list.
cs-410_5_3_99,"00:07:18,070","00:07:21,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,And then we had computed the DCG for
cs-410_5_3_100,"00:07:23,250","00:07:27,062",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,So this would be given by this
cs-410_5_3_101,"00:07:27,062","00:07:35,723",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,And so this ideal DCG would then
cs-410_5_3_102,"00:07:35,723","00:07:36,845",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So here.
cs-410_5_3_103,"00:07:36,845","00:07:40,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,And this idea of DCG would
cs-410_5_3_104,"00:07:40,040","00:07:43,726",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,"So you can imagine now,"
cs-410_5_3_105,"00:07:43,726","00:07:49,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,compare the actual DCG with the best DCG
cs-410_5_3_106,"00:07:49,590","00:07:51,146",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,Now why do we want to do this?
cs-410_5_3_107,"00:07:51,146","00:07:56,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"Well, by doing this we'll map the DCG"
cs-410_5_3_108,"00:07:57,900","00:08:01,650",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"So the best value, or the highest value,"
cs-410_5_3_109,"00:08:01,650","00:08:07,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"That's when your rank list is,"
cs-410_5_3_110,"00:08:07,500","00:08:12,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"otherwise, in general,"
cs-410_5_3_111,"00:08:13,405","00:08:14,954",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"Now, what if we don't do that?"
cs-410_5_3_112,"00:08:14,954","00:08:19,737",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,"Well, you can see, this transformation,"
cs-410_5_3_113,"00:08:19,737","00:08:24,108",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,doesn't really affect the relative
cs-410_5_3_114,"00:08:24,108","00:08:29,053",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,"just one topic, because this ideal"
cs-410_5_3_115,"00:08:29,053","00:08:33,834",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,so the ranking of systems based on
cs-410_5_3_116,"00:08:33,834","00:08:36,986",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,if you rank them based
cs-410_5_3_117,"00:08:36,986","00:08:40,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,The difference however is
cs-410_5_3_118,"00:08:40,760","00:08:42,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"Because if we don't do normalization,"
cs-410_5_3_119,"00:08:42,894","00:08:45,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,different topics will have
cs-410_5_3_120,"00:08:46,740","00:08:51,951",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"For a topic like this one,"
cs-410_5_3_121,"00:08:51,951","00:08:56,593",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,"the DCG can get really high,"
cs-410_5_3_122,"00:08:56,593","00:09:02,794",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,there are only two very relevant documents
cs-410_5_3_123,"00:09:02,794","00:09:06,124",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,Then the highest DCG that
cs-410_5_3_124,"00:09:06,124","00:09:09,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,such a topic would not be very high.
cs-410_5_3_125,"00:09:09,210","00:09:15,555",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"So again, we face the problem of"
cs-410_5_3_126,"00:09:15,555","00:09:17,028",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"When we take an average,"
cs-410_5_3_127,"00:09:17,028","00:09:20,826",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,we don't want the average to be
cs-410_5_3_128,"00:09:20,826","00:09:23,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"Those are, again, easy queries."
cs-410_5_3_129,"00:09:23,360","00:09:27,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"So, by doing the normalization,"
cs-410_5_3_130,"00:09:27,220","00:09:31,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,making all the queries contribute
cs-410_5_3_131,"00:09:31,690","00:09:34,882",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"So, this is a idea of NDCG, it's used for"
cs-410_5_3_132,"00:09:34,882","00:09:40,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,measuring a rank list based on multiple
cs-410_5_3_133,"00:09:42,830","00:09:47,951",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,In a more general way this
cs-410_5_3_134,"00:09:47,951","00:09:55,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,that can be applied to any ranked task
cs-410_5_3_135,"00:09:55,900","00:10:01,111",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,And the scale of the judgements
cs-410_5_3_136,"00:10:01,111","00:10:07,094",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,binary not only more than binary they
cs-410_5_3_137,"00:10:07,094","00:10:11,365",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"0, 5 or"
cs-410_5_3_138,"00:10:11,365","00:10:15,631",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,"And the main idea of this measure,"
cs-410_5_3_139,"00:10:15,631","00:10:19,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,is to measure the total utility
cs-410_5_3_140,"00:10:19,920","00:10:24,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,So you always choose a cutoff and
cs-410_5_3_141,"00:10:24,120","00:10:28,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,And it would discount the contribution
cs-410_5_3_142,"00:10:28,700","00:10:31,811",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"And then finally,"
cs-410_5_3_143,"00:10:31,811","00:10:37,645",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,it would do normalization to ensure
cs-410_5_3_144,"00:10:37,645","00:10:43,093",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,comparability across queries.
cs-410_5_3_145,"00:10:43,093","00:10:48,319",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,[MUSIC]
cs-410_6_3_1,"00:00:00,004","00:00:06,485",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_6_3_2,"00:00:06,485","00:00:10,694",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about some practical
cs-410_6_3_3,"00:00:10,694","00:00:12,939",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,evaluation of text retrieval systems.
cs-410_6_3_4,"00:00:14,440","00:00:17,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we will continue"
cs-410_6_3_5,"00:00:17,730","00:00:21,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,We'll cover some practical
cs-410_6_3_6,"00:00:21,250","00:00:24,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,in actual evaluation of
cs-410_6_3_7,"00:00:25,500","00:00:29,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"So, in order to create"
cs-410_6_3_8,"00:00:29,060","00:00:31,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,we have to create a set of queries.
cs-410_6_3_9,"00:00:31,540","00:00:34,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,A set of documents and
cs-410_6_3_10,"00:00:35,750","00:00:39,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,It turns out that each is
cs-410_6_3_11,"00:00:39,680","00:00:43,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"First, the documents and"
cs-410_6_3_12,"00:00:43,240","00:00:47,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,They must represent the real queries and
cs-410_6_3_13,"00:00:48,290","00:00:50,990",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And we also have to use many queries and
cs-410_6_3_14,"00:00:50,990","00:00:55,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,many documents in order to
cs-410_6_3_15,"00:00:56,470","00:01:02,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,For the matching of relevant
cs-410_6_3_16,"00:01:02,560","00:01:10,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,We also need to ensure that there exists a
cs-410_6_3_17,"00:01:10,050","00:01:13,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"If a query has only one, that's"
cs-410_6_3_18,"00:01:13,900","00:01:18,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,It's not very informative to
cs-410_6_3_19,"00:01:18,300","00:01:23,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,using such a query because there's not
cs-410_6_3_20,"00:01:23,120","00:01:27,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"So ideally, there should be more"
cs-410_6_3_21,"00:01:27,390","00:01:30,469",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,the queries also should represent
cs-410_6_3_22,"00:01:31,470","00:01:35,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"In terms of relevance judgments,"
cs-410_6_3_23,"00:01:35,240","00:01:38,970",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,complete judgments of all
cs-410_6_3_24,"00:01:38,970","00:01:40,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"Yet, minimizing human and"
cs-410_6_3_25,"00:01:40,670","00:01:44,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"fault, because we have to use human"
cs-410_6_3_26,"00:01:44,980","00:01:47,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,It's very labor intensive.
cs-410_6_3_27,"00:01:47,690","00:01:52,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And as a result, it's impossible to"
cs-410_6_3_28,"00:01:52,550","00:01:57,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,"all the queries, especially considering"
cs-410_6_3_29,"00:01:58,750","00:02:03,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,"So this is actually a major challenge,"
cs-410_6_3_30,"00:02:03,590","00:02:07,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"For measures, it's also challenging,"
cs-410_6_3_31,"00:02:07,160","00:02:11,680",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,accurately reflect
cs-410_6_3_32,"00:02:11,680","00:02:15,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,We have to consider carefully
cs-410_6_3_33,"00:02:15,430","00:02:18,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,And then design measures to measure that.
cs-410_6_3_34,"00:02:18,530","00:02:21,482",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,If your measure is not
cs-410_6_3_35,"00:02:21,482","00:02:23,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,then your conclusion would be misled.
cs-410_6_3_36,"00:02:23,820","00:02:25,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,So it's very important.
cs-410_6_3_37,"00:02:26,880","00:02:29,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,So we're going to talk about
cs-410_6_3_38,"00:02:29,290","00:02:31,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,One is the statistical significance test.
cs-410_6_3_39,"00:02:31,360","00:02:36,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,And this also is a reason why
cs-410_6_3_40,"00:02:36,350","00:02:41,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,And the question here is how sure can
cs-410_6_3_41,"00:02:41,060","00:02:44,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,doesn't simply result from
cs-410_6_3_42,"00:02:44,800","00:02:49,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,So here are some sample results of
cs-410_6_3_43,"00:02:49,770","00:02:53,320",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,System B into different experiments.
cs-410_6_3_44,"00:02:53,320","00:02:57,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"And you can see in the bottom,"
cs-410_6_3_45,"00:02:57,540","00:03:02,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"So the mean, if you look at the mean"
cs-410_6_3_46,"00:03:02,668","00:03:08,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,of positions are exactly the same
cs-410_6_3_47,"00:03:08,300","00:03:13,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"So you can see this is 0.20,"
cs-410_6_3_48,"00:03:13,250","00:03:18,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,And again here it's also 0.20 and
cs-410_6_3_49,"00:03:18,520","00:03:23,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"Yet, if you look at these exact average"
cs-410_6_3_50,"00:03:23,440","00:03:29,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"If you look at these numbers in detail,"
cs-410_6_3_51,"00:03:29,810","00:03:35,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,you would feel that you can trust
cs-410_6_3_52,"00:03:36,100","00:03:41,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"In the another case, in the other case,"
cs-410_6_3_53,"00:03:41,610","00:03:48,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"So, why don't you take a look at all these"
cs-410_6_3_54,"00:03:48,470","00:03:52,565",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"So, if you look at the average,"
cs-410_6_3_55,"00:03:52,565","00:03:56,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"we can easily, say that well,"
cs-410_6_3_56,"00:03:56,660","00:03:59,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"So, after all it's 0.40 and"
cs-410_6_3_57,"00:03:59,630","00:04:05,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"this is twice as much as 0.20,"
cs-410_6_3_58,"00:04:05,950","00:04:10,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"But if you look at these two experiments,"
cs-410_6_3_59,"00:04:11,150","00:04:16,170",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"You will see that, we've been more"
cs-410_6_3_60,"00:04:16,170","00:04:17,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,in experiment one.
cs-410_6_3_61,"00:04:17,040","00:04:19,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,In this case.
cs-410_6_3_62,"00:04:19,160","00:04:23,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,Because these numbers seem to be
cs-410_6_3_63,"00:04:25,110","00:04:32,342",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,"Whereas in Experiment 2, we're not sure"
cs-410_6_3_64,"00:04:32,342","00:04:38,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,after System A is better and
cs-410_6_3_65,"00:04:39,335","00:04:43,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"But yet if we look at only average,"
cs-410_6_3_66,"00:04:45,750","00:04:47,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"So, what do you think?"
cs-410_6_3_67,"00:04:49,170","00:04:54,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"How reliable is our conclusion,"
cs-410_6_3_68,"00:04:55,940","00:04:59,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"Now in this case, intuitively,"
cs-410_6_3_69,"00:05:01,020","00:05:04,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,But how can we quantitate
cs-410_6_3_70,"00:05:04,630","00:05:08,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,And this is why we need to do
cs-410_6_3_71,"00:05:09,440","00:05:13,910",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"So, the idea of the statistical"
cs-410_6_3_72,"00:05:13,910","00:05:18,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,assess the variants across
cs-410_6_3_73,"00:05:18,330","00:05:21,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"If there is a big variance,"
cs-410_6_3_74,"00:05:21,160","00:05:25,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,that means the results could fluctuate
cs-410_6_3_75,"00:05:25,880","00:05:30,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"Then we should believe that,"
cs-410_6_3_76,"00:05:30,740","00:05:35,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,the results might change if we
cs-410_6_3_77,"00:05:35,210","00:05:39,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"Right, so this is then not so"
cs-410_6_3_78,"00:05:39,350","00:05:42,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,if you have c high variance
cs-410_6_3_79,"00:05:43,660","00:05:49,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,So let's look at these results
cs-410_6_3_80,"00:05:49,390","00:05:54,200",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,"So, here we show two different"
cs-410_6_3_81,"00:05:54,200","00:05:57,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,One is a sign test where
cs-410_6_3_82,"00:05:57,470","00:06:01,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"If System B is better than System A,"
cs-410_6_3_83,"00:06:01,260","00:06:05,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,When System A is better we
cs-410_6_3_84,"00:06:05,400","00:06:09,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,"Using this case, if you see this,"
cs-410_6_3_85,"00:06:09,600","00:06:12,980",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,We actually have four cases
cs-410_6_3_86,"00:06:12,980","00:06:16,671",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,"But three cases of System A is better,"
cs-410_6_3_87,"00:06:16,671","00:06:19,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"this is almost like a random results,"
cs-410_6_3_88,"00:06:19,880","00:06:25,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,So if you just take a random
cs-410_6_3_89,"00:06:25,880","00:06:30,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,if you use plus to denote the head and
cs-410_6_3_90,"00:06:30,090","00:06:34,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,that could easily be the results of just
cs-410_6_3_91,"00:06:34,920","00:06:39,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"So, the fact that the average is"
cs-410_6_3_92,"00:06:39,700","00:06:41,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,We can't reliably conclude that.
cs-410_6_3_93,"00:06:41,330","00:06:45,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,And this can be quantitatively
cs-410_6_3_94,"00:06:45,890","00:06:48,380",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,And that basically means
cs-410_6_3_95,"00:06:49,660","00:06:54,480",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,the probability that this result is
cs-410_6_3_96,"00:06:54,480","00:06:56,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,"In this case, probability is 1.0."
cs-410_6_3_97,"00:06:56,140","00:07:00,050",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,It means it surely is
cs-410_6_3_98,"00:07:01,310","00:07:06,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"Now in Willcoxan test,"
cs-410_6_3_99,"00:07:06,470","00:07:09,430",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,and we would be not only
cs-410_6_3_100,"00:07:09,430","00:07:12,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,we'll be also looking at
cs-410_6_3_101,"00:07:12,520","00:07:14,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"But we can draw a similar conclusion,"
cs-410_6_3_102,"00:07:14,690","00:07:18,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,where you say it's very
cs-410_6_3_103,"00:07:18,630","00:07:22,395",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"To illustrate this, let's think"
cs-410_6_3_104,"00:07:22,395","00:07:23,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,And this is called a now distribution.
cs-410_6_3_105,"00:07:23,895","00:07:26,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,We assume that the mean is zero here.
cs-410_6_3_106,"00:07:26,085","00:07:28,705",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,Lets say we started with
cs-410_6_3_107,"00:07:28,705","00:07:31,405",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,no difference between the two systems.
cs-410_6_3_108,"00:07:31,405","00:07:35,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,But we assume that because of random
cs-410_6_3_109,"00:07:35,230","00:07:37,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,we might observe a difference.
cs-410_6_3_110,"00:07:37,190","00:07:41,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,So the actual difference might
cs-410_6_3_111,"00:07:41,300","00:07:42,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"on the right side here, right?"
cs-410_6_3_112,"00:07:43,920","00:07:48,102",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"So, and this curve kind of shows"
cs-410_6_3_113,"00:07:48,102","00:07:52,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,actually observe values that
cs-410_6_3_114,"00:07:53,770","00:07:59,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"Now, so if we look at this picture then,"
cs-410_6_3_115,"00:08:01,070","00:08:05,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"if a difference is observed here, then"
cs-410_6_3_116,"00:08:05,530","00:08:11,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,the chance is very high that this is
cs-410_6_3_117,"00:08:11,180","00:08:16,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,We can define a region of
cs-410_6_3_118,"00:08:16,150","00:08:21,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,random fluctuation and
cs-410_6_3_119,"00:08:21,890","00:08:27,894",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,And in this then the observed may
cs-410_6_3_120,"00:08:28,960","00:08:34,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,But if you observe a value in this
cs-410_6_3_121,"00:08:34,830","00:08:39,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,then the difference is unlikely
cs-410_6_3_122,"00:08:39,880","00:08:44,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"All right, so there's a very small"
cs-410_6_3_123,"00:08:44,460","00:08:47,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,such a difference just because
cs-410_6_3_124,"00:08:48,400","00:08:52,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"So in that case, we can then conclude"
cs-410_6_3_125,"00:08:52,800","00:08:54,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,So System B is indeed better.
cs-410_6_3_126,"00:08:56,120","00:08:59,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,So this is the idea of
cs-410_6_3_127,"00:08:59,550","00:09:03,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,The takeaway message here is that you
cs-410_6_3_128,"00:09:03,870","00:09:05,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,jumping into a conclusion.
cs-410_6_3_129,"00:09:05,770","00:09:08,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"As in this case,"
cs-410_6_3_130,"00:09:09,790","00:09:13,259",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,There are many different ways of doing
cs-410_6_3_131,"00:09:15,260","00:09:20,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"So now, let's talk about the other"
cs-410_6_3_132,"00:09:20,270","00:09:24,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"as we said earlier,"
cs-410_6_3_133,"00:09:24,590","00:09:27,700",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,completely unless it's
cs-410_6_3_134,"00:09:27,700","00:09:31,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"So the question is,"
cs-410_6_3_135,"00:09:31,530","00:09:33,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"in the collection,"
cs-410_6_3_136,"00:09:35,000","00:09:38,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,And the solution here is Pooling.
cs-410_6_3_137,"00:09:38,230","00:09:45,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,And this is a strategy that has been used
cs-410_6_3_138,"00:09:46,710","00:09:49,800",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,So the idea of Pooling is the following.
cs-410_6_3_139,"00:09:49,800","00:09:54,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,We would first choose a diverse
cs-410_6_3_140,"00:09:54,410","00:09:56,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,These are Text Retrieval systems.
cs-410_6_3_141,"00:09:57,130","00:10:02,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,And we hope these methods can help us
cs-410_6_3_142,"00:10:02,830","00:10:05,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,So the goal is to pick out
cs-410_6_3_143,"00:10:05,400","00:10:08,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,We want to make judgements on relevant
cs-410_6_3_144,"00:10:08,770","00:10:12,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,useful documents from users perspectives.
cs-410_6_3_145,"00:10:12,720","00:10:16,339",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,So then we're going to have
cs-410_6_3_146,"00:10:17,380","00:10:19,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,The K can vary from systems.
cs-410_6_3_147,"00:10:19,720","00:10:24,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,But the point is to ask them to suggest
cs-410_6_3_148,"00:10:25,530","00:10:29,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,And then we simply combine
cs-410_6_3_149,"00:10:29,780","00:10:34,478",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,to form a pool of documents for
cs-410_6_3_150,"00:10:34,478","00:10:41,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"To judge, so imagine you have many"
cs-410_6_3_151,"00:10:41,370","00:10:44,498",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,"We take the top-K documents,"
cs-410_6_3_152,"00:10:44,498","00:10:48,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"Now, of course, there are many"
cs-410_6_3_153,"00:10:48,060","00:10:51,860",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,many systems might have retrieved
cs-410_6_3_154,"00:10:51,860","00:10:55,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,So there will be some duplicate documents.
cs-410_6_3_155,"00:10:56,480","00:11:00,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,And there are also unique documents
cs-410_6_3_156,"00:11:00,690","00:11:03,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,So the idea of having diverse
cs-410_6_3_157,"00:11:03,490","00:11:07,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,set of ranking methods is to
cs-410_6_3_158,"00:11:07,470","00:11:11,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,And can include as many possible
cs-410_6_3_159,"00:11:12,360","00:11:17,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"And then, the users would,"
cs-410_6_3_160,"00:11:17,180","00:11:21,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,"the judgments on this data set, this pool."
cs-410_6_3_161,"00:11:21,250","00:11:26,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,And the other unjudged the documents are
cs-410_6_3_162,"00:11:26,710","00:11:30,900",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,"Now if the pool is large enough,"
cs-410_6_3_163,"00:11:32,080","00:11:38,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,"But if the pool is not very large,"
cs-410_6_3_164,"00:11:38,600","00:11:41,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,And we might use other
cs-410_6_3_165,"00:11:41,190","00:11:46,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,there are indeed other
cs-410_6_3_166,"00:11:46,100","00:11:49,840",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,And such a strategy is generally okay for
cs-410_6_3_167,"00:11:49,840","00:11:54,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,comparing systems that
cs-410_6_3_168,"00:11:54,740","00:11:57,740",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,That means if you participate
cs-410_6_3_169,"00:11:57,740","00:12:00,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,then it's unlikely that it
cs-410_6_3_170,"00:12:00,850","00:12:03,100",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,because the problematic
cs-410_6_3_171,"00:12:04,300","00:12:07,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"However, this is problematic for"
cs-410_6_3_172,"00:12:07,060","00:12:11,880",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,evaluating a new system that may
cs-410_6_3_173,"00:12:11,880","00:12:16,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"In this case, a new system might"
cs-410_6_3_174,"00:12:16,010","00:12:20,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,nominated some read only documents
cs-410_6_3_175,"00:12:20,850","00:12:24,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,So those documents might be
cs-410_6_3_176,"00:12:24,370","00:12:26,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,That's unfair.
cs-410_6_3_177,"00:12:26,150","00:12:32,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,So to summarize the whole part of textual
cs-410_6_3_178,"00:12:32,810","00:12:37,150",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,Because the problem is the empirically
cs-410_6_3_179,"00:12:38,450","00:12:42,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,"don't rely on users, there's no way to"
cs-410_6_3_180,"00:12:43,580","00:12:46,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,If we have in the property
cs-410_6_3_181,"00:12:46,600","00:12:49,710",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,we might misguide our research or
cs-410_6_3_182,"00:12:49,710","00:12:52,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,And we might just draw wrong conclusions.
cs-410_6_3_183,"00:12:52,470","00:12:55,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,And we have seen this is
cs-410_6_3_184,"00:12:55,250","00:12:58,190",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,So make sure to get it right for
cs-410_6_3_185,"00:13:00,150","00:13:03,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,The main methodology is the Cranfield
cs-410_6_3_186,"00:13:03,400","00:13:08,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,And they are the main paradigm used in
cs-410_6_3_187,"00:13:08,230","00:13:10,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,not just a search engine variation.
cs-410_6_3_188,"00:13:10,820","00:13:16,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,Map and nDCG are the two main
cs-410_6_3_189,"00:13:16,020","00:13:19,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=796,know about and they are appropriate for
cs-410_6_3_190,"00:13:19,530","00:13:22,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,You will see them often
cs-410_6_3_191,"00:13:22,950","00:13:27,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,Precision at 10 documents is easier
cs-410_6_3_192,"00:13:27,080","00:13:28,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,So that's also often useful.
cs-410_6_3_193,"00:13:30,580","00:13:37,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,What's not covered is some other
cs-410_6_3_194,"00:13:37,610","00:13:43,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"Where the system would mix two,"
cs-410_6_3_195,"00:13:43,720","00:13:46,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,And then would show
cs-410_6_3_196,"00:13:46,580","00:13:49,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,"Of course, the users don't see"
cs-410_6_3_197,"00:13:49,780","00:13:52,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,The users would judge those results or
cs-410_6_3_198,"00:13:52,410","00:13:58,096",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,click on those documents in
cs-410_6_3_199,"00:13:58,096","00:14:02,080",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=838,"In this case then, the search engine"
cs-410_6_3_200,"00:14:02,080","00:14:07,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,see if one method has contributed
cs-410_6_3_201,"00:14:07,250","00:14:11,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,"If the user tends to click on one,"
cs-410_6_3_202,"00:14:13,050","00:14:17,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,then it suggests that
cs-410_6_3_203,"00:14:17,730","00:14:21,008",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,So this is what leverages the real users
cs-410_6_3_204,"00:14:21,008","00:14:25,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,It's called A-B Test and
cs-410_6_3_205,"00:14:25,640","00:14:29,370",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,the modern search engines or
cs-410_6_3_206,"00:14:29,370","00:14:32,590",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,Another way to evaluate IR or
cs-410_6_3_207,"00:14:32,590","00:14:36,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,textual retrieval is user studies and
cs-410_6_3_208,"00:14:36,020","00:14:39,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=876,I've put some references here
cs-410_6_3_209,"00:14:39,390","00:14:40,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=879,to know more about that.
cs-410_6_3_210,"00:14:41,760","00:14:44,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,"So, there are three"
cs-410_6_3_211,"00:14:44,180","00:14:49,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,These are three mini books about
cs-410_6_3_212,"00:14:49,280","00:14:54,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,in covering a broad review of
cs-410_6_3_213,"00:14:54,280","00:14:58,237",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,And it covers some of the things
cs-410_6_3_214,"00:14:58,237","00:15:01,085",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,they also have a lot of others to offer.
cs-410_6_3_215,"00:15:02,777","00:15:12,777",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,[MUSIC]
cs-410_2_3_1,"00:00:00,012","00:00:06,908",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_3_2,"00:00:06,908","00:00:13,498",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,lecture is about the basic measures for
cs-410_2_3_3,"00:00:13,498","00:00:18,955",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture,"
cs-410_2_3_4,"00:00:18,955","00:00:24,528",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,measures to quantitatively
cs-410_2_3_5,"00:00:24,528","00:00:29,163",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,This is a slide that you have seen
cs-410_2_3_6,"00:00:29,163","00:00:32,318",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,about the Granville
cs-410_2_3_7,"00:00:32,318","00:00:39,122",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,We can have a test faction that consists
cs-410_2_3_8,"00:00:39,122","00:00:47,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,We can then run two systems on these
cs-410_2_3_9,"00:00:47,930","00:00:49,528",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,Their performance.
cs-410_2_3_10,"00:00:49,528","00:00:54,828",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,"And we raise the question,"
cs-410_2_3_11,"00:00:54,828","00:00:57,928",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,Is system A better or is system B better?
cs-410_2_3_12,"00:00:57,928","00:01:02,398",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,So let's now talk about how to
cs-410_2_3_13,"00:01:02,398","00:01:07,841",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,Suppose we have a total of 10 relevant
cs-410_2_3_14,"00:01:07,841","00:01:08,848",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,this query.
cs-410_2_3_15,"00:01:08,848","00:01:15,162",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,"Now, the relevant judgments show on"
cs-410_2_3_16,"00:01:15,162","00:01:19,908",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"And we have only seen 3 [INAUDIBLE] there,"
cs-410_2_3_17,"00:01:19,908","00:01:26,133",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"But, we can imagine there are other Random"
cs-410_2_3_18,"00:01:26,133","00:01:30,895",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,"So now, intuitively,"
cs-410_2_3_19,"00:01:30,895","00:01:35,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,A is better because it
cs-410_2_3_20,"00:01:35,668","00:01:42,019",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,And in particular we have seen
cs-410_2_3_21,"00:01:42,019","00:01:46,251",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"two of them are relevant but in system B,"
cs-410_2_3_22,"00:01:46,251","00:01:52,248",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,we have five results and
cs-410_2_3_23,"00:01:52,248","00:01:56,418",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,So intuitively it looks like
cs-410_2_3_24,"00:01:56,418","00:02:00,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And this infusion can be captured
cs-410_2_3_25,"00:02:00,670","00:02:05,866",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,where we simply compute to what extent
cs-410_2_3_26,"00:02:05,866","00:02:07,788",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"If you have 100% position,"
cs-410_2_3_27,"00:02:07,788","00:02:11,638",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,that would mean that all
cs-410_2_3_28,"00:02:11,638","00:02:16,476",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So in this case system A has
cs-410_2_3_29,"00:02:16,476","00:02:20,606",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,three System B has some
cs-410_2_3_30,"00:02:20,606","00:02:25,208",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,this shows that system
cs-410_2_3_31,"00:02:25,208","00:02:30,065",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,But we also talked about System B
cs-410_2_3_32,"00:02:30,065","00:02:35,220",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,would like to retrieve as many
cs-410_2_3_33,"00:02:35,220","00:02:39,839",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So in that case we'll have to compare
cs-410_2_3_34,"00:02:39,839","00:02:42,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,retrieve and
cs-410_2_3_35,"00:02:42,940","00:02:48,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,This method uses the completeness
cs-410_2_3_36,"00:02:48,000","00:02:51,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,In your retrieval result.
cs-410_2_3_37,"00:02:51,090","00:02:57,500",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,So we just assume that there are ten
cs-410_2_3_38,"00:02:57,500","00:03:01,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"And here we've got two of them,"
cs-410_2_3_39,"00:03:01,510","00:03:04,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,So the recall is 2 out of 10.
cs-410_2_3_40,"00:03:04,130","00:03:07,630",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"Whereas System B has called a 3,"
cs-410_2_3_41,"00:03:07,630","00:03:11,278",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,Now we can see by recall
cs-410_2_3_42,"00:03:11,278","00:03:15,240",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,And these two measures turn out to
cs-410_2_3_43,"00:03:15,240","00:03:16,978",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,evaluating search engine.
cs-410_2_3_44,"00:03:16,978","00:03:21,824",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,And they are very important because
cs-410_2_3_45,"00:03:21,824","00:03:24,298",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,other test evaluation problems.
cs-410_2_3_46,"00:03:24,298","00:03:28,660",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"For example, if you look at"
cs-410_2_3_47,"00:03:28,660","00:03:34,030",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,you tend to see precision recall numbers
cs-410_2_3_48,"00:03:35,290","00:03:38,520",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Okay so, now let's define these"
cs-410_2_3_49,"00:03:38,520","00:03:44,410",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,And these measures are to evaluate a set
cs-410_2_3_50,"00:03:44,410","00:03:48,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,we are considering that approximation
cs-410_2_3_51,"00:03:50,100","00:03:53,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,We can distinguish 4 cases depending
cs-410_2_3_52,"00:03:53,300","00:03:59,720",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,A document can be retrieved or
cs-410_2_3_53,"00:03:59,720","00:04:01,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,Because we are talking
cs-410_2_3_54,"00:04:02,710","00:04:05,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,A document can be also relevant or
cs-410_2_3_55,"00:04:05,640","00:04:10,310",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,not relevant depending on whether the user
cs-410_2_3_56,"00:04:11,950","00:04:16,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So we can now have counts of documents in.
cs-410_2_3_57,"00:04:16,890","00:04:21,610",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,Each of the four categories again
cs-410_2_3_58,"00:04:21,610","00:04:24,420",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,documents that have been retrieved and
cs-410_2_3_59,"00:04:24,420","00:04:30,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,B for documents that are not retrieved but
cs-410_2_3_60,"00:04:31,750","00:04:35,550",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,No with this table then
cs-410_2_3_61,"00:04:36,690","00:04:42,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,As the ratio of the relevant
cs-410_2_3_62,"00:04:42,450","00:04:47,440",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,retrieved documents A to the total
cs-410_2_3_63,"00:04:48,450","00:04:53,390",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"So, this is just A divided"
cs-410_2_3_64,"00:04:53,390","00:04:55,640",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,The sum of this column.
cs-410_2_3_65,"00:04:56,820","00:05:04,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,Singularly recall is defined by
cs-410_2_3_66,"00:05:04,360","00:05:07,470",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,So that's again to divide a by.
cs-410_2_3_67,"00:05:07,470","00:05:10,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,The sum of the row instead of the column.
cs-410_2_3_68,"00:05:10,360","00:05:15,810",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"All right, so we can see precision and"
cs-410_2_3_69,"00:05:16,930","00:05:20,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,that's the number of
cs-410_2_3_70,"00:05:20,000","00:05:22,449",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,But we're going to use
cs-410_2_3_71,"00:05:23,590","00:05:27,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,"Okay, so what would be an ideal result."
cs-410_2_3_72,"00:05:27,300","00:05:31,330",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Well, you can easily see being"
cs-410_2_3_73,"00:05:31,330","00:05:34,060",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,recall oil to be 1.0.
cs-410_2_3_74,"00:05:34,060","00:05:39,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,That means We have got 1% of
cs-410_2_3_75,"00:05:39,510","00:05:44,770",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"in our results, and all of the results"
cs-410_2_3_76,"00:05:44,770","00:05:47,540",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,At least there's no single
cs-410_2_3_77,"00:05:48,680","00:05:53,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"In reality, however, high recall tends"
cs-410_2_3_78,"00:05:53,920","00:05:56,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,And you can imagine why that's the case.
cs-410_2_3_79,"00:05:56,210","00:06:00,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,As you go down the to try to get as
cs-410_2_3_80,"00:06:00,790","00:06:05,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,"you tend to encounter a lot of documents,"
cs-410_2_3_81,"00:06:05,890","00:06:11,450",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,Note that this set can also
cs-410_2_3_82,"00:06:11,450","00:06:15,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"In the rest of this, that's why although"
cs-410_2_3_83,"00:06:15,490","00:06:20,560",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,"retrieve the documents, they are actually"
cs-410_2_3_84,"00:06:20,560","00:06:24,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,They are the fundamental measures in
cs-410_2_3_85,"00:06:24,270","00:06:30,010",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,We often are interested in The precision
cs-410_2_3_86,"00:06:30,010","00:06:33,400",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,This means we look at how many documents
cs-410_2_3_87,"00:06:33,400","00:06:35,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,among the top ten results
cs-410_2_3_88,"00:06:35,870","00:06:38,290",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,"Now, this is a very meaningful measure,"
cs-410_2_3_89,"00:06:38,290","00:06:43,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,because it tells us how many relevant
cs-410_2_3_90,"00:06:43,780","00:06:47,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,On the first page of where they
cs-410_2_3_91,"00:06:50,000","00:06:55,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,So precision and recall
cs-410_2_3_92,"00:06:55,780","00:07:02,040",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,use them to further evaluate a search
cs-410_2_3_93,"00:07:03,460","00:07:07,210",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We just said that there tends to be
cs-410_2_3_94,"00:07:07,210","00:07:10,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,so naturally it would be
cs-410_2_3_95,"00:07:10,730","00:07:15,490",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"And here's one method that's often used,"
cs-410_2_3_96,"00:07:15,490","00:07:21,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,it's a [INAUDIBLE] mean of precision and
cs-410_2_3_97,"00:07:22,450","00:07:27,741",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"So, you can see at first, compute the."
cs-410_2_3_98,"00:07:29,210","00:07:34,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,"Inverse of R and P here,"
cs-410_2_3_99,"00:07:34,360","00:07:41,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,the 2 by using coefficients
cs-410_2_3_100,"00:07:42,850","00:07:47,029",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,And after some transformation you can
cs-410_2_3_101,"00:07:49,010","00:07:51,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,And in any case it just becomes
cs-410_2_3_102,"00:07:51,790","00:07:56,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"recall, and beta is a parameter,"
cs-410_2_3_103,"00:07:56,360","00:08:01,572",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,It can control the emphasis
cs-410_2_3_104,"00:08:01,572","00:08:08,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,set beta to 1 We end up having a special
cs-410_2_3_105,"00:08:08,360","00:08:13,460",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,This is a popular measure that's often
cs-410_2_3_106,"00:08:13,460","00:08:14,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,And the formula looks very simple.
cs-410_2_3_107,"00:08:16,170","00:08:17,948",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"It's just this, here."
cs-410_2_3_108,"00:08:20,718","00:08:24,668",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,Now it's easy to see that if
cs-410_2_3_109,"00:08:24,668","00:08:28,570",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,larger recall than f
cs-410_2_3_110,"00:08:28,570","00:08:32,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"But, what's interesting is that"
cs-410_2_3_111,"00:08:32,940","00:08:36,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,recall is captured
cs-410_2_3_112,"00:08:36,260","00:08:41,000",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"So, in order to understand that, we"
cs-410_2_3_113,"00:08:42,170","00:08:48,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,can first look at the natural
cs-410_2_3_114,"00:08:48,270","00:08:53,090",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,using the symbol arithmetically
cs-410_2_3_115,"00:08:53,090","00:09:00,730",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,That would be likely the most natural way
cs-410_2_3_116,"00:09:01,870","00:09:05,940",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"If you want to think more,"
cs-410_2_3_117,"00:09:07,940","00:09:10,960",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,So why is this not as good as F1?
cs-410_2_3_118,"00:09:13,550","00:09:15,038",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,Or what's the problem with this?
cs-410_2_3_119,"00:09:18,121","00:09:23,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"Now, if you think about"
cs-410_2_3_120,"00:09:23,270","00:09:28,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,you can see this is
cs-410_2_3_121,"00:09:28,300","00:09:31,870",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"In this case,"
cs-410_2_3_122,"00:09:31,870","00:09:36,580",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"In the case of a sum, the total value"
cs-410_2_3_123,"00:09:36,580","00:09:42,850",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,that means if you have a very high P or
cs-410_2_3_124,"00:09:42,850","00:09:47,820",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,don't care about whether the other value
cs-410_2_3_125,"00:09:47,820","00:09:53,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,Now this is not desirable because one
cs-410_2_3_126,"00:09:53,920","00:09:57,110",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,We have perfect recall easily.
cs-410_2_3_127,"00:09:57,110","00:09:58,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,Can we imagine how?
cs-410_2_3_128,"00:09:59,810","00:10:03,830",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,It's probably very easy to
cs-410_2_3_129,"00:10:03,830","00:10:06,399",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,all the documents in the collection and
cs-410_2_3_130,"00:10:07,420","00:10:11,130",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,And this will give us 0.5 as the average.
cs-410_2_3_131,"00:10:11,130","00:10:15,583",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,But such results are clearly not
cs-410_2_3_132,"00:10:15,583","00:10:20,350",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,though the average using this
cs-410_2_3_133,"00:10:21,750","00:10:25,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,In contrast you can see F 1 would
cs-410_2_3_134,"00:10:25,930","00:10:27,750",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"recall are roughly That seminar, so"
cs-410_2_3_135,"00:10:27,750","00:10:33,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,it would a case where you had
cs-410_2_3_136,"00:10:35,320","00:10:38,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,So this means f one encodes
cs-410_2_3_137,"00:10:38,360","00:10:43,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,Now this example shows
cs-410_2_3_138,"00:10:43,690","00:10:44,230",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,Methodology here.
cs-410_2_3_139,"00:10:44,230","00:10:49,950",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,But when you try to solve a problem you
cs-410_2_3_140,"00:10:49,950","00:10:52,120",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,let's say in this it's
cs-410_2_3_141,"00:10:53,790","00:10:57,160",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,But it's important not to
cs-410_2_3_142,"00:10:57,160","00:11:00,790",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,It's important to think whether you
cs-410_2_3_143,"00:11:02,170","00:11:06,180",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,And once you think about the multiple
cs-410_2_3_144,"00:11:06,180","00:11:10,930",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,"difference, and then think about"
cs-410_2_3_145,"00:11:10,930","00:11:13,280",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,"In this case, if you think more carefully,"
cs-410_2_3_146,"00:11:13,280","00:11:15,920",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,you will think that F1
cs-410_2_3_147,"00:11:15,920","00:11:18,300",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Than the simple.
cs-410_2_3_148,"00:11:18,300","00:11:21,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,Although in other cases there
cs-410_2_3_149,"00:11:21,670","00:11:25,858",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,But in this case the seems not reasonable.
cs-410_2_3_150,"00:11:25,858","00:11:29,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,But if you don't pay attention
cs-410_2_3_151,"00:11:29,260","00:11:33,780",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,you might just take a easy way to
cs-410_2_3_152,"00:11:33,780","00:11:37,360",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,"And here later, you will find that,"
cs-410_2_3_153,"00:11:37,360","00:11:38,620",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,All right.
cs-410_2_3_154,"00:11:38,620","00:11:43,760",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,So this methodology is actually very
cs-410_2_3_155,"00:11:43,760","00:11:46,020",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,Try to think about the best solution.
cs-410_2_3_156,"00:11:46,020","00:11:50,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,"Try to understand the problem very well,"
cs-410_2_3_157,"00:11:50,890","00:11:55,890",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,"know why you needed this measure, and why"
cs-410_2_3_158,"00:11:55,890","00:11:59,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,And then use that to guide you in
cs-410_2_3_159,"00:12:03,320","00:12:08,510",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,"To summarize, we talked about"
cs-410_2_3_160,"00:12:08,510","00:12:11,530",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,are there retrievable
cs-410_2_3_161,"00:12:11,530","00:12:13,690",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,We also talk about the Recall.
cs-410_2_3_162,"00:12:13,690","00:12:17,260",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,"Which addresses the question, have all of"
cs-410_2_3_163,"00:12:17,260","00:12:21,250",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"These two, are the two,"
cs-410_2_3_164,"00:12:21,250","00:12:25,270",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,They are used for
cs-410_2_3_165,"00:12:25,270","00:12:28,670",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,We talk about F measure as a way to
cs-410_2_3_166,"00:12:29,970","00:12:33,600",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,We also talked about the tradeoff
cs-410_2_3_167,"00:12:33,600","00:12:38,140",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,And this turns out to depend
cs-410_2_3_168,"00:12:38,140","00:12:42,133",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,we'll discuss this point
cs-410_2_3_169,"00:12:42,133","00:12:52,133",https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,[MUSIC]
